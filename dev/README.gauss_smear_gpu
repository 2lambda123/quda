We are often spending too much time on Gaussian smearing when running
on GPUs as it done on the CPU.  Even on the CPU, it is not done optimally
as the gathers are done in two steps.  This is exacerbated by the number
of times the smearing step is repeated.

Here are some details about current setup and what needs to be done.

Current code is in generic_ks/gauss_smear_ks.c

lines 17-37: malloc_kg_temps()
	creates temporary storage on CPU.  May not need much of this on CPU,
	but could need it on GPU.  Possible we don't need this at all when
	we are doing single two loop gather.

lines 40-49: cleanup_kg_temps()
	see above

lines 51-112: forward2(int dir, su3_vector *dest, su3_vector *src,
     su3_matrix *t_links, int t0)
	This is the routine that gathes from two links away by doing two one
	link gathers.  Even within the MILC context it could be sped up by
	creating two-link gauge fields and then defining a new gather from
	two sites away.  We do this with the Naik links both on CPU and GPU,
	so it should be very easy to look at that code and adjust to only
	do product of two links, etc.
        Note on line 70 the if(t0 == ALL_T_SLICES) statement

	I just noticed that line 77 lacks any provision for OMP.  I bet this
	is an addition source of slowness
	For lines 94 and 101 see comments above
lines 114-194: backward2(int dir, su3_vector *dest, su3_vector *src,su3_matrix *t_links, int t0)
	This is very similar to previous routine.

lines 196-255: klein_gord_field(su3_vector *psi, su3_vector *chi, su3_matrix *t_links, Real msq, int t0)
	chi <- msq * psi - Lapl_3d psi
	line 224-228: This is where we would just need to gather neighbors
	from two links away and multiply then by the new stored two-link
	gauge field computing once in a new smearing routing and then used
	through all the iterations.
	lines 232, 242: see comments above
	This all needs to be migrated to GPU

lines 257-303: gauss_smear_v_field(su3_vector *src, su3_matrix *t_links, Real width, int iters, int t0)
	This is where multiple iterations are done.  Calls klein_gord_field.
	All of this must be done on GPU

lines 307-327: laplacian_v_field(su3_vector *src, su3_matrix *t_links, int t0)
	calls klein_gord_field with zero mass
	I wonder where we use this

lines 329-338: gauss_smear_ks_prop_field(ks_prop_field *src, su3_matrix *t_links, Real width, int iters, int t0)
	This calls gauss_smear_v_field for multiple colors, so it could remain
	on CPU as long as gauss_smear_v_field runs on GPU and returns result
	to CPU.  Should think about shared memory issues.

Some comments about what we need in the GPU code:  The main tasks are writing
a routine to create the two-link gauge fields, and writing routines to gather
from two sites away, then multiply by the two-link gauge field and sum up
as in gauss_smear_v_field.  Also need to manage, that the two-link term is
computed on the GPU when needed and not recomputed if the gauge field has not
changed (assuming we can afford the memory long term).

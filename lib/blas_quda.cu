#include <stdlib.h>
#include <stdio.h>

#include <quda_internal.h>
#include <blas_quda.h>
#include <color_spinor_field.h>
#include <face_quda.h> // this is where the MPI / QMP depdendent code is

#define REDUCE_MAX_BLOCKS 65536

#if (__COMPUTE_CAPABILITY__ >= 130)
#define QudaSumFloat double
#define QudaSumFloat2 double2
#define QudaSumFloat3 double3
#else
#define QudaSumFloat doublesingle
#define QudaSumFloat2 doublesingle2
#define QudaSumFloat3 doublesingle3
#include <double_single.h>
#endif

// These are used for reduction kernels
static QudaSumFloat *d_reduce=0;
static QudaSumFloat *h_reduce=0;

namespace quda {
  unsigned long long blas_flops;
  unsigned long long blas_bytes;
}

static dim3 blasBlock;
static dim3 blasGrid;

// generated by blas_test
#include <blas_param.h>

#include <float_vector.h>
#include <texture.h>

void zeroCuda(cudaColorSpinorField &a) { a.zero(); }

// blasTuning = 1 turns off error checking
static QudaTune blasTuning = QUDA_TUNE_NO;

namespace quda {

void initBlas(void)
{  
  if (!d_reduce) {
    if (cudaMalloc((void**) &d_reduce, 3*REDUCE_MAX_BLOCKS*sizeof(QudaSumFloat)) == cudaErrorMemoryAllocation) {
      errorQuda("Error allocating device reduction array");
    }
  }

  if (!h_reduce) {
    if (cudaMallocHost((void**) &h_reduce, 3*REDUCE_MAX_BLOCKS*sizeof(QudaSumFloat)) == cudaErrorMemoryAllocation) {
      errorQuda("Error allocating host reduction array");
    }
  }

}


void endBlas(void)
{
  if (d_reduce) {
    cudaFree(d_reduce);
    d_reduce = 0;
  }
  if (h_reduce) {
    cudaFreeHost(h_reduce);
    h_reduce = 0;
  }
}

void setBlasTuning(QudaTune tune)
{
  blasTuning = tune;
}

void setBlasParam(int kernel, int prec, int threads, int blocks)
{
  blas_threads[kernel][prec] = threads;
  blas_blocks[kernel][prec] = blocks;
}

} 

// FIXME: this should be queried from the device
#if (__COMPUTE_CAPACITY__ < 200)
#define MAX_BLOCK 512
#else
#define MAX_BLOCK 1024
#endif

void setBlock(int kernel, int length, QudaPrecision precision)
{
  int prec;
  switch(precision) {
  case QUDA_HALF_PRECISION:
    prec = 0;
    break;
  case QUDA_SINGLE_PRECISION:
    prec = 1;
    break;
  case QUDA_DOUBLE_PRECISION:
    prec = 2;
    break;
  }

  blasBlock.x = min(MAX_BLOCK, blas_threads[kernel][prec]);
  blasBlock.y = 1;
  blasBlock.z = 1;

  // the min here prevents creating too big a grid
  int grid = min(blas_blocks[kernel][prec], max(length/blas_threads[kernel][prec], 1));
  blasGrid.x = grid;
  blasGrid.y = 1;
  blasGrid.z = 1;
}

#define checkSpinor(a, b)						\
  {									\
    if (a.Precision() != b.Precision())					\
      errorQuda("precisions do not match: %d %d", a.Precision(), b.Precision()); \
    if (a.Length() != b.Length())					\
      errorQuda("lengths do not match: %d %d", a.Length(), b.Length());	\
    if (a.Stride() != b.Stride())					\
      errorQuda("strides do not match: %d %d", a.Stride(), b.Stride());	\
  }

// For kernels with precision conversion built in
#define checkSpinorLength(a, b)						\
  {									\
    if (a.Length() != b.Length()) {					\
      errorQuda("engths do not match: %d %d", a.Length(), b.Length());	\
    }									


template <typename FloatN, int N, typename Output, typename Input>
__global__ void copyKernel(Output Y, Input X, int length) {
  unsigned int i = blockIdx.x*(blockDim.x) + threadIdx.x;
  unsigned int gridSize = gridDim.x*blockDim.x;

  while (i < length) {
    FloatN x[N];
    X.load(x, i);
    Y.save(x, i);
    i += gridSize;
  }
}


void copyCuda(cudaColorSpinorField &dst, const cudaColorSpinorField &src) {

  if (&src == &dst) return; // aliasing fields
  if (src.Nspin() != 1 && src.Nspin() != 4) errorQuda("nSpin(%d) not supported\n", src.Nspin());

  if (dst.SiteSubset() == QUDA_FULL_SITE_SUBSET || src.SiteSubset() == QUDA_FULL_SITE_SUBSET) {
    copyCuda(dst.Even(), src.Even());
    copyCuda(dst.Odd(), src.Odd());
    return;
  }

  // For a given dst precision, there are two non-trivial possibilities for the
  // src precision.  The higher one corresponds to kernel index 0 (in the table
  // of block and grid dimensions), while the lower one corresponds to index 1.
  int id;
  if (src.Precision() == QUDA_DOUBLE_PRECISION ||
      dst.Precision() == QUDA_DOUBLE_PRECISION && src.Precision() == QUDA_SINGLE_PRECISION) {
    id = 0;
  } else {
    id = 1;
  }
  setBlock(id, dst.Stride(), dst.Precision());

  quda::blas_bytes += src.RealLength()*((int)src.Precision() + (int)dst.Precision());

  if (dst.Precision() == QUDA_DOUBLE_PRECISION && src.Precision() == QUDA_SINGLE_PRECISION) {
    if (src.Nspin() == 4){
      SpinorTexture<float4, float4, float4, 6, 0> src_tex(src);
      Spinor<float4, float2, double2, 6> dst_spinor(dst);
      copyKernel<float4, 6><<<blasGrid, blasBlock>>>(dst_spinor, src_tex, src.Volume());
    } else { //src.Nspin() == 1
      SpinorTexture<float2, float2, float2, 3, 0> src_tex(src);
      Spinor<float2, float2, double2, 3> dst_spinor(dst);
      copyKernel<float2, 3><<<blasGrid, blasBlock>>>(dst_spinor, src_tex, src.Volume());
    }

  } else if (dst.Precision() == QUDA_SINGLE_PRECISION && src.Precision() == QUDA_DOUBLE_PRECISION) {
    if (src.Nspin() == 4){
      SpinorTexture<float4, float2, double2, 6, 0> src_tex(src);
      Spinor<float4, float4, float4, 6> dst_spinor(dst);
      copyKernel<float4, 6><<<blasGrid, blasBlock>>>(dst_spinor, src_tex, src.Volume());
    } else { //src.Nspin() ==1
      Spinor<float2, float2, double2, 3> src_tex(src);
      Spinor<float2, float2, float2, 3> dst_spinor(dst);
      copyKernel<float2, 3><<<blasGrid, blasBlock>>>(dst_spinor, src_tex, src.Volume());
    }
  } else if (dst.Precision() == QUDA_SINGLE_PRECISION && src.Precision() == QUDA_HALF_PRECISION) {
    quda::blas_bytes += src.Volume()*sizeof(float);
    if (src.Nspin() == 4){      
      SpinorTexture<float4, float4, short4, 6, 0> src_tex(src);
      Spinor<float4, float4, float4, 6> dst_spinor(dst);
      copyKernel<float4, 6><<<blasGrid, blasBlock>>>(dst_spinor, src_tex, src.Volume());
    } else { //nSpin== 1;
      SpinorTexture<float2, float2, short2, 3, 0> src_tex(src);
      Spinor<float2, float2, float2, 3> dst_spinor(dst);
      copyKernel<float2, 3><<<blasGrid, blasBlock>>>(dst_spinor, src_tex, src.Volume());
    }
  } else if (dst.Precision() == QUDA_HALF_PRECISION && src.Precision() == QUDA_SINGLE_PRECISION) {
    quda::blas_bytes += dst.Volume()*sizeof(float);
    if (src.Nspin() == 4){
      SpinorTexture<float4, float4, float4, 6, 0> src_tex(src);
      Spinor<float4, float4, short4, 6> dst_spinor(dst);
      copyKernel<float4, 6><<<blasGrid, blasBlock>>>(dst_spinor, src_tex, src.Volume());
    } else { //nSpin == 1
      SpinorTexture<float2, float2, float2, 3, 0> src_tex(src);
      Spinor<float2, float2, short2, 3> dst_spinor(dst);
      copyKernel<float2, 3><<<blasGrid, blasBlock>>>(dst_spinor, src_tex, src.Volume());
    }
  } else if (dst.Precision() == QUDA_DOUBLE_PRECISION && src.Precision() == QUDA_HALF_PRECISION) {
    quda::blas_bytes += src.Volume()*sizeof(float);
    if (src.Nspin() == 4){
      SpinorTexture<double2, float4, short4, 12, 0> src_tex(src);
      Spinor<double2, double2, double2, 12> dst_spinor(dst);
      copyKernel<double2, 12><<<blasGrid, blasBlock>>>(dst_spinor, src_tex, src.Volume());
    } else { //nSpin == 1
      SpinorTexture<double2, float2, short2, 3, 0> src_tex(src);
      Spinor<double2, double2, double2, 3> dst_spinor(dst);
      copyKernel<double2, 3><<<blasGrid, blasBlock>>>(dst_spinor, src_tex, src.Volume());
    }
  } else if (dst.Precision() == QUDA_HALF_PRECISION && src.Precision() == QUDA_DOUBLE_PRECISION) {
    quda::blas_bytes += dst.Volume()*sizeof(float);
    if (src.Nspin() == 4){
      SpinorTexture<double2, double2, double2, 12, 0> src_tex(src);
      Spinor<double2, double4, short4, 12> dst_spinor(dst);
      copyKernel<double2, 12><<<blasGrid, blasBlock>>>(dst_spinor, src_tex, src.Volume());
    } else { //nSpin == 1
      SpinorTexture<double2, double2, double2, 3, 0> src_tex(src);
      Spinor<double2, double2, short2, 3> dst_spinor(dst);
      copyKernel<double2, 3><<<blasGrid, blasBlock>>>(dst_spinor, src_tex, src.Volume());
    }
  } else {
    cudaMemcpy(dst.V(), src.V(), dst.Bytes(), cudaMemcpyDeviceToDevice);
    if (dst.Precision() == QUDA_HALF_PRECISION) {
      cudaMemcpy(dst.Norm(), src.Norm(), dst.NormBytes(), cudaMemcpyDeviceToDevice);
      quda::blas_bytes += 2*dst.RealLength()*sizeof(float);
    }
  }
  
  if (!blasTuning) checkCudaError();
}

/**
   Generic blas kernel with four loads and up to four stores.
 */
template <typename FloatN, int M, int writeX, int writeY, int writeZ, int writeW, 
	  typename InputX, typename InputY, typename InputZ, typename InputW, 
	  typename OutputX, typename OutputY, typename OutputZ, typename OutputW, typename Functor>
__global__ void blasKernel(InputX X, InputY Y, InputZ Z, InputW W, Functor f, 
			   OutputX XX, OutputY YY, OutputZ ZZ, OutputW WW, int length) {
  unsigned int i = blockIdx.x*(blockDim.x) + threadIdx.x;
  unsigned int gridSize = gridDim.x*blockDim.x;
  while (i < length) {
    FloatN x[M], y[M], z[M], w[M];
    X.load(x, i);
    Y.load(y, i);
    Z.load(z, i);
    W.load(w, i);

#pragma unroll
    for (int j=0; j<M; j++) f(x[j], y[j], z[j], w[j]);

    if (writeX) XX.save(x, i);
    if (writeY) YY.save(y, i);
    if (writeZ) ZZ.save(z, i);
    if (writeW) WW.save(w, i);
    i += gridSize;
  }
}

/**
   Driver for generic blas routine with four loads and two store.
 */
template <template <typename Float, typename FloatN> class Functor,
	  int writeX, int writeY, int writeZ, int writeW>
void blasCuda(const int kernel, const double2 &a, const double2 &b, const double2 &c,
	      cudaColorSpinorField &x, cudaColorSpinorField &y, 
	      cudaColorSpinorField &z, cudaColorSpinorField &w) {
  setBlock(kernel, x.Length(), x.Precision());
  checkSpinor(x, y);
  checkSpinor(x, z);
  checkSpinor(x, w);

  if (x.SiteSubset() == QUDA_FULL_SITE_SUBSET) {
    blasCuda<Functor,writeX,writeY,writeZ,writeW>(kernel, a, b, c, x.Even(), y.Even(), z.Even(), w.Even());
    blasCuda<Functor,writeX,writeY,writeZ,writeW>(kernel, a, b, c, x.Odd(), y.Odd(), z.Odd(), w.Even());
    return;
  }

  if (x.Precision() == QUDA_DOUBLE_PRECISION) {
    Spinor<double2,double2,double2,1> X(x);
    Spinor<double2,double2,double2,1> Y(y);
    Spinor<double2,double2,double2,1> Z(z);
    Spinor<double2,double2,double2,1> W(w);
    Functor<double2, double2> f(a,b,c);
    blasKernel<double2,1,writeX,writeY,writeZ,writeW><<<blasGrid, blasBlock>>>
      (X, Y, Z, W, f, X, Y, Z, W, x.Length()/2);
  } else if (x.Precision() == QUDA_SINGLE_PRECISION) {
    Spinor<float4,float4,float4,1> X(x);
    Spinor<float4,float4,float4,1> Y(y);
    Spinor<float4,float4,float4,1> Z(z);
    Spinor<float4,float4,float4,1> W(w);
    Functor<float2, float4> f(make_float2(a.x, a.y), make_float2(b.x, b.y), make_float2(c.x, c.y));
    blasKernel<float4,1,writeX,writeY,writeZ,writeW><<<blasGrid, blasBlock>>>
      (X, Y, Z, W, f, X, Y, Z, W, x.Length()/4);
  } else {
    if (x.Nspin() == 4){ //wilson
      SpinorTexture<float4,float4,short4,6,0> xTex(x);
      SpinorTexture<float4,float4,short4,6,1> yTex(y);
      SpinorTexture<float4,float4,short4,6,2> zTex(z);
      SpinorTexture<float4,float4,short4,6,3> wTex(w);
      Spinor<float4,float4,short4,6> xStore(x);
      Spinor<float4,float4,short4,6> yStore(y);
      Spinor<float4,float4,short4,6> zStore(z);
      Spinor<float4,float4,short4,6> wStore(w);
      Functor<float2, float4> f(make_float2(a.x, a.y), make_float2(b.x, b.y), make_float2(c.x, c.y));
      blasKernel<float4, 6, writeX, writeY, writeZ, writeW> <<<blasGrid, blasBlock>>> 
	(xTex, yTex, zTex, wTex, f, xStore, yStore, zStore, wStore, y.Volume());
    } else if (x.Nspin() == 1) {//staggered
      SpinorTexture<float2,float2,short2,3,0> xTex(x);
      SpinorTexture<float2,float2,short2,3,1> yTex(y);
      SpinorTexture<float2,float2,short2,3,2> zTex(z);
      SpinorTexture<float2,float2,short2,3,3> wTex(w);
      Spinor<float2,float2,short2,3> xStore(x);
      Spinor<float2,float2,short2,3> yStore(y);
      Spinor<float2,float2,short2,3> zStore(z);
      Spinor<float2,float2,short2,3> wStore(w);
      Functor<float2, float2> f(make_float2(a.x, a.y), make_float2(b.x, b.y), make_float2(c.x, c.y));
      blasKernel<float2, 3,writeX,writeY,writeZ,writeW> <<<blasGrid, blasBlock>>>
	(xTex, yTex, zTex, wTex, f, xStore, yStore, zStore, wStore, y.Volume());
    } else { errorQuda("ERROR: nSpin=%d is not supported\n", x.Nspin()); }
    quda::blas_bytes += Functor<double2,double2>::streams()*x.Volume()*sizeof(float);
  }
  quda::blas_bytes += Functor<double2,double2>::streams()*x.RealLength()*x.Precision();
  quda::blas_flops += Functor<double2,double2>::flops()*x.RealLength();

  if (!blasTuning) checkCudaError();
}

/**
   Functor to perform the operation y = a*x + b*y
*/
template <typename Float2, typename FloatN>
struct axpby {
  const Float2 a;
  const Float2 b;
  axpby(const Float2 &a, const Float2 &b, const Float2 &c) : a(a), b(b) { ; }
  __device__ void operator()(const FloatN &x, FloatN &y, const FloatN &z, const FloatN &w) { y = a.x*x + b.x*y; }
  static int streams() { return 3; } //! total number of input and output streams
  static int flops() { return 3; } //! flops per element
};

void axpbyCuda(const double &a, cudaColorSpinorField &x, const double &b, cudaColorSpinorField &y) {
  const int kernel = 2;
  blasCuda<axpby,0,1,0,0>(kernel, make_double2(a, 0.0), make_double2(b, 0.0), make_double2(0.0, 0.0),
		  x, y, x, x);
}

/**
   Functor to perform the operation y += x
*/
template <typename Float2, typename FloatN>
struct xpy {
  xpy(const Float2 &a, const Float2 &b, const Float2 &c) { ; }
  __device__ void operator()(const FloatN &x, FloatN &y, const FloatN &z, const FloatN &w) { y += x ; }
  static int streams() { return 3; } //! total number of input and output streams
  static int flops() { return 1; } //! flops per element
};

void xpyCuda(cudaColorSpinorField &x, cudaColorSpinorField &y) {
  const int kernel = 3;
  blasCuda<xpy,0,1,0,0>(kernel, make_double2(1.0, 0.0), make_double2(1.0, 0.0), make_double2(0.0, 0.0), 
		x, y, x, x);
}

/**
   Functor to perform the operation y += a*x
*/
template <typename Float2, typename FloatN>
struct axpy {
  const Float2 a;
  axpy(const Float2 &a, const Float2 &b, const Float2 &c) : a(a) { ; }
  __device__ void operator()(const FloatN &x, FloatN &y, const FloatN &z, const FloatN &w) { y = a.x*x + y; }
  static int streams() { return 3; } //! total number of input and output streams
  static int flops() { return 2; } //! flops per element
};

void axpyCuda(const double &a, cudaColorSpinorField &x, cudaColorSpinorField &y) {
  const int kernel = 4;
  blasCuda<axpy,0,1,0,0>(kernel, make_double2(a, 0.0), make_double2(1.0, 0.0), make_double2(0.0, 0.0), 
		 x, y, x, x);
}

/**
   Functor to perform the operation y = x + a*y
*/
template <typename Float2, typename FloatN>
struct xpay {
  const Float2 a;
  xpay(const Float2 &a, const Float2 &b, const Float2 &c) : a(a) { ; }
  __device__ void operator()(const FloatN &x, FloatN &y, const FloatN &z, const FloatN &w) { y = x + a.x*y; }
  static int streams() { return 3; } //! total number of input and output streams
  static int flops() { return 2; } //! flops per element
};

void xpayCuda(cudaColorSpinorField &x, const double &a, cudaColorSpinorField &y) {
  const int kernel = 5;
  blasCuda<xpay,0,1,0,0>(kernel, make_double2(a,0.0), make_double2(0.0, 0.0), make_double2(0.0, 0.0),
			 x, y, x, x);
}

/**
   Functor to perform the operation y -= x;
*/
template <typename Float2, typename FloatN>
struct mxpy {
  mxpy(const Float2 &a, const Float2 &b, const Float2 &c) { ; }
  __device__ void operator()(const FloatN &x, FloatN &y, const FloatN &z, const FloatN &w) { y -= x; }
  static int streams() { return 3; } //! total number of input and output streams
  static int flops() { return 1; } //! flops per element
};

void mxpyCuda(cudaColorSpinorField &x, cudaColorSpinorField &y) {
  const int kernel = 6;
  blasCuda<mxpy,0,1,0,0>(kernel, make_double2(1.0, 0.0), make_double2(1.0, 0.0), 
			 make_double2(0.0, 0.0), x, y, x, x);
}

/**
   Functor to perform the operation x *= a
*/
template <typename Float2, typename FloatN>
struct ax {
  const Float2 a;
  ax(const Float2 &a, const Float2 &b, const Float2 &c) : a(a) { ; }
  __device__ void operator()(FloatN &x, const FloatN &y, const FloatN &z, const FloatN &w) { x *= a.x; }
  static int streams() { return 2; } //! total number of input and output streams
  static int flops() { return 1; } //! flops per element
};

void axCuda(const double &a, cudaColorSpinorField &x) {
  const int kernel = 7;
  blasCuda<ax,1,0,0,0>(kernel, make_double2(a, 0.0), make_double2(0.0, 0.0), 
		       make_double2(0.0, 0.0), x, x, x, x);
}

/**
   Functor to perform the operation y += a * x  (complex-valued)
*/

__device__ void caxpy_(const float2 &a, const float4 &x, float4 &y) {
  y.x += a.x*x.x; y.x -= a.y*x.y;
  y.y += a.y*x.x; y.y += a.x*x.y;
  y.z += a.x*x.z; y.z -= a.y*x.w;
  y.w += a.y*x.z; y.w += a.x*x.w;
}

__device__ void caxpy_(const float2 &a, const float2 &x, float2 &y) {
  y.x += a.x*x.x; y.x -= a.y*x.y;
  y.y += a.y*x.x; y.y += a.x*x.y;
}

__device__ void caxpy_(const double2 &a, const double2 &x, double2 &y) {
  y.x += a.x*x.x; y.x -= a.y*x.y;
  y.y += a.y*x.x; y.y += a.x*x.y;
}

template <typename Float2, typename FloatN>
struct caxpy {
  const Float2 a;
  caxpy(const Float2 &a, const Float2 &b, const Float2 &c) : a(a) { ; }
  __device__ void operator()(const FloatN &x, FloatN &y, const FloatN &z, const FloatN &w) { caxpy_(a, x, y); }
  static int streams() { return 3; } //! total number of input and output streams
  static int flops() { return 4; } //! flops per element
};

void caxpyCuda(const quda::Complex &a, cudaColorSpinorField &x, cudaColorSpinorField &y) {
  const int kernel = 8;
  blasCuda<caxpy,0,1,0,0>(kernel, make_double2(real(a),imag(a)), make_double2(0.0, 0.0), 
			  make_double2(0.0, 0.0), x, y, x, x);
}

/**
   Functor to perform the operation y = a*x + b*y  (complex-valued)
*/

__device__ void caxpby_(const float2 &a, const float4 &x, const float2 &b, float4 &y)					
  { float4 yy;								
  yy.x = a.x*x.x; yy.x -= a.y*x.y; yy.x += b.x*y.x; yy.x -= b.y*y.y;	
  yy.y = a.y*x.x; yy.y += a.x*x.y; yy.y += b.y*y.x; yy.y += b.x*y.y;	
  yy.z = a.x*x.z; yy.z -= a.y*x.w; yy.z += b.x*y.z; yy.z -= b.y*y.w;	
  yy.w = a.y*x.z; yy.w += a.x*x.w; yy.w += b.y*y.z; yy.w += b.x*y.w;	
  y = yy; }

__device__ void caxpby_(const float2 &a, const float2 &x, const float2 &b, float2 &y)
  { float2 yy;								
  yy.x = a.x*x.x; yy.x -= a.y*x.y; yy.x += b.x*y.x; yy.x -= b.y*y.y;	
  yy.y = a.y*x.x; yy.y += a.x*x.y; yy.y += b.y*y.x; yy.y += b.x*y.y;	
  y = yy; }

__device__ void caxpby_(const double2 &a, const double2 &x, const double2 &b, double2 &y)				 
  { double2 yy;								
  yy.x = a.x*x.x; yy.x -= a.y*x.y; yy.x += b.x*y.x; yy.x -= b.y*y.y;	
  yy.y = a.y*x.x; yy.y += a.x*x.y; yy.y += b.y*y.x; yy.y += b.x*y.y;	
  y = yy; }

template <typename Float2, typename FloatN>
struct caxpby {
  const Float2 a;
  const Float2 b;
  caxpby(const Float2 &a, const Float2 &b, const Float2 &c) : a(a), b(b) { ; }
  __device__ void operator()(const FloatN &x, FloatN &y, const FloatN &z, const FloatN &w) { caxpby_(a, x, b, y); }
  static int streams() { return 3; } //! total number of input and output streams
  static int flops() { return 7; } //! flops per element
};

void caxpbyCuda(const quda::Complex &a, cudaColorSpinorField &x, const quda::Complex &b, cudaColorSpinorField &y) {
  const int kernel = 9;
  blasCuda<caxpby,0,1,0,0>(kernel, make_double2(a.real(),a.imag()), make_double2(b.real(), b.imag()), 
			   make_double2(0.0, 0.0), x, y, x, x);
}

/**
   Functor to performs the operation z[i] = x[i] + a*y[i] + b*z[i]
*/

__device__ void cxpaypbz_(const float4 &x, const float2 &a, const float4 &y, const float2 &b, float4 &z) {
  float4 zz;
  zz.x = x.x + a.x*y.x; zz.x -= a.y*y.y; zz.x += b.x*z.x; zz.x -= b.y*z.y;
  zz.y = x.y + a.y*y.x; zz.y += a.x*y.y; zz.y += b.y*z.x; zz.y += b.x*z.y;
  zz.z = x.z + a.x*y.z; zz.z -= a.y*y.w; zz.z += b.x*z.z; zz.z -= b.y*z.w;
  zz.w = x.w + a.y*y.z; zz.w += a.x*y.w; zz.w += b.y*z.z; zz.w += b.x*z.w;
  z = zz;
}

__device__ void cxpaypbz_(const float2 &x, const float2 &a, const float2 &y, const float2 &b, float2 &z) {
  float2 zz;
  zz.x = x.x + a.x*y.x; zz.x -= a.y*y.y; zz.x += b.x*z.x; zz.x -= b.y*z.y;
  zz.y = x.y + a.y*y.x; zz.y += a.x*y.y; zz.y += b.y*z.x; zz.y += b.x*z.y;
  z = zz;
}

__device__ void cxpaypbz_(const double2 &x, const double2 &a, const double2 &y, const double2 &b, double2 &z) {
  double2 zz;
  zz.x = x.x + a.x*y.x; zz.x -= a.y*y.y; zz.x += b.x*z.x; zz.x -= b.y*z.y;
  zz.y = x.y + a.y*y.x; zz.y += a.x*y.y; zz.y += b.y*z.x; zz.y += b.x*z.y;
  z = zz;
}

template <typename Float2, typename FloatN>
struct cxpaypbz {
  const Float2 a;
  const Float2 b;
  cxpaypbz(const Float2 &a, const Float2 &b, const Float2 &c) : a(a), b(b) { ; }
  __device__ void operator()(const FloatN &x, const FloatN &y, FloatN &z, FloatN &w) 
  { cxpaypbz_(x, a, y, b, z); }
  static int streams() { return 4; } //! total number of input and output streams
  static int flops() { return 8; } //! flops per element
};

void cxpaypbzCuda(cudaColorSpinorField &x, const quda::Complex &a, cudaColorSpinorField &y, 
		  const quda::Complex &b, cudaColorSpinorField &z) {
  const int kernel = 10;
  blasCuda<cxpaypbz,0,0,1,0>(kernel, make_double2(a.real(),a.imag()), make_double2(b.real(), b.imag()), 
			     make_double2(0.0, 0.0), x, y, z, z);
}

/**
   Functor performing the operations: y[i] = a*x[i] + y[i]; x[i] = b*z[i] + c*x[i]
*/
template <typename Float2, typename FloatN>
struct axpyBzpcx {
  const Float2 a;
  const Float2 b;
  const Float2 c;
  axpyBzpcx(const Float2 &a, const Float2 &b, const Float2 &c) : a(a), b(b), c(c) { ; }
  __device__ void operator()(FloatN &x, FloatN &y, const FloatN &z, const FloatN &w)
  { y += a.x*x; x = b.x*z + c.x*x; }
  static int streams() { return 5; } //! total number of input and output streams
  static int flops() { return 10; } //! flops per element
};

void axpyBzpcxCuda(const double &a, cudaColorSpinorField& x, cudaColorSpinorField& y, const double &b, 
		   cudaColorSpinorField& z, const double &c) {
  const int kernel = 11;
  blasCuda<axpyBzpcx,1,1,0,0>(kernel, make_double2(a,0.0), make_double2(b,0.0), make_double2(c,0.0), 
			      x, y, z, x);
}

/**
   Functor performing the operations: y[i] = a*x[i] + y[i]; x[i] = z[i] + b*x[i]
*/
template <typename Float2, typename FloatN>
struct axpyZpbx {
  const Float2 a;
  const Float2 b;
  axpyZpbx(const Float2 &a, const Float2 &b, const Float2 &c) : a(a), b(b) { ; }
  __device__ void operator()(FloatN &x, FloatN &y, const FloatN &z, const FloatN &w)
  { y += a.x*x; x = z + b.x*x; }
  static int streams() { return 5; } //! total number of input and output streams
  static int flops() { return 8; } //! flops per element
};

void axpyZpbxCuda(const double &a, cudaColorSpinorField& x, cudaColorSpinorField& y,
		  cudaColorSpinorField& z, const double &b) {
  const int kernel = 12;
  // swap arguments around 
  blasCuda<axpyZpbx,1,1,0,0>(kernel, make_double2(a,0.0), make_double2(b,0.0), make_double2(0.0,0.0),
			     x, y, z, x);
}

/**
   Functor performing the operations z[i] = a*x[i] + b*y[i] + z[i] and y[i] -= b*w[i]
*/
template <typename Float2, typename FloatN>
struct caxpbypzYmbw {
  const Float2 a;
  const Float2 b;
  caxpbypzYmbw(const Float2 &a, const Float2 &b, const Float2 &c) : a(a), b(b) { ; }
  __device__ void operator()(const FloatN &x, FloatN &y, FloatN &z, const FloatN &w)
  { caxpy_(a, x, z); caxpy_(b, y, z); caxpy_(-b, w, y); }

  static int streams() { return 6; } //! total number of input and output streams
  static int flops() { return 12; } //! flops per element
};

void caxpbypzYmbwCuda(const quda::Complex &a, cudaColorSpinorField &x, const quda::Complex &b, 
		      cudaColorSpinorField &y, cudaColorSpinorField &z, cudaColorSpinorField &w) {
  const int kernel = 12;
  blasCuda<caxpbypzYmbw,0,1,1,0>(kernel, make_double2(a.real(),a.imag()), make_double2(b.real(), b.imag()), 
				 make_double2(0.0,0.0), x, y, z, w);
}

/**
   Functor performing the operation y[i] += a*b*x[i], x[i] *= a
*/
template <typename Float2, typename FloatN>
struct cabxpyAx {
  const Float2 a;
  const Float2 b;
  cabxpyAx(const Float2 &a, const Float2 &b, const Float2 &c) : a(a), b(b) { ; }
  __device__ void operator()(FloatN &x, FloatN &y, const FloatN &z, const FloatN &w) 
  { x *= a.x; caxpy_(b, x, y); }
  static int streams() { return 4; } //! total number of input and output streams
  static int flops() { return 5; } //! flops per element
};

void cabxpyAxCuda(const double &a, const quda::Complex &b, 
		  cudaColorSpinorField &x, cudaColorSpinorField &y) {
  const int kernel = 14;
  // swap arguments around 
  blasCuda<cabxpyAx,1,1,0,0>(kernel, make_double2(a,0.0), make_double2(b.real(),b.imag()), 
			     make_double2(0.0,0.0), x, y, x, x);
}

/**
   Functor performing the operation z[i] = a*x[i] + b*y[i] + z[i]
*/
template <typename Float2, typename FloatN>
struct caxpbypz {
  const Float2 a;
  const Float2 b;
  caxpbypz(const Float2 &a, const Float2 &b, const Float2 &c) : a(a), b(b) { ; }
  __device__ void operator()(const FloatN &x, const FloatN &y, FloatN &z, const FloatN &w) 
  { caxpy_(a, x, z); caxpy_(b, y, z); }
  static int streams() { return 4; } //! total number of input and output streams
  static int flops() { return 5; } //! flops per element
};

void caxpbypzCuda(const quda::Complex &a, cudaColorSpinorField &x, const quda::Complex &b, 
		  cudaColorSpinorField &y, cudaColorSpinorField &z) {
  const int kernel = 15;
  blasCuda<caxpbypz,0,0,1,0>(kernel, make_double2(a.real(),a.imag()), make_double2(b.real(),b.imag()), 
			     make_double2(0.0,0.0), x, y, z, z);
}

/**
   Functor Performing the operation w[i] = a*x[i] + b*y[i] + c*z[i] + w[i]
*/
template <typename Float2, typename FloatN>
struct caxpbypczpw {
  const Float2 a;
  const Float2 b;
  const Float2 c;
  caxpbypczpw(const Float2 &a, const Float2 &b, const Float2 &c) : a(a), b(b), c(c) { ; }
  __device__ void operator()(const FloatN &x, const FloatN &y, const FloatN &z, FloatN &w) 
  { caxpy_(a, x, w); caxpy_(b, y, w); caxpy_(c, z, w); }

  static int streams() { return 4; } //! total number of input and output streams
  static int flops() { return 5; } //! flops per element
};

void caxpbypczpwCuda(const quda::Complex &a, cudaColorSpinorField &x, const quda::Complex &b, 
		     cudaColorSpinorField &y, const quda::Complex &c, cudaColorSpinorField &z, 
		     cudaColorSpinorField &w) {
  const int kernel = 16;
  blasCuda<caxpbypczpw,0,0,0,1>(kernel, make_double2(a.real(),a.imag()), make_double2(b.real(),b.imag()), 
				make_double2(c.real(), c.imag()), x, y, z, w);
}

__host__ __device__ void zero(double &x) { x = 0.0; }
__host__ __device__ void zero(double2 &x) { x.x = 0.0; x.y = 0.0; }
__host__ __device__ void zero(double3 &x) { x.x = 0.0; x.y = 0.0; x.z = 0.0; }
__device__ void copytoshared(double *s, const int i, const double x, const int block) { s[i] = x; }
__device__ void copytoshared(double *s, const int i, const double2 x, const int block) 
{ s[i] = x.x; s[i+block] = x.y; }
__device__ void copytoshared(double *s, const int i, const double3 x, const int block) 
{ s[i] = x.x; s[i+block] = x.y; s[i+2*block] = x.z; }
__device__ void copyfromshared(double &x, const double *s, const int i, const int block) { x = s[i]; }
__device__ void copyfromshared(double2 &x, const double *s, const int i, const int block) 
{ x.x = s[i]; x.y = s[i+block]; }
__device__ void copyfromshared(double3 &x, const double *s, const int i, const int block) 
{ x.x = s[i]; x.y = s[i+block]; x.z = s[i+2*block]; }

template<typename ReduceType, typename ReduceSimpleType> 
__device__ void add(ReduceSimpleType *s, const int i, const int j, const int block) { }
template<typename ReduceType, typename ReduceSimpleType> 
__device__ void add(volatile ReduceSimpleType *s, const int i, const int j, const int block) { }

template<> __device__ void add<double,double>(double *s, const int i, const int j, const int block) 
{ s[i] += s[j]; }
template<> __device__ void add<double,double>(volatile double *s, const int i, const int j, const int block) 
{ s[i] += s[j]; }

template<> __device__ void add<double2,double>(double *s, const int i, const int j, const int block) 
{ s[i] += s[j]; s[i+block] += s[j+block];}
template<> __device__ void add<double2,double>(volatile double *s, const int i, const int j, const int block) 
{ s[i] += s[j]; s[i+block] += s[j+block];}

template<> __device__ void add<double3,double>(double *s, const int i, const int j, const int block) 
{ s[i] += s[j]; s[i+block] += s[j+block]; s[i+2*block] += s[j+2*block];}
template<> __device__ void add<double3,double>(volatile double *s, const int i, const int j, const int block) 
{ s[i] += s[j]; s[i+block] += s[j+block]; s[i+2*block] += s[j+2*block];}

#if (__COMPUTE_CAPABILITY__ < 130)
__host__ __device__ void zero(doublesingle &x) { x = 0.0; }
__host__ __device__ void zero(doublesingle2 &x) { x.x = 0.0; x.y = 0.0; }
__host__ __device__ void zero(doublesingle3 &x) { x.x = 0.0; x.y = 0.0; x.z = 0.0; }
__device__ void copytoshared(doublesingle *s, const int i, const doublesingle x, const int block) { s[i] = x; }
__device__ void copytoshared(doublesingle *s, const int i, const doublesingle2 x, const int block) 
{ s[i] = x.x; s[i+block] = x.y; }
__device__ void copytoshared(doublesingle *s, const int i, const doublesingle3 x, const int block) 
{ s[i] = x.x; s[i+block] = x.y; s[i+2*block] = x.z; }
__device__ void copyfromshared(doublesingle &x, const doublesingle *s, const int i, const int block) { x = s[i]; }
__device__ void copyfromshared(doublesingle2 &x, const doublesingle *s, const int i, const int block) 
{ x.x = s[i]; x.y = s[i+block]; }
__device__ void copyfromshared(doublesingle3 &x, const doublesingle *s, const int i, const int block) 
{ x.x = s[i]; x.y = s[i+block]; x.z = s[i+2*block]; }

template<> __device__ void add<doublesingle,doublesingle>(doublesingle *s, const int i, const int j, const int block) 
{ s[i] += s[j]; }
template<> __device__ void add<doublesingle,doublesingle>(volatile doublesingle *s, const int i, const int j, const int block) 
{ s[i] += s[j]; }

template<> __device__ void add<doublesingle2,doublesingle>(doublesingle *s, const int i, const int j, const int block) 
{ s[i] += s[j]; s[i+block] += s[j+block];}
template<> __device__ void add<doublesingle2,doublesingle>(volatile doublesingle *s, const int i, const int j, const int block) 
{ s[i] += s[j]; s[i+block] += s[j+block];}

template<> __device__ void add<doublesingle3,doublesingle>(doublesingle *s, const int i, const int j, const int block) 
{ s[i] += s[j]; s[i+block] += s[j+block]; s[i+2*block] += s[j+2*block];}
template<> __device__ void add<doublesingle3,doublesingle>(volatile doublesingle *s, const int i, const int j, const int block) 
{ s[i] += s[j]; s[i+block] += s[j+block]; s[i+2*block] += s[j+2*block];}
#endif

/**
   Generic reduction kernel with up to four loads and three saves.
 */
template <int reduce_threads, typename ReduceType, typename ReduceSimpleType, 
	  typename FloatN, int M, int writeX, int writeY, int writeZ,
	  typename InputX, typename InputY, typename InputZ, typename InputW, typename InputV,
	  typename OutputX, typename OutputY, typename OutputZ, typename Reducer>
__global__ void reduceKernel(InputX X, InputY Y, InputZ Z, InputW W, InputV V, Reducer r, 
			     ReduceType *reduce, 
			     OutputX XX, OutputY YY, OutputZ ZZ, int length) {
  unsigned int tid = threadIdx.x;
  unsigned int i = blockIdx.x*(blockDim.x) + threadIdx.x;
  unsigned int gridSize = gridDim.x*blockDim.x;

  ReduceType sum;
  zero(sum); 
  while (i < length) {
    FloatN x[M], y[M], z[M], w[M], v[M];
    X.load(x, i);
    Y.load(y, i);
    Z.load(z, i);
    W.load(w, i);
    V.load(v, i);
#pragma unroll
    for (int j=0; j<M; j++) r(sum, x[j], y[j], z[j], w[j], v[j]);

    if (writeX) XX.save(x, i);
    if (writeY) YY.save(y, i);
    if (writeZ) ZZ.save(z, i);

    i += gridSize;
  }

  extern __shared__ ReduceSimpleType sdata[];
  ReduceSimpleType *s = sdata + tid;  
  copytoshared(s, 0, sum, reduce_threads);
  __syncthreads();
  
  // do reduction in shared mem
  if (reduce_threads >= 1024) { if (tid < 512) { add<ReduceType>(s, 0, 512, reduce_threads); } __syncthreads(); }
  if (reduce_threads >= 512) { if (tid < 256) { add<ReduceType>(s, 0, 256, reduce_threads); } __syncthreads(); }
  if (reduce_threads >= 256) { if (tid < 128) { add<ReduceType>(s, 0, 128, reduce_threads); } __syncthreads(); }
  if (reduce_threads >= 128) { if (tid <  64) { add<ReduceType>(s, 0, 64, reduce_threads); } __syncthreads(); }
  
  if (tid < 32) {
    volatile ReduceSimpleType *sv = s;
    if (reduce_threads >=  64) { add<ReduceType>(sv, 0, 32, reduce_threads); }
    if (reduce_threads >=  32) { add<ReduceType>(sv, 0, 16, reduce_threads); }
    if (reduce_threads >=  16) { add<ReduceType>(sv, 0, 8, reduce_threads); }
    if (reduce_threads >=  8)  { add<ReduceType>(sv, 0, 4, reduce_threads); }
    if (reduce_threads >=  4)  { add<ReduceType>(sv, 0, 2, reduce_threads); }
    if (reduce_threads >=  2)  { add<ReduceType>(sv, 0, 1, reduce_threads); }
  }
  
  // write result for this block to global mem 
  if (tid == 0) {    
    ReduceType tmp;
    copyfromshared(tmp, s, 0, reduce_threads);
    reduce[blockIdx.x] = tmp;
  }
}

/**
   Generic reduction Kernel launcher
*/
template <typename doubleN, typename ReduceType, typename ReduceSimpleType, typename FloatN, 
	  int M, int writeX, int writeY, int writeZ, 
	  typename InputX, typename InputY, typename InputZ, typename InputW, typename InputV,
	  typename Reducer, typename OutputX, typename OutputY, typename OutputZ>
doubleN reduceLaunch(InputX X, InputY Y, InputZ Z, InputW W, InputV V, Reducer r, 
		     OutputX XX, OutputY YY, OutputZ ZZ, int length) {
  // when there is only one warp per block, we need to allocate two warps 
  // worth of shared memory so that we don't index shared memory out of bounds
  size_t smemSize = (blasBlock.x <= 32) ? blasBlock.x * 2 * sizeof(ReduceType) : 
    blasBlock.x * sizeof(ReduceType);

  ReduceType *reduce = (ReduceType*)d_reduce;
  if (blasBlock.x == 32) {
    reduceKernel<32,ReduceType,ReduceSimpleType,FloatN,M,writeX,writeY,writeZ>
      <<< blasGrid, blasBlock, smemSize >>>(X, Y, Z, W, V, r, reduce, XX, YY, ZZ, length);
  } else if (blasBlock.x == 64) {
    reduceKernel<64,ReduceType,ReduceSimpleType,FloatN,M,writeX,writeY,writeZ>
      <<< blasGrid, blasBlock, smemSize >>>(X, Y, Z, W, V, r, reduce, XX, YY, ZZ, length);
  } else if (blasBlock.x == 128) {
    reduceKernel<128,ReduceType,ReduceSimpleType,FloatN,M,writeX,writeY,writeZ>
      <<< blasGrid, blasBlock, smemSize >>>(X, Y, Z, W, V, r, reduce, XX, YY, ZZ, length);
  } else if (blasBlock.x == 256) {
    reduceKernel<256,ReduceType,ReduceSimpleType,FloatN,M,writeX,writeY,writeZ>
      <<< blasGrid, blasBlock, smemSize >>>(X, Y, Z, W, V, r, reduce, XX, YY, ZZ, length);
  } else if (blasBlock.x == 512) {
    reduceKernel<512,ReduceType,ReduceSimpleType,FloatN,M,writeX,writeY,writeZ>
      <<< blasGrid, blasBlock, smemSize >>>(X, Y, Z, W, V, r, reduce, XX, YY, ZZ, length);
  } else if (blasBlock.x == 1024) {
    reduceKernel<1024,ReduceType,ReduceSimpleType,FloatN,M,writeX,writeY,writeZ>
      <<< blasGrid, blasBlock, smemSize >>>(X, Y, Z, W, V, r, reduce, XX, YY, ZZ, length);
  } else {
    errorQuda("Reduction not implemented for %d threads", blasBlock.x);
  }

  // copy result from device to host, and perform final reduction on CPU
  cudaMemcpy(h_reduce, d_reduce, blasGrid.x*sizeof(ReduceType), cudaMemcpyDeviceToHost);

  doubleN cpu_sum;
  zero(cpu_sum);
  for (unsigned int i = 0; i < blasGrid.x; i++) {
    cpu_sum += ((ReduceType*)h_reduce)[i];
  }
  const int Nreduce = sizeof(doubleN) / sizeof(double);
  reduceDoubleArray((double*)&cpu_sum, Nreduce);

  return cpu_sum;
}

/**
   Driver for generic reduction routine with two loads.
   @param ReduceType 
 */
template <typename doubleN, typename ReduceType, typename ReduceSimpleType,
	  template <typename ReduceType, typename Float, typename FloatN> class Reducer,
	  int writeX, int writeY, int writeZ>
doubleN reduceCuda(const int kernel, const double2 &a, const double2 &b, cudaColorSpinorField &x, 
		   cudaColorSpinorField &y, cudaColorSpinorField &z, cudaColorSpinorField &w,
		   cudaColorSpinorField &v) {
  if (x.SiteSubset() == QUDA_FULL_SITE_SUBSET) {
    doubleN even =
      reduceCuda<doubleN,ReduceType,ReduceSimpleType,Reducer,writeX,writeY,writeZ>
      (kernel, a, b, x.Even(), y.Even(), z.Even(), w.Even(), v.Even());
    doubleN odd = 
      reduceCuda<doubleN,ReduceType,ReduceSimpleType,Reducer,writeX,writeY,writeZ>
      (kernel, a, b, x.Odd(), y.Odd(), z.Odd(), w.Odd(), v.Odd());
    return even + odd;
  }

  setBlock(kernel, x.Length(), x.Precision());
  checkSpinor(x, y);
  checkSpinor(x, z);
  checkSpinor(x, w);
  checkSpinor(x, v);

  if (blasGrid.x > REDUCE_MAX_BLOCKS) {
    errorQuda("reduce_core: grid size %d must be smaller than %d", blasGrid.x, REDUCE_MAX_BLOCKS);
  }

  doubleN value;
  if (x.Precision() == QUDA_DOUBLE_PRECISION) {
    Spinor<double2,double2,double2,1> X(x);
    Spinor<double2,double2,double2,1> Y(y);
    Spinor<double2,double2,double2,1> Z(z);
    Spinor<double2,double2,double2,1> W(w);
    Spinor<double2,double2,double2,1> V(v);
    Reducer<ReduceType, double2, double2> r(a,b);
    value = reduceLaunch<doubleN,ReduceType,ReduceSimpleType,double2,1,writeX,writeY,writeZ>
      (X, Y, Z, W, V, r, X, Y, Z, x.Length()/2);
  } else if (x.Precision() == QUDA_SINGLE_PRECISION) {
    Spinor<float4,float4,float4,1> X(x);
    Spinor<float4,float4,float4,1> Y(y);
    Spinor<float4,float4,float4,1> Z(z);
    Spinor<float4,float4,float4,1> W(w);
    Spinor<float4,float4,float4,1> V(v);
    Reducer<ReduceType, float2, float4> r(make_float2(a.x, a.y), make_float2(b.x, b.y));
    value = reduceLaunch<doubleN,ReduceType,ReduceSimpleType,float4,1,writeX,writeY,writeZ>
      (X, Y, Z, W, V, r, X, Y, Z, x.Length()/4);
  } else {
    if (x.Nspin() == 4){ //wilson
      SpinorTexture<float4,float4,short4,6,0> xTex(x);
      SpinorTexture<float4,float4,short4,6,1> yTex(y);
      SpinorTexture<float4,float4,short4,6,2> zTex(z);
      SpinorTexture<float4,float4,short4,6,3> wTex(w);
      SpinorTexture<float4,float4,short4,6,4> vTex(v);
      Spinor<float4,float4,short4,6> xOut(x);
      Spinor<float4,float4,short4,6> yOut(y);
      Spinor<float4,float4,short4,6> zOut(z);
      Reducer<ReduceType, float2, float4> r(make_float2(a.x, a.y), make_float2(b.x, b.y));
      value = reduceLaunch<doubleN,ReduceType,ReduceSimpleType,float4,6,writeX,writeY,writeZ>
	(xTex,yTex,zTex,wTex,vTex,r,xOut,yOut,zOut,y.Volume());
    } else if (x.Nspin() == 1) {//staggered
      SpinorTexture<float2,float2,short2,3,0> xTex(x);
      SpinorTexture<float2,float2,short2,3,1> yTex(y);
      SpinorTexture<float2,float2,short2,3,2> zTex(z);
      SpinorTexture<float2,float2,short2,3,3> wTex(w);
      SpinorTexture<float2,float2,short2,3,4> vTex(v);
      Spinor<float2,float2,short2,3> xOut(x);
      Spinor<float2,float2,short2,3> yOut(y);
      Spinor<float2,float2,short2,3> zOut(z);
      Reducer<ReduceType, float2, float2> r(make_float2(a.x, a.y), make_float2(b.x, b.y));
      value = reduceLaunch<doubleN,ReduceType,ReduceSimpleType,float2,3,writeX,writeY,writeZ>
	(xTex,yTex,zTex,wTex,vTex,r,xOut,yOut,zOut,y.Volume());
    } else { errorQuda("ERROR: nSpin=%d is not supported\n", x.Nspin()); }
    quda::blas_bytes += Reducer<ReduceType,double2,double2>::streams()*x.Volume()*sizeof(float);
  }
  quda::blas_bytes += Reducer<ReduceType,double2,double2>::streams()*x.RealLength()*x.Precision();
  quda::blas_flops += Reducer<ReduceType,double2,double2>::flops()*x.RealLength();

  if (!blasTuning) checkCudaError();

  return value;
}

/**
   Return the L2 norm of x
*/
__device__ double norm2_(const double2 &a) { return a.x*a.x + a.y*a.y; }
__device__ float norm2_(const float2 &a) { return a.x*a.x + a.y*a.y; }
__device__ float norm2_(const float4 &a) { return a.x*a.x + a.y*a.y + a.z*a.z + a.w*a.w; }

template <typename ReduceType, typename Float2, typename FloatN>
struct Norm2 {
  Norm2(const Float2 &a, const Float2 &b) { ; }
  __device__ void operator()(ReduceType &sum, const FloatN &x, FloatN &y, const FloatN &z, const FloatN &w, const FloatN &v) { sum += norm2_(x); }
  static int streams() { return 1; } //! total number of input and output streams
  static int flops() { return 2; } //! flops per element
};

double normCuda(const cudaColorSpinorField &x) {
  const int kernel = 17;
  cudaColorSpinorField &y = (cudaColorSpinorField&)x; // FIXME
  return reduceCuda<double,QudaSumFloat,QudaSumFloat,Norm2,0,0,0>
    (kernel, make_double2(0.0, 0.0), make_double2(0.0, 0.0), y, y, y, y, y);
}

/**
   Return the real dot product of x and y
*/
__device__ double dot_(const double2 &a, const double2 &b) { return a.x*b.x + a.y*b.y; }
__device__ float dot_(const float2 &a, const float2 &b) { return a.x*b.x + a.y*b.y; }
__device__ float dot_(const float4 &a, const float4 &b) { return a.x*b.x + a.y*b.y + a.z*b.z + a.w*b.w; }

template <typename ReduceType, typename Float2, typename FloatN>
struct Dot {
  Dot(const Float2 &a, const Float2 &b) { ; }
  __device__ void operator()(ReduceType &sum, const FloatN &x, const FloatN &y, const FloatN &z, 
			     const FloatN &w, const FloatN &v) { sum += dot_(x,y); }
  static int streams() { return 2; } //! total number of input and output streams
  static int flops() { return 2; } //! flops per element
};

double reDotProductCuda(cudaColorSpinorField &x, cudaColorSpinorField &y) {
  const int kernel = 18;
  return reduceCuda<double,QudaSumFloat,QudaSumFloat,Dot,0,0,0>
    (kernel, make_double2(0.0, 0.0), make_double2(0.0, 0.0), x, y, y, y, y);
}

/**
   First performs the operation y[i] = a*x[i]
   Return the norm of y
*/
template <typename ReduceType, typename Float2, typename FloatN>
struct axpyNorm2 {
  Float2 a;
  axpyNorm2(const Float2 &a, const Float2 &b) : a(a) { ; }
  __device__ void operator()(ReduceType &sum, const FloatN &x, FloatN &y, const FloatN &z, const FloatN &w, const FloatN &v) { 
    y += a.x*x; sum += norm2_(y); }
  static int streams() { return 3; } //! total number of input and output streams
  static int flops() { return 4; } //! flops per element
};

double axpyNormCuda(const double &a, cudaColorSpinorField &x, cudaColorSpinorField &y) {
  const int kernel = 19;
  return reduceCuda<double,QudaSumFloat,QudaSumFloat,axpyNorm2,0,1,0>
    (kernel, make_double2(a, 0.0), make_double2(0.0, 0.0), x, y, y, y, y);
}

/**
   First performs the operation y[i] = x[i] - y[i]
   Second returns the norm of y
*/
template <typename ReduceType, typename Float2, typename FloatN>
struct xmyNorm2 {
  xmyNorm2(const Float2 &a, const Float2 &b) { ; }
  __device__ void operator()(ReduceType &sum, const FloatN &x, FloatN &y, const FloatN &z, const FloatN &w, const FloatN &v) { 
    y = x - y; sum += norm2_(y); }
  static int streams() { return 3; } //! total number of input and output streams
  static int flops() { return 3; } //! flops per element
};

double xmyNormCuda(cudaColorSpinorField &x, cudaColorSpinorField &y) {
  const int kernel = 20;
  return reduceCuda<double,QudaSumFloat,QudaSumFloat,xmyNorm2,0,1,0>
    (kernel, make_double2(0.0, 0.0), make_double2(0.0, 0.0), x, y, y, y, y);
}

/**
  First performs the operation y[i] = a*x[i] + y[i] (complex-valued)
  Second returns the norm of y
*/
template <typename ReduceType, typename Float2, typename FloatN>
struct caxpyNorm2 {
  Float2 a;
  caxpyNorm2(const Float2 &a, const Float2 &b) : a(a) { ; }
  __device__ void operator()(ReduceType &sum, const FloatN &x, FloatN &y, const FloatN &z, const FloatN &w, const FloatN &v) { 
    caxpy_(a, x, y); sum += norm2_(y); }
  static int streams() { return 3; } //! total number of input and output streams
  static int flops() { return 6; } //! flops per element
};

double caxpyNormCuda(const quda::Complex &a, cudaColorSpinorField &x, cudaColorSpinorField &y) {
  const int kernel = 21;
  return reduceCuda<double,QudaSumFloat,QudaSumFloat,caxpyNorm2,0,1,0>
    (kernel, make_double2(a.real(), a.imag()), make_double2(0.0, 0.0), x, y, x, x, x);
}

/**
   double caxpyXmayNormCuda(float a, float *x, float *y, n){}
   
   First performs the operation y[i] = a*x[i] + y[i]
   Second performs the operator x[i] -= a*z[i]
   Third returns the norm of x
*/
template <typename ReduceType, typename Float2, typename FloatN>
struct caxpyxmaznormx {
  Float2 a;
  caxpyxmaznormx(const Float2 &a, const Float2 &b) : a(a) { ; }
  __device__ void operator()(ReduceType &sum, FloatN &x, FloatN &y, const FloatN &z, const FloatN &w, const FloatN &v) { caxpy_(a, x, y); x-= a.x*z; sum += norm2_(x); }
  static int streams() { return 5; } //! total number of input and output streams
  static int flops() { return 10; } //! flops per element
};

double caxpyXmazNormXCuda(const quda::Complex &a, cudaColorSpinorField &x, 
			  cudaColorSpinorField &y, cudaColorSpinorField &z) {
  const int kernel = 22;
  return reduceCuda<double,QudaSumFloat,QudaSumFloat,caxpyxmaznormx,1,1,0>
    (kernel, make_double2(a.real(), a.imag()), make_double2(0.0, 0.0), x, y, z, x, x);
}

/**
   double cabxpyAxNormCuda(float a, complex b, float *x, float *y, n){}
   
   First performs the operation y[i] += a*b*x[i]
   Second performs x[i] *= a
   Third returns the norm of x
*/
template <typename ReduceType, typename Float2, typename FloatN>
struct cabxpyaxnorm {
  Float2 a;
  Float2 b;
  cabxpyaxnorm(const Float2 &a, const Float2 &b) : a(a), b(b) { ; }
  __device__ void operator()(ReduceType &sum, FloatN &x, FloatN &y, const FloatN &z, const FloatN &w, const FloatN &v) { x *= a.x; caxpy_(b, x, y); sum += norm2_(y); }
  static int streams() { return 4; } //! total number of input and output streams
  static int flops() { return 10; } //! flops per element
};

double cabxpyAxNormCuda(const double &a, const quda::Complex &b, 
			cudaColorSpinorField &x, cudaColorSpinorField &y) {
  const int kernel = 23;
  return reduceCuda<double,QudaSumFloat,QudaSumFloat,cabxpyaxnorm,1,1,0>
    (kernel, make_double2(a, 0.0), make_double2(b.real(), b.imag()), x, y, x, x, x);
}

/**
   Returns complex-valued dot product of x and y
*/
__device__ double2 cdot_(const double2 &a, const double2 &b) 
{ return make_double2(a.x*b.x + a.y*b.y, a.x*b.y - a.y*b.x); }
__device__ double2 cdot_(const float2 &a, const float2 &b) 
{ return make_double2(a.x*b.x + a.y*b.y, a.x*b.y - a.y*b.x); }
__device__ double2 cdot_(const float4 &a, const float4 &b) 
{ return make_double2(a.x*b.x + a.y*b.y + a.z*b.z + a.w*b.w, a.x*b.y - a.y*b.x + a.z*b.w - a.w*b.z); }

template <typename ReduceType, typename Float2, typename FloatN>
struct Cdot {
  Cdot(const Float2 &a, const Float2 &b) { ; }
  __device__ void operator()(ReduceType &sum, const FloatN &x, FloatN &y, const FloatN &z, const FloatN &w, const FloatN &v) { sum += cdot_(x,y); }
  static int streams() { return 2; } //! total number of input and output streams
  static int flops() { return 4; } //! flops per element
};

quda::Complex cDotProductCuda(cudaColorSpinorField &x, cudaColorSpinorField &y) {
  const int kernel = 24;
  double2 cdot = reduceCuda<double2,QudaSumFloat2,QudaSumFloat,Cdot,0,0,0>
    (kernel, make_double2(0.0, 0.0), make_double2(0.0, 0.0), x, y, y, y, y);
  return quda::Complex(cdot.x, cdot.y);
}

/**
   double2 xpaycDotzyCuda(float2 *x, float a, float2 *y, float2 *z, int n) {}
   
   First performs the operation y = x + a*y
   Second returns cdot product (z,y)
*/
template <typename ReduceType, typename Float2, typename FloatN>
struct xpaycdotzy {
  Float2 a;
  xpaycdotzy(const Float2 &a, const Float2 &b) : a(a) { ; }
  __device__ void operator()(ReduceType &sum, const FloatN &x, FloatN &y, const FloatN &z, const FloatN &w, const FloatN &v) { y = x + a.x*y; sum += cdot_(z,y); }
  static int streams() { return 4; } //! total number of input and output streams
  static int flops() { return 6; } //! flops per element
};

quda::Complex xpaycDotzyCuda(cudaColorSpinorField &x, const double &a, cudaColorSpinorField &y, cudaColorSpinorField &z) {
  const int kernel = 25;
  double2 cdot = reduceCuda<double2,QudaSumFloat2,QudaSumFloat,xpaycdotzy,0,1,0>
    (kernel, make_double2(a, 0.0), make_double2(0.0, 0.0), x, y, z, y, y);
  return quda::Complex(cdot.x, cdot.y);
}

/**
   double caxpyDotzyCuda(float a, float *x, float *y, float *z, n){}
   
   First performs the operation y[i] = a*x[i] + y[i]
   Second returns the dot product (z,y)
*/
template <typename ReduceType, typename Float2, typename FloatN>
struct caxpydotzy {
  Float2 a;
   caxpydotzy(const Float2 &a, const Float2 &b) : a(a) { ; }
  __device__ void operator()(ReduceType &sum, const FloatN &x, FloatN &y, const FloatN &z, const FloatN &w, const FloatN &v) { caxpy_(a, x, y); sum += cdot_(z,y); }
  static int streams() { return 4; } //! total number of input and output streams
  static int flops() { return 8; } //! flops per element
};

quda::Complex caxpyDotzyCuda(const quda::Complex &a, cudaColorSpinorField &x, cudaColorSpinorField &y,
		       cudaColorSpinorField &z) {
  const int kernel = 26;
  double2 cdot = reduceCuda<double2,QudaSumFloat2,QudaSumFloat,caxpydotzy,0,1,0>
    (kernel, make_double2(a.real(), a.imag()), make_double2(0.0, 0.0), x, y, z, y, y);
  return quda::Complex(cdot.x, cdot.y);
}

/**
   First returns the dot product (x,y)
   Returns the norm of x
*/
__device__ double3 cdotNormA_(const double2 &a, const double2 &b) 
{ return make_double3(a.x*b.x + a.y*b.y, a.x*b.y - a.y*b.x, a.x*a.x + a.y*a.y); }
__device__ double3 cdotNormA_(const float2 &a, const float2 &b) 
{ return make_double3(a.x*b.x + a.y*b.y, a.x*b.y - a.y*b.x, a.x*a.x + a.y*a.y); }
__device__ double3 cdotNormA_(const float4 &a, const float4 &b) 
{ return make_double3(a.x*b.x + a.y*b.y + a.z*b.z + a.w*b.w, 
		      a.x*b.y - a.y*b.x + a.z*b.w - a.w*b.z,
		      a.x*a.x + a.y*a.y + a.z*a.z + a.w*a.w); }

template <typename ReduceType, typename Float2, typename FloatN>
struct CdotNormA {
  CdotNormA(const Float2 &a, const Float2 &b) { ; }
  __device__ void operator()(ReduceType &sum, const FloatN &x, FloatN &y, const FloatN &z, const FloatN &w, const FloatN &v) { sum += cdotNormA_(x,y); }
  static int streams() { return 2; } //! total number of input and output streams
  static int flops() { return 6; } //! flops per element
};

double3 cDotProductNormACuda(cudaColorSpinorField &x, cudaColorSpinorField &y) {
  const int kernel = 27;
  return reduceCuda<double3,QudaSumFloat3,QudaSumFloat,CdotNormA,0,0,0>
    (kernel, make_double2(0.0, 0.0), make_double2(0.0, 0.0), x, y, y, y, y);
}

/**
   First returns the dot product (x,y)
   Returns the norm of y
*/
__device__ double3 cdotNormB_(const double2 &a, const double2 &b) 
{ return make_double3(a.x*b.x + a.y*b.y, a.x*b.y - a.y*b.x, b.x*b.x + b.y*b.y); }
__device__ double3 cdotNormB_(const float2 &a, const float2 &b) 
{ return make_double3(a.x*b.x + a.y*b.y, a.x*b.y - a.y*b.x, b.x*b.x + b.y*b.y); }
__device__ double3 cdotNormB_(const float4 &a, const float4 &b) 
{ return make_double3(a.x*b.x + a.y*b.y + a.z*b.z + a.w*b.w, a.x*b.y - a.y*b.x + a.z*b.w - a.w*b.z,
		      b.x*b.x + b.y*b.y + b.z*b.z + b.w*b.w); }

template <typename ReduceType, typename Float2, typename FloatN>
struct CdotNormB {
  CdotNormB(const Float2 &a, const Float2 &b) { ; }
  __device__ void operator()(ReduceType &sum, const FloatN &x, FloatN &y, const FloatN &z, const FloatN &w, const FloatN &v) { sum += cdotNormB_(x,y); }
  static int streams() { return 2; } //! total number of input and output streams
  static int flops() { return 6; } //! flops per element
};

double3 cDotProductNormBCuda(cudaColorSpinorField &x, cudaColorSpinorField &y) {
  const int kernel = 28;
  return reduceCuda<double3,QudaSumFloat3,QudaSumFloat,CdotNormB,0,0,0>
    (kernel, make_double2(0.0, 0.0), make_double2(0.0, 0.0), x, y, y, y, y);
}

/**
   This convoluted kernel does the following: 
   z += a*x + b*y, y -= b*w, norm = (y,y), dot = (u, y)
 */
template <typename ReduceType, typename Float2, typename FloatN>
struct caxpbypzYmbwcDotProductUYNormY {
  Float2 a;
  Float2 b;
  caxpbypzYmbwcDotProductUYNormY(const Float2 &a, const Float2 &b) : a(a), b(b) { ; }
  __device__ void operator()(ReduceType &sum, const FloatN &x, FloatN &y, FloatN &z, const FloatN &w, const FloatN &v) { caxpy_(a, x, z); caxpy_(b, y, z); caxpy_(-b, w, y); sum += cdotNormB_(v,y); }
  static int streams() { return 7; } //! total number of input and output streams
  static int flops() { return 18; } //! flops per element
};

double3 caxpbypzYmbwcDotProductUYNormYCuda(const quda::Complex &a, cudaColorSpinorField &x, 
					   const quda::Complex &b, cudaColorSpinorField &y,
					   cudaColorSpinorField &z, cudaColorSpinorField &w,
					   cudaColorSpinorField &u) {
  const int kernel = 29;
  return reduceCuda<double3,QudaSumFloat3,QudaSumFloat,caxpbypzYmbwcDotProductUYNormY,0,1,1>
    (kernel, make_double2(a.real(), a.imag()), make_double2(b.real(), b.imag()), x, y, z, w, u);
}

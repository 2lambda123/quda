#include <stdlib.h>
#include <stdio.h>

#include <quda_internal.h>
#include <blas_quda.h>
#include <color_spinor_field.h>
#include <face_quda.h> // this is where the MPI / QMP depdendent code is

#include <cuComplex.h>

#define REDUCE_MAX_BLOCKS 65536

#define REDUCE_DOUBLE 64
#if (__COMPUTE_CAPABILITY__ >= 130)
#define REDUCE_TYPE REDUCE_DOUBLE
#define QudaSumFloat double
#define QudaSumComplex cuDoubleComplex
#define QudaSumFloat3 double3
#else
#define QudaSumFloat doublesingle
#define QudaSumComplex doublesingle2
#define QudaSumFloat3 doublesingle3
#endif

#include <double_single.h>

// These are used for reduction kernels
static QudaSumFloat *d_reduceFloat=0;
static QudaSumComplex *d_reduceComplex=0;
static QudaSumFloat3 *d_reduceFloat3=0;

static QudaSumFloat *h_reduceFloat=0;
static QudaSumComplex *h_reduceComplex=0;
static QudaSumFloat3 *h_reduceFloat3=0;

namespace quda {
  unsigned long long blas_flops;
  unsigned long long blas_bytes;
}

static dim3 blasBlock;
static dim3 blasGrid;

// generated by blas_test
#include <blas_param.h>

#include <float_vector.h>

void zeroCuda(cudaColorSpinorField &a) { a.zero(); }

// blasTuning = 1 turns off error checking
static QudaTune blasTuning = QUDA_TUNE_NO;

namespace quda {

void initBlas(void)
{  
  if (!d_reduceFloat) {
    if (cudaMalloc((void**) &d_reduceFloat, REDUCE_MAX_BLOCKS*sizeof(QudaSumFloat)) == cudaErrorMemoryAllocation) {
      errorQuda("Error allocating device reduction array");
    }
  }

  if (!d_reduceComplex) {
    if (cudaMalloc((void**) &d_reduceComplex, REDUCE_MAX_BLOCKS*sizeof(QudaSumComplex)) == cudaErrorMemoryAllocation) {
      errorQuda("Error allocating device reduction array");
    }
  }
  
  if (!d_reduceFloat3) {
    if (cudaMalloc((void**) &d_reduceFloat3, REDUCE_MAX_BLOCKS*sizeof(QudaSumFloat3)) == cudaErrorMemoryAllocation) {
      errorQuda("Error allocating device reduction array");
    }
  }

  if (!h_reduceFloat) {
    if (cudaMallocHost((void**) &h_reduceFloat, REDUCE_MAX_BLOCKS*sizeof(QudaSumFloat)) == cudaErrorMemoryAllocation) {
      errorQuda("Error allocating host reduction array");
    }
  }

  if (!h_reduceComplex) {
    if (cudaMallocHost((void**) &h_reduceComplex, REDUCE_MAX_BLOCKS*sizeof(QudaSumComplex)) == cudaErrorMemoryAllocation) {
      errorQuda("Error allocating host reduction array");
    }
  }
  
  if (!h_reduceFloat3) {
    if (cudaMallocHost((void**) &h_reduceFloat3, REDUCE_MAX_BLOCKS*sizeof(QudaSumFloat3)) == cudaErrorMemoryAllocation) {
      errorQuda("Error allocating host reduction array");
    }
  }
}


void endBlas(void)
{
  if (d_reduceFloat) {
    cudaFree(d_reduceFloat);
    d_reduceFloat = 0;
  }
  if (d_reduceComplex) {
    cudaFree(d_reduceComplex);
    d_reduceComplex = 0;
  }
  if (d_reduceFloat3) {
    cudaFree(d_reduceFloat3);
    d_reduceFloat3 = 0;
  }
  if (h_reduceFloat) {
    cudaFreeHost(h_reduceFloat);
    h_reduceFloat = 0;
  }
  if (h_reduceComplex) {
    cudaFreeHost(h_reduceComplex);
    h_reduceComplex = 0;
  }
  if (h_reduceFloat3) {
    cudaFreeHost(h_reduceFloat3);
    h_reduceFloat3 = 0;
  }
}

void setBlasTuning(QudaTune tune)
{
  blasTuning = tune;
}

void setBlasParam(int kernel, int prec, int threads, int blocks)
{
  blas_threads[kernel][prec] = threads;
  blas_blocks[kernel][prec] = blocks;
}

} 

// FIXME: this should be queried from the device
#if (__COMPUTE_CAPACITY__ < 200)
#define MAX_BLOCK 512
#else
#define MAX_BLOCK 1024
#endif

void setBlock(int kernel, int length, QudaPrecision precision)
{
  int prec;
  switch(precision) {
  case QUDA_HALF_PRECISION:
    prec = 0;
    break;
  case QUDA_SINGLE_PRECISION:
    prec = 1;
    break;
  case QUDA_DOUBLE_PRECISION:
    prec = 2;
    break;
  }

  //printf("threads %d %d %d\n", kernel, precision, blas_threads[kernel][prec]);
  blasBlock.x = min(MAX_BLOCK, blas_threads[kernel][prec]);
  blasBlock.y = 1;
  blasBlock.z = 1;

  // the min here prevents creating too big a grid
  int grid = min(blas_blocks[kernel][prec], max(length/blas_threads[kernel][prec], 1));
  blasGrid.x = grid;
  blasGrid.y = 1;
  blasGrid.z = 1;

  //printf("Setting block = (%d, %d, %d), grid = (%d, %d, %d)\n", blasBlock.x,
  //	 blasBlock.y, blasBlock.z, blasGrid.x, blasGrid.y, blasGrid.z);
}

#if (__COMPUTE_CAPABILITY__ >= 130)
__inline__ __device__ double2 fetch_double2(texture<int4, 1> t, int i)
{
  int4 v = tex1Dfetch(t,i);
  return make_double2(__hiloint2double(v.y, v.x), __hiloint2double(v.w, v.z));
}
#else
__inline__ __device__ double2 fetch_double2(texture<int4, 1> t, int i)
{
  // do nothing
  return make_double2(0.0, 0.0);
}
#endif

#include <texture.h>



float2 __device__ read_Float2(float2 *x, int i) {
  return make_float2(x[i].x, x[i].y);
}

double2 __device__ read_Float2(double2 *x, int i) {
  return make_double2(x[i].x, x[i].y);
}

#if FERMI_NO_DBLE_TEX
#define READ_DOUBLE2_TEXTURE(x, i) \
  read_Float2(x, i)
#else
#define READ_DOUBLE2_TEXTURE(x, i) \
  fetch_double2(x##TexDouble2, i)
#endif

#define READ_FLOAT2_TEXTURE(x, i) \
  tex1Dfetch(x##TexSingle2, i)

float2 __device__ make_Float2(float2 x) {
  return make_float2(x.x, x.y);
}

double2 __device__ make_Float2(double2 x) {
  return make_double2(x.x, x.y);
}

#define RECONSTRUCT_HALF_SPINOR(a, texHalf, texNorm, length)		\
  float a##c = tex1Dfetch(texNorm, i);					\
  float4 a##0 = tex1Dfetch(texHalf, i + 0*length);			\
  float4 a##1 = tex1Dfetch(texHalf, i + 1*length);			\
  float4 a##2 = tex1Dfetch(texHalf, i + 2*length);			\
  float4 a##3 = tex1Dfetch(texHalf, i + 3*length);			\
  float4 a##4 = tex1Dfetch(texHalf, i + 4*length);			\
  float4 a##5 = tex1Dfetch(texHalf, i + 5*length);			\
  a##0 *= a##c;								\
  a##1 *= a##c;								\
  a##2 *= a##c;								\
  a##3 *= a##c;								\
  a##4 *= a##c;								\
  a##5 *= a##c;

#define RECONSTRUCT_HALF_SPINOR_ST(a, texHalf, texNorm, length)		\
  float a##c = tex1Dfetch(texNorm, i);					\
  float2 a##0 = tex1Dfetch(texHalf, i + 0*length);			\
  float2 a##1 = tex1Dfetch(texHalf, i + 1*length);			\
  float2 a##2 = tex1Dfetch(texHalf, i + 2*length);			\
  (a##0) *= a##c;							        \
  (a##1) *= a##c;								\
  (a##2) *= a##c;

#define READ_HALF_SPINOR_TEX(a, tex, texNorm, length)			\
  float a##c = tex1Dfetch(texNorm, i);					\
  float4 a##0 = tex1Dfetch(tex, i + 0*length);				\
  float4 a##1 = tex1Dfetch(tex, i + 1*length);				\
  float4 a##2 = tex1Dfetch(tex, i + 2*length);				\
  float4 a##3 = tex1Dfetch(tex, i + 3*length);				\
  float4 a##4 = tex1Dfetch(tex, i + 4*length);				\
  float4 a##5 = tex1Dfetch(tex, i + 5*length);				\

#define READ_HALF_SPINOR(a, tex, length)				\
  float4 a##0 = tex1Dfetch(tex, i + 0*length);				\
  float4 a##1 = tex1Dfetch(tex, i + 1*length);				\
  float4 a##2 = tex1Dfetch(tex, i + 2*length);				\
  float4 a##3 = tex1Dfetch(tex, i + 3*length);				\
  float4 a##4 = tex1Dfetch(tex, i + 4*length);				\
  float4 a##5 = tex1Dfetch(tex, i + 5*length);				\
  float a##c = a##N[i];

#define READ_HALF_SPINOR_ST(a, tex, length)				\
  float2 a##0 = tex1Dfetch(tex, i + 0*length);				\
  float2 a##1 = tex1Dfetch(tex, i + 1*length);				\
  float2 a##2 = tex1Dfetch(tex, i + 2*length);				\
  float a##c = a##N[i];

#define FAST_ABS_MAX(a, b) fmaxf(fabsf(a), fabsf(b));
#define FAST_MAX(a, b) fmaxf(a, b);

__device__ float fast_abs_max(float4 a) {
  float c0 = FAST_ABS_MAX(a.x, a.y);
  float c1 = FAST_ABS_MAX(a.z, a.w);
  return FAST_MAX(c0, c1);
}

#define CONSTRUCT_HALF_SPINOR_FROM_SINGLE(h, n, a, length) {		\
    float c0 = fast_abs_max(a##0);					\
    float c1 = fast_abs_max(a##1);					\
    c0 = FAST_MAX(c0, c1);						\
    float c2 = fast_abs_max(a##2);					\
    float c3 = fast_abs_max(a##3);					\
    c1 = FAST_MAX(c2, c3);						\
    c0 = FAST_MAX(c0, c1);						\
    c2 = fast_abs_max(a##4);					        \
    c3 = fast_abs_max(a##5);						\
    c1 = FAST_MAX(c2, c3);						\
    c0 = FAST_MAX(c0, c1);						\
    n[i] = c0;								\
    float C = __fdividef(MAX_SHORT, c0);				\
    h[i+0*length] = make_short4((short)(C*(float)(a##0).x), (short)(C*(float)(a##0).y), \
				(short)(C*(float)(a##0).z), (short)(C*(float)(a##0).w)); \
    h[i+1*length] = make_short4((short)(C*(float)(a##1).x), (short)(C*(float)(a##1).y), \
				(short)(C*(float)(a##1).z), (short)(C*(float)(a##1).w)); \
    h[i+2*length] = make_short4((short)(C*(float)(a##2).x), (short)(C*(float)(a##2).y), \
				(short)(C*(float)(a##2).z), (short)(C*(float)(a##2).w)); \
    h[i+3*length] = make_short4((short)(C*(float)(a##3).x), (short)(C*(float)(a##3).y), \
				(short)(C*(float)(a##3).z), (short)(C*(float)(a##3).w)); \
    h[i+4*length] = make_short4((short)(C*(float)(a##4).x), (short)(C*(float)(a##4).y), \
				(short)(C*(float)(a##4).z), (short)(C*(float)(a##4).w)); \
    h[i+5*length] = make_short4((short)(C*(float)(a##5).x), (short)(C*(float)(a##5).y),	\
				(short)(C*(float)(a##5).z), (short)(C*(float)(a##5).w));}

#define CONSTRUCT_HALF_SPINOR_FROM_DOUBLE(h, n, a, length)		\
  {float c0 = fmaxf(fabsf((a##0).x), fabsf((a##0).y));			\
  float c1 = fmaxf(fabsf((a##1).x), fabsf((a##1).y));		     	\
  float c2 = fmaxf(fabsf((a##2).x), fabsf((a##2).y));			\
  float c3 = fmaxf(fabsf((a##3).x), fabsf((a##3).y));			\
  float c4 = fmaxf(fabsf((a##4).x), fabsf((a##4).y));			\
  float c5 = fmaxf(fabsf((a##5).x), fabsf((a##5).y));			\
  float c6 = fmaxf(fabsf((a##6).x), fabsf((a##6).y));			\
  float c7 = fmaxf(fabsf((a##7).x), fabsf((a##7).y));			\
  float c8 = fmaxf(fabsf((a##8).x), fabsf((a##8).y));			\
  float c9 = fmaxf(fabsf((a##9).x), fabsf((a##9).y));			\
  float c10 = fmaxf(fabsf((a##10).x), fabsf((a##10).y));		\
  float c11 = fmaxf(fabsf((a##11).x), fabsf((a##11).y));		\
  c0 = fmaxf(c0, c1); c1 = fmaxf(c2, c3);  c2 = fmaxf(c4, c5); c3 = fmaxf(c6, c7); \
  c4 = fmaxf(c8, c9); c5 = fmaxf(c10, c11); c0 = fmaxf(c0, c1); c1 = fmaxf(c2, c3); \
  c2 = fmaxf(c4, c5); c0 = fmaxf(c0, c1); c0 = fmaxf(c0, c2);		\
  n[i] = c0;								\
  float C = __fdividef(MAX_SHORT, c0);					\
  h[i+0*length] = make_short4((short)(C*(float)(a##0).x), (short)(C*(float)(a##0).y), \
			      (short)(C*(float)(a##1).x), (short)(C*(float)(a##1).y)); \
  h[i+1*length] = make_short4((short)(C*(float)(a##2).x), (short)(C*(float)(a##2).y), \
			      (short)(C*(float)(a##3).x), (short)(C*(float)(a##3).y)); \
  h[i+2*length] = make_short4((short)(C*(float)(a##4).x), (short)(C*(float)(a##4).y), \
			      (short)(C*(float)(a##5).x), (short)(C*(float)(a##5).y)); \
  h[i+3*length] = make_short4((short)(C*(float)(a##6).x), (short)(C*(float)(a##6).y), \
			      (short)(C*(float)(a##7).x), (short)(C*(float)(a##7).y)); \
  h[i+4*length] = make_short4((short)(C*(float)(a##8).x), (short)(C*(float)(a##8).y), \
			      (short)(C*(float)(a##9).x), (short)(C*(float)(a##9).y)); \
  h[i+5*length] = make_short4((short)(C*(float)(a##10).x), (short)(C*(float)(a##10).y),	\
			      (short)(C*(float)(a##11).x), (short)(C*(float)(a##11).y));}

#define CONSTRUCT_HALF_SPINOR_FROM_SINGLE_ST(h, n, a, length)		\
    {float c0 = fmaxf(fabsf((a##0).x), fabsf((a##0).y));		\
	float c1 = fmaxf(fabsf((a##1).x), fabsf((a##1).y));		\
	float c2 = fmaxf(fabsf((a##2).x), fabsf((a##2).y));		\
	c0 = fmaxf(c0, c1); c0 = fmaxf(c0, c2);				\
	n[i] = c0;							\
	float C = __fdividef(MAX_SHORT, c0);				\
	h[i+0*length] = make_short2((short)(C*(float)(a##0).x), (short)(C*(float)(a##0).y)); \
	h[i+1*length] = make_short2((short)(C*(float)(a##1).x), (short)(C*(float)(a##1).y)); \
	h[i+2*length] = make_short2((short)(C*(float)(a##2).x), (short)(C*(float)(a##2).y));}

#define CONSTRUCT_HALF_SPINOR_FROM_DOUBLE_ST(h, n, a, length)		\
    {float c0 = fmaxf(fabsf((a##0).x), fabsf((a##0).y));		\
	float c1 = fmaxf(fabsf((a##1).x), fabsf((a##1).y));		\
	float c2 = fmaxf(fabsf((a##2).x), fabsf((a##2).y));		\
	c0 = fmaxf(c0, c1); c0 = fmaxf(c0, c2);				\
	n[i] = c0;							\
	float C = __fdividef(MAX_SHORT, c0);				\
	h[i+0*length] = make_short2((short)(C*(float)(a##0).x), (short)(C*(float)(a##0).y)); \
	h[i+1*length] = make_short2((short)(C*(float)(a##1).x), (short)(C*(float)(a##1).y)); \
	h[i+2*length] = make_short2((short)(C*(float)(a##2).x), (short)(C*(float)(a##2).y));}


#define SUM_FLOAT4(sum, a)			\
  float sum = fabs(a.x) + fabs(a.y) + fabs(a.z) + fabs(a.w);

#define SUM_FLOAT2(sum, a)			\
  float sum = fabs(a.x) + fabs(a.y);

#if (__COMPUTE_CAPABILITY__ < 200) 
#define REAL_DOT_FLOAT4(dot, a, b) \
  float dot = a.x*b.x + a.y*b.y + a.z*b.z + a.w*b.w;
#else
#define REAL_DOT_FLOAT4(dot, a, b)		\
  float dot = fmaf(a.x, b.x, 0.0f);		\
  dot = fmaf(a.y, b.y, dot);			\
  dot = fmaf(a.z, b.z, dot);			\
  dot = fmaf(a.w, b.w, dot)
#endif

#define REAL_DOT_FLOAT2(dot, a, b) \
  float dot = a.x*b.x + a.y*b.y;

#if (__COMPUTE_CAPABILITY__ < 200) 
#define IMAG_DOT_FLOAT4(dot, a, b)			\
  float dot = a.x*b.y - a.y*b.x + a.z*b.w - a.w*b.z;
#else
#define IMAG_DOT_FLOAT4(dot, a, b)		\
  float dot = fmaf(a.x, b.y, 0.0f);		\
  dot = fmaf(-a.y, b.x, dot);			\
  dot = fmaf(a.z, b.w, dot);			\
  dot = fmaf(-a.w, b.z, dot)
#endif

#define IMAG_DOT_FLOAT2(dot, a, b)			\
  float dot = a.x*b.y - a.y*b.x;

#define AX_FLOAT4(a, X)				\
  X.x *= a; X.y *= a; X.z *= a; X.w *= a;

#define AX_FLOAT2(a, X)				\
  X.x *= a; X.y *= a;

#define XPY_FLOAT4(X, Y)		     \
  Y.x += X.x; Y.y += X.y; Y.z += X.z; Y.w += X.w;

#define XPY_FLOAT2(X, Y)		     \
  Y.x += X.x; Y.y += X.y;

#define XMY_FLOAT4(X, Y)		     \
  Y.x = X.x - Y.x; Y.y = X.y - Y.y; Y.z = X.z - Y.z; Y.w = X.w - Y.w;

#define XMY_FLOAT2(X, Y)		     \
  Y.x = X.x - Y.x; Y.y = X.y - Y.y;

#define MXPY_FLOAT4(X, Y)		     \
  Y.x -= X.x; Y.y -= X.y; Y.z -= X.z; Y.w -= X.w;

#define MXPY_FLOAT2(X, Y)		     \
  Y.x -= X.x; Y.y -= X.y; 

#if (__COMPUTE_CAPABILITY__ < 200) 
#define AXPY_FLOAT4(a, X, Y)		     \
  Y.x += a*X.x;	Y.y += a*X.y;		     \
  Y.z += a*X.z;	Y.w += a*X.w;
#else
#define AXPY_FLOAT4(a, X, Y)				\
  Y.x = fmaf(a, X.x, Y.x); Y.y = fmaf(a, X.y, Y.y);	\
  Y.z = fmaf(a, X.z, Y.z); Y.w = fmaf(a, X.w, Y.w);
#endif

#define AXPY_FLOAT2(a, X, Y)		     \
  Y.x += a*X.x;	Y.y += a*X.y;		     

#define AXPBY_FLOAT4(a, X, b, Y)				\
  Y.x = b*Y.x; Y.x += a*X.x; Y.y = b*Y.y; Y.y += a*X.y;		\
  Y.z = b*Y.z; Y.z += a*X.z; Y.w = b*Y.w; Y.w += a*X.w; 

#define AXPBY_FLOAT2(a, X, b, Y)			\
  Y.x = b*Y.x; Y.x += a*X.x; Y.y = b*Y.y; Y.y += a*X.y;		\

#if (__COMPUTE_CAPABILITY__ < 200)
#define XPAY_FLOAT4(X, a, Y)			     \
  Y.x = X.x + a*Y.x; Y.y = X.y + a*Y.y;		     \
  Y.z = X.z + a*Y.z; Y.w = X.w + a*Y.w;
#else
#define XPAY_FLOAT4(X, a, Y)			     \
  Y.x = fmaf(a, Y.x, X.x); Y.y = fmaf(a, Y.y, X.y);  \
  Y.z = fmaf(a, Y.z, X.z); Y.w = fmaf(a, Y.w, X.w);
#endif

#define XPAY_FLOAT2(X, a, Y)			     \
  Y.x = X.x + a*Y.x; Y.y = X.y + a*Y.y;		     

__device__ void caxpy_(const float2 &a, const float4 &x, float4 &y) {
  y.x += a.x*x.x; y.x -= a.y*x.y;
  y.y += a.y*x.x; y.y += a.x*x.y;
  y.z += a.x*x.z; y.z -= a.y*x.w;
  y.w += a.y*x.z; y.w += a.x*x.w;
}

__device__ void caxpy_(const float2 &a, const float2 &x, float2 &y) {
  y.x += a.x*x.x; y.x -= a.y*x.y;
  y.y += a.y*x.x; y.y += a.x*x.y;
}

__device__ void caxpy_(const double2 &a, const double2 &x, double2 &y) {
  y.x += a.x*x.x; y.x -= a.y*x.y;
  y.y += a.y*x.x; y.y += a.x*x.y;
}

__device__ void caxpby_(const float2 &a, const float4 &x, const float2 &b, float4 &y)					
  { float4 yy;								
  yy.x = a.x*x.x; yy.x -= a.y*x.y; yy.x += b.x*y.x; yy.x -= b.y*y.y;	
  yy.y = a.y*x.x; yy.y += a.x*x.y; yy.y += b.y*y.x; yy.y += b.x*y.y;	
  yy.z = a.x*x.z; yy.z -= a.y*x.w; yy.z += b.x*y.z; yy.z -= b.y*y.w;	
  yy.w = a.y*x.z; yy.w += a.x*x.w; yy.w += b.y*y.z; yy.w += b.x*y.w;	
  y = yy; }

__device__ void caxpby_(const float2 &a, const float2 &x, const float2 &b, float2 &y)
  { float2 yy;								
  yy.x = a.x*x.x; yy.x -= a.y*x.y; yy.x += b.x*y.x; yy.x -= b.y*y.y;	
  yy.y = a.y*x.x; yy.y += a.x*x.y; yy.y += b.y*y.x; yy.y += b.x*y.y;	
  y = yy; }

__device__ void caxpby_(const double2 &a, const double2 &x, const double2 &b, double2 &y)				 
  { double2 yy;								
  yy.x = a.x*x.x; yy.x -= a.y*x.y; yy.x += b.x*y.x; yy.x -= b.y*y.y;	
  yy.y = a.y*x.x; yy.y += a.x*x.y; yy.y += b.y*y.x; yy.y += b.x*y.y;	
  y = yy; }

#if (__COMPUTE_CAPABILITY__ < 200)
#define CAXPY_FLOAT4(a, X, Y)		\
  Y.x += a.x*X.x; Y.x -= a.y*X.y;	\
  Y.y += a.y*X.x; Y.y += a.x*X.y;	\
  Y.z += a.x*X.z; Y.z -= a.y*X.w;	\
  Y.w += a.y*X.z; Y.w += a.x*X.w;
#else
#define CAXPY_FLOAT4(a, X, Y)					\
  Y.x = fmaf(a.x, X.x, Y.x); Y.x = fmaf(-a.y, X.y, Y.x);	\
  Y.y = fmaf(a.y, X.x, Y.y); Y.y = fmaf( a.x, X.y, Y.y);	\
  Y.z = fmaf(a.x, X.z, Y.z); Y.z = fmaf(-a.y, X.w, Y.z);	\
  Y.w = fmaf(a.y, X.z, Y.w); Y.w = fmaf( a.x, X.w, Y.w);
#endif // (__COMPUTE_CAPABILITY__ < 200)

#if (__COMPUTE_CAPABILITY__ < 200)
#define CAXPY_FLOAT2(a, X, Y)		\
  Y.x += a.x*X.x; Y.x -= a.y*X.y;	\
  Y.y += a.y*X.x; Y.y += a.x*X.y;
#else
#define CAXPY_FLOAT2(a, X, Y)					\
  Y.x = fmaf(a.x, X.x, Y.x); Y.x = fmaf(-a.y, X.y, Y.x);	\
  Y.y = fmaf(a.y, X.x, Y.y); Y.y = fmaf( a.x, X.y, Y.y);
#endif // (__COMPUTE_CAPABILITY__ < 200)

#define CAXPY_DOUBLE2(a, X, Y)		\
  Y.x += a.x*X.x; Y.x -= a.y*X.y;	\
  Y.y += a.y*X.x; Y.y += a.x*X.y;	\

#define CMAXPY_FLOAT4(a, X, Y)			\
  Y.x -= a.x*X.x; Y.x += a.y*X.y;		\
  Y.y -= a.y*X.x; Y.y -= a.x*X.y;		\
  Y.z -= a.x*X.z; Y.z += a.y*X.w;		\
  Y.w -= a.y*X.z; Y.w -= a.x*X.w;

#define CMAXPY_FLOAT2(a, X, Y)			\
  Y.x -= a.x*X.x; Y.x += a.y*X.y;		\
  Y.y -= a.y*X.x; Y.y -= a.x*X.y;		

#define CAXPBY_FLOAT4(a, X, b, Y)					\
  { float2 y;								\
  y.x = a.x*X.x; y.x -= a.y*X.y; y.x += b.x*Y.x; y.x -= b.y*Y.y;	\
  y.y = a.y*X.x; y.y += a.x*X.y; y.y += b.y*Y.x; y.y += b.x*Y.y;	\
  Y.x = y.x; Y.y = y.y;							\
  y.x = a.x*X.z; y.x -= a.y*X.w; y.x += b.x*Y.z; y.x -= b.y*Y.w;	\
  y.y = a.y*X.z; y.y += a.x*X.w; y.y += b.y*Y.z; y.y += b.x*Y.w;	\
  Y.z = y.x; Y.w = y.y;}

#define CAXPBY_FLOAT2(a, X, b, Y)					\
  { float2 y;								\
  y.x = a.x*X.x; y.x -= a.y*X.y; y.x += b.x*Y.x; y.x -= b.y*Y.y;	\
  y.y = a.y*X.x; y.y += a.x*X.y; y.y += b.y*Y.x; y.y += b.x*Y.y;	\
  Y.x = y.x; Y.y = y.y;}

__device__ void cxpaypbz_(const float4 &x, const float2 &a, const float4 &y, const float2 &b, float4 &z) {
  float4 zz;
  zz.x = x.x + a.x*y.x; zz.x -= a.y*y.y; zz.x += b.x*z.x; zz.x -= b.y*z.y;
  zz.y = x.y + a.y*y.x; zz.y += a.x*y.y; zz.y += b.y*z.x; zz.y += b.x*z.y;
  zz.z = x.z + a.x*y.z; zz.z -= a.y*y.w; zz.z += b.x*z.z; zz.z -= b.y*z.w;
  zz.w = x.w + a.y*y.z; zz.w += a.x*y.w; zz.w += b.y*z.z; zz.w += b.x*z.w;
  z = zz;
}


__device__ void cxpaypbz_(const float2 &x, const float2 &a, const float2 &y, const float2 &b, float2 &z) {
  float2 zz;
  zz.x = x.x + a.x*y.x; zz.x -= a.y*y.y; zz.x += b.x*z.x; zz.x -= b.y*z.y;
  zz.y = x.y + a.y*y.x; zz.y += a.x*y.y; zz.y += b.y*z.x; zz.y += b.x*z.y;
  z = zz;
}

__device__ void cxpaypbz_(const double2 &x, const double2 &a, const double2 &y, const double2 &b, double2 &z) {
  double2 zz;
  zz.x = x.x + a.x*y.x; zz.x -= a.y*y.y; zz.x += b.x*z.x; zz.x -= b.y*z.y;
  zz.y = x.y + a.y*y.x; zz.y += a.x*y.y; zz.y += b.y*z.x; zz.y += b.x*z.y;
  z = zz;
}

__device__ void caxpbypz_(const float2 &a, const float4 &x, const float2 &b, const float4 &y, float4 &z) {
  z.x += a.x*x.x - a.y*x.y + b.x*y.x - b.y*y.y;   
  z.y += a.y*x.x + a.x*x.y + b.y*y.x + b.x*y.y;  
  z.z += a.x*x.z - a.y*x.w + b.x*y.z - b.y*y.w;  
  z.w += a.y*x.z + a.x*x.w + b.y*y.z + b.x*y.w;
}

__device__ void caxpbypz_(const float2 &a, const float2 &x, const float2 &b, const float2 &y, float2 &z) {
  z.x += a.x*x.x - a.y*x.y + b.x*y.x - b.y*y.y;   
  z.y += a.y*x.x + a.x*x.y + b.y*y.x + b.x*y.y;  
}

__device__ void caxpbypz_(const double2 &a, const double2 &x, const double2 &b, const double2 &y, double2 &z) {
  z.x += a.x*x.x - a.y*x.y + b.x*y.x - b.y*y.y;   
  z.y += a.y*x.x + a.x*x.y + b.y*y.x + b.x*y.y;  
}

#if (__COMPUTE_CAPABILITY__ < 200)
#define CAXPBYPZ_FLOAT4(a, X, b, Y, Z)		  \
  Z.x += a.x*X.x - a.y*X.y + b.x*Y.x - b.y*Y.y;   \
  Z.y += a.y*X.x + a.x*X.y + b.y*Y.x + b.x*Y.y;   \
  Z.z += a.x*X.z - a.y*X.w + b.x*Y.z - b.y*Y.w;   \
  Z.w += a.y*X.z + a.x*X.w + b.y*Y.z + b.x*Y.w;
#else
#define CAXPBYPZ_FLOAT4(a, X, b, Y, Z)				\
  Z.x = fmaf(a.x, X.x, Z.x); Z.x = fmaf(-a.y, X.y, Z.x); Z.x = fmaf(b.x, Y.x, Z.x); Z.x = fmaf(-b.y, Y.y, Z.x); \
  Z.y = fmaf(a.y, X.x, Z.y); Z.y = fmaf( a.x, X.y, Z.y); Z.y = fmaf(b.y, Y.x, Z.y); Z.y = fmaf( b.x, Y.y, Z.y); \
  Z.z = fmaf(a.x, X.z, Z.z); Z.z = fmaf(-a.y, X.w, Z.z); Z.z = fmaf(b.x, Y.z, Z.z); Z.z = fmaf(-b.y, Y.w, Z.z); \
  Z.w = fmaf(a.y, X.z, Z.w); Z.w = fmaf( a.x, X.w, Z.w); Z.w = fmaf(b.y, Y.z, Z.w); Z.w = fmaf( b.x, Y.w, Z.w);
#endif // (__COMPUTE_CAPABILITY__ < 200)

#if (__COMPUTE_CAPABILITY__ < 200)
#define CAXPBYPZ_FLOAT2(a, X, b, Y, Z)		  \
  Z.x += a.x*X.x - a.y*X.y + b.x*Y.x - b.y*Y.y;   \
  Z.y += a.y*X.x + a.x*X.y + b.y*Y.x + b.x*Y.y;
#else
#define CAXPBYPZ_FLOAT2(a, X, b, Y, Z)				\
  Z.x = fmaf(a.x, X.x, Z.x); Z.x = fmaf(-a.y, X.y, Z.x); Z.x = fmaf(b.x, Y.x, Z.x); Z.x = fmaf(-b.y, Y.y, Z.x); \
  Z.y = fmaf(a.y, X.x, Z.y); Z.y = fmaf( a.x, X.y, Z.y); Z.y = fmaf(b.y, Y.x, Z.y); Z.y = fmaf( b.x, Y.y, Z.y);
#endif // (__COMPUTE_CAPABILITY__ < 200)

// Double precision input spinor field
texture<int4, 1> xTexDouble2;
texture<int4, 1> yTexDouble2;
texture<int4, 1> zTexDouble2;
texture<int4, 1> wTexDouble2;
texture<int4, 1> uTexDouble2;

// Single precision input spinor field
texture<float2, 1> xTexSingle2;
texture<float2, 1> yTexSingle2;

texture<float4, 1> xTexSingle4;

// Half precision input spinor field
texture<short4, 1, cudaReadModeNormalizedFloat> texHalf1;
texture<short2, 1, cudaReadModeNormalizedFloat> texHalfSt1;
texture<float, 1, cudaReadModeElementType> texNorm1;

// Half precision input spinor field
texture<short4, 1, cudaReadModeNormalizedFloat> texHalf2;
texture<short2, 1, cudaReadModeNormalizedFloat> texHalfSt2;
texture<float, 1, cudaReadModeElementType> texNorm2;

// Half precision input spinor field
texture<short4, 1, cudaReadModeNormalizedFloat> texHalf3;
texture<short2, 1, cudaReadModeNormalizedFloat> texHalfSt3;
texture<float, 1, cudaReadModeElementType> texNorm3;

// Half precision input spinor field
texture<short4, 1, cudaReadModeNormalizedFloat> texHalf4;
texture<short2, 1, cudaReadModeNormalizedFloat> texHalfSt4;
texture<float, 1, cudaReadModeElementType> texNorm4;

// Half precision input spinor field
texture<short4, 1, cudaReadModeNormalizedFloat> texHalf5;
texture<short2, 1, cudaReadModeNormalizedFloat> texHalfSt5;
texture<float, 1, cudaReadModeElementType> texNorm5;

__forceinline__ __device__ float max_fabs(const float4 &c) {
  float a = fmaxf(fabsf(c.x), fabsf(c.y));
  float b = fmaxf(fabsf(c.z), fabsf(c.w));
  return fmaxf(a, b);
};

__forceinline__ __device__ float max_fabs(const float2 &b) {
  return fmaxf(fabsf(b.x), fabsf(b.y));
};

__forceinline__ __device__ float2 make_FloatN(const double2 &a) {
  return make_float2(a.x, a.y);
}

__forceinline__ __device__ float4 make_FloatN(const double4 &a) {
  return make_float4(a.x, a.y, a.z, a.w);
}

__forceinline__ __device__ double2 make_FloatN(const float2 &a) {
  return make_double2(a.x, a.y);
}

__forceinline__ __device__ double4 make_FloatN(const float4 &a) {
  return make_double4(a.x, a.y, a.z, a.w);
}

__forceinline__ __device__ short4 make_shortN(const float4 &a) {
  return make_short4(a.x, a.y, a.z, a.w);
}

__forceinline__ __device__ short2 make_shortN(const float2 &a) {
  return make_short2(a.x, a.y);
}

__forceinline__ __device__ double max_fabs(const double4 &c) {
  double a = fmaxf(fabsf(c.x), fabsf(c.y));
  double b = fmaxf(fabsf(c.z), fabsf(c.w));
  return fmaxf(a, b);
};

__forceinline__ __device__ double max_fabs(const double2 &b) {
  return fmaxf(fabsf(b.x), fabsf(b.y));
};

__forceinline__ __device__ short4 make_shortN(const double4 &a) {
  return make_short4(a.x, a.y, a.z, a.w);
}

__forceinline__ __device__ short2 make_shortN(const double2 &a) {
  return make_short2(a.x, a.y);
}


void bindTexture(const cudaColorSpinorField *x, const cudaColorSpinorField *y=0,
		 const cudaColorSpinorField *z=0, const cudaColorSpinorField *w=0, 
		 const cudaColorSpinorField *u=0)
{
  QudaPrecision precision = x->Precision();
  size_t spinor_bytes = x->Bytes() * precision;
  size_t norm_bytes = x->NormBytes();
  if (precision == QUDA_DOUBLE_PRECISION) {

    cudaBindTexture(0, xTexDouble2, x->V(), spinor_bytes); 
    if (y) cudaBindTexture(0, yTexDouble2, y->V(), spinor_bytes); 
    if (z) cudaBindTexture(0, zTexDouble2, z->V(), spinor_bytes); 
    if (w) cudaBindTexture(0, wTexDouble2, w->V(), spinor_bytes); 
    if (u) cudaBindTexture(0, uTexDouble2, u->V(), spinor_bytes); 

  } else if (precision == QUDA_SINGLE_PRECISION) {

    if (x->Nspin() == 4) {
      cudaBindTexture(0, xTexSingle4, x->V(), spinor_bytes); 
    } else if (x->Nspin() == 1) {
      cudaBindTexture(0, xTexSingle2, x->V(), spinor_bytes); 
      if (y) cudaBindTexture(0, yTexSingle2, y->V(), spinor_bytes); 
    } else {
      errorQuda("Number of spins undefined");
    }

  } else if (precision == QUDA_HALF_PRECISION) {

    if (x->Nspin() == 4){ //wilson
      cudaBindTexture(0, texHalf1, x->V(), spinor_bytes); 
      cudaBindTexture(0, texNorm1, x->Norm(), norm_bytes);    
      if (y) {
	cudaBindTexture(0, texHalf2, y->V(), spinor_bytes); 
	cudaBindTexture(0, texNorm2, y->Norm(), norm_bytes);  
      }
      if (z) {
	cudaBindTexture(0, texHalf3, z->V(), spinor_bytes); 
	cudaBindTexture(0, texNorm3, z->Norm(), norm_bytes);  
      }
      if (w) {
	cudaBindTexture(0, texHalf4, w->V(), spinor_bytes); 
	cudaBindTexture(0, texNorm4, w->Norm(), norm_bytes);  
      }
      if (u) {
	cudaBindTexture(0, texHalf5, u->V(), spinor_bytes); 
	cudaBindTexture(0, texNorm5, u->Norm(), norm_bytes);  
      }
    } else if (x->Nspin() == 1){ //staggered
      cudaBindTexture(0, texHalfSt1, x->V(), spinor_bytes); 
      cudaBindTexture(0, texNorm1, x->Norm(), norm_bytes);    
      if (y) {
	cudaBindTexture(0, texHalfSt2, y->V(), spinor_bytes); 
	cudaBindTexture(0, texNorm2, y->Norm(), norm_bytes);
      }
      if (z) {
	cudaBindTexture(0, texHalfSt3, z->V(), spinor_bytes); 
	cudaBindTexture(0, texNorm3, z->Norm(), norm_bytes);
      }
      if (w) {
	cudaBindTexture(0, texHalfSt4, w->V(), spinor_bytes); 
	cudaBindTexture(0, texNorm4, w->Norm(), norm_bytes);
      }
      if (u) {
	cudaBindTexture(0, texHalfSt5, u->V(), spinor_bytes); 
	cudaBindTexture(0, texNorm5, u->Norm(), norm_bytes);
      }
    } else {
      errorQuda("Number of spins undefined");
    }

  } else {
    errorQuda("Precision undefined");
  }
  
}


#define checkSpinor(a, b)						\
  {									\
    if (a.Precision() != b.Precision())					\
      errorQuda("precisions do not match: %d %d", a.Precision(), b.Precision()); \
    if (a.Length() != b.Length())					\
      errorQuda("lengths do not match: %d %d", a.Length(), b.Length());	\
    if (a.Stride() != b.Stride())					\
      errorQuda("strides do not match: %d %d", a.Stride(), b.Stride());	\
  }

// For kernels with precision conversion built in
#define checkSpinorLength(a, b)						\
  {									\
    if (a.Length() != b.Length()) {					\
      errorQuda("engths do not match: %d %d", a.Length(), b.Length());	\
    }									


template <typename FloatN, int N, typename Output, typename Input>
__global__ void copyKernel(Output Y, Input X, int length) {
  unsigned int i = blockIdx.x*(blockDim.x) + threadIdx.x;
  unsigned int gridSize = gridDim.x*blockDim.x;

  while (i < length) {
    FloatN x[N];
    X.load(x, i);
    Y.save(x, i);
    i += gridSize;
  }
}


void copyCuda(cudaColorSpinorField &dst, const cudaColorSpinorField &src) {

  if (&src == &dst) return; // aliasing fields
  if (src.Nspin() != 1 && src.Nspin() != 4) errorQuda("nSpin(%d) not supported\n", src.Nspin());

  if (dst.SiteSubset() == QUDA_FULL_SITE_SUBSET || src.SiteSubset() == QUDA_FULL_SITE_SUBSET) {
    copyCuda(dst.Even(), src.Even());
    copyCuda(dst.Odd(), src.Odd());
    return;
  }

  // For a given dst precision, there are two non-trivial possibilities for the
  // src precision.  The higher one corresponds to kernel index 0 (in the table
  // of block and grid dimensions), while the lower one corresponds to index 1.
  int id;
  if (src.Precision() == QUDA_DOUBLE_PRECISION ||
      dst.Precision() == QUDA_DOUBLE_PRECISION && src.Precision() == QUDA_SINGLE_PRECISION) {
    id = 0;
  } else {
    id = 1;
  }
  setBlock(id, dst.Stride(), dst.Precision());

  quda::blas_bytes += src.RealLength()*((int)src.Precision() + (int)dst.Precision());

  if (dst.Precision() == QUDA_DOUBLE_PRECISION && src.Precision() == QUDA_SINGLE_PRECISION) {
    if (src.Nspin() == 4){
      SpinorTexture<float4, float4, float4, 6, 0> src_tex(src);
      Spinor<float4, float2, double2, 6> dst_spinor(dst);
      copyKernel<float4, 6><<<blasGrid, blasBlock>>>(dst_spinor, src_tex, src.Volume());
    } else { //src.Nspin() == 1
      SpinorTexture<float2, float2, float2, 3, 0> src_tex(src);
      Spinor<float2, float2, double2, 3> dst_spinor(dst);
      copyKernel<float2, 3><<<blasGrid, blasBlock>>>(dst_spinor, src_tex, src.Volume());
    }

  } else if (dst.Precision() == QUDA_SINGLE_PRECISION && src.Precision() == QUDA_DOUBLE_PRECISION) {
    if (src.Nspin() == 4){
      SpinorTexture<float4, float2, double2, 6, 0> src_tex(src);
      Spinor<float4, float4, float4, 6> dst_spinor(dst);
      copyKernel<float4, 6><<<blasGrid, blasBlock>>>(dst_spinor, src_tex, src.Volume());
    } else { //src.Nspin() ==1
      Spinor<float2, float2, double2, 3> src_tex(src);
      Spinor<float2, float2, float2, 3> dst_spinor(dst);
      copyKernel<float2, 3><<<blasGrid, blasBlock>>>(dst_spinor, src_tex, src.Volume());
    }
  } else if (dst.Precision() == QUDA_SINGLE_PRECISION && src.Precision() == QUDA_HALF_PRECISION) {
    quda::blas_bytes += src.Volume()*sizeof(float);
    if (src.Nspin() == 4){      
      SpinorTexture<float4, float4, short4, 6, 0> src_tex(src);
      Spinor<float4, float4, float4, 6> dst_spinor(dst);
      copyKernel<float4, 6><<<blasGrid, blasBlock>>>(dst_spinor, src_tex, src.Volume());
    } else { //nSpin== 1;
      SpinorTexture<float2, float2, short2, 3, 0> src_tex(src);
      Spinor<float2, float2, float2, 3> dst_spinor(dst);
      copyKernel<float2, 3><<<blasGrid, blasBlock>>>(dst_spinor, src_tex, src.Volume());
    }
  } else if (dst.Precision() == QUDA_HALF_PRECISION && src.Precision() == QUDA_SINGLE_PRECISION) {
    quda::blas_bytes += dst.Volume()*sizeof(float);
    if (src.Nspin() == 4){
      SpinorTexture<float4, float4, float4, 6, 0> src_tex(src);
      Spinor<float4, float4, short4, 6> dst_spinor(dst);
      copyKernel<float4, 6><<<blasGrid, blasBlock>>>(dst_spinor, src_tex, src.Volume());
    } else { //nSpin == 1
      SpinorTexture<float2, float2, float2, 3, 0> src_tex(src);
      Spinor<float2, float2, short2, 3> dst_spinor(dst);
      copyKernel<float2, 3><<<blasGrid, blasBlock>>>(dst_spinor, src_tex, src.Volume());
    }
  } else if (dst.Precision() == QUDA_DOUBLE_PRECISION && src.Precision() == QUDA_HALF_PRECISION) {
    quda::blas_bytes += src.Volume()*sizeof(float);
    if (src.Nspin() == 4){
      SpinorTexture<double2, float4, short4, 12, 0> src_tex(src);
      Spinor<double2, double2, double2, 12> dst_spinor(dst);
      copyKernel<double2, 12><<<blasGrid, blasBlock>>>(dst_spinor, src_tex, src.Volume());
    } else { //nSpin == 1
      SpinorTexture<double2, float2, short2, 3, 0> src_tex(src);
      Spinor<double2, double2, double2, 3> dst_spinor(dst);
      copyKernel<double2, 3><<<blasGrid, blasBlock>>>(dst_spinor, src_tex, src.Volume());
    }
  } else if (dst.Precision() == QUDA_HALF_PRECISION && src.Precision() == QUDA_DOUBLE_PRECISION) {
    quda::blas_bytes += dst.Volume()*sizeof(float);
    if (src.Nspin() == 4){
      SpinorTexture<double2, double2, double2, 12, 0> src_tex(src);
      Spinor<double2, double4, short4, 12> dst_spinor(dst);
      copyKernel<double2, 12><<<blasGrid, blasBlock>>>(dst_spinor, src_tex, src.Volume());
    } else { //nSpin == 1
      SpinorTexture<double2, double2, double2, 3, 0> src_tex(src);
      Spinor<double2, double2, short2, 3> dst_spinor(dst);
      copyKernel<double2, 3><<<blasGrid, blasBlock>>>(dst_spinor, src_tex, src.Volume());
    }
  } else {
    cudaMemcpy(dst.V(), src.V(), dst.Bytes(), cudaMemcpyDeviceToDevice);
    if (dst.Precision() == QUDA_HALF_PRECISION) {
      cudaMemcpy(dst.Norm(), src.Norm(), dst.NormBytes(), cudaMemcpyDeviceToDevice);
      quda::blas_bytes += 2*dst.RealLength()*sizeof(float);
    }
  }
  
  if (!blasTuning) checkCudaError();
}

/**
   Generic blas kernel with two loads and one store.
 */
template <typename FloatN, int M, typename InputX, typename InputY, typename OutputY, typename Functor>
__global__ void blasKernel(InputX X, InputY Y, Functor f, OutputY YY, int length) {
  unsigned int i = blockIdx.x*(blockDim.x) + threadIdx.x;
  unsigned int gridSize = gridDim.x*blockDim.x;
  while (i < length) {
    FloatN x[M], y[M];
    X.load(x, i);
    Y.load(y, i);

#pragma unroll
    for (int j=0; j<M; j++) f(x[j], y[j]);

    YY.save(y, i);
    i += gridSize;
  }
}

/**
   Driver for generic blas routine with two loads and one store.
 */
template <template <typename Float, typename FloatN> class Functor>
void blasCuda(const int kernel, const double2 &a, cudaColorSpinorField &x, 
	      const double2 &b, cudaColorSpinorField &y) {
  setBlock(kernel, x.Length(), x.Precision());
  checkSpinor(x, y);

  if (x.SiteSubset() == QUDA_FULL_SITE_SUBSET) {
    blasCuda<Functor>(kernel, a, x.Even(), b, y.Even());
    blasCuda<Functor>(kernel, a, x.Odd(), b, y.Odd());
    return;
  }

  if (x.Precision() == QUDA_DOUBLE_PRECISION) {
    Spinor<double2,double2,double2,1> X(x);
    Spinor<double2,double2,double2,1> Y(y);
    Functor<double2, double2> f(a,b);
    blasKernel<double2,1><<<blasGrid, blasBlock>>>(X, Y, f, Y, x.Length()/2);
  } else if (x.Precision() == QUDA_SINGLE_PRECISION) {
    Spinor<float4,float4,float4,1> X(x);
    Spinor<float4,float4,float4,1> Y(y);
    Functor<float2, float4> f(make_float2(a.x, a.y), make_float2(b.x, b.y));
    blasKernel<float4,1><<<blasGrid, blasBlock>>>(X, Y, f, Y, x.Length()/4);
  } else {
    if (x.Nspin() == 4){ //wilson
      SpinorTexture<float4,float4,short4,6,0> xTex(x);
      SpinorTexture<float4,float4,short4,6,1> yTex(y);
      Spinor<float4,float4,short4,6> yStore(y);
      Functor<float2, float4> f(make_float2(a.x, a.y), make_float2(b.x, b.y));
      blasKernel<float4, 6> <<<blasGrid, blasBlock>>> (xTex, yTex, f, yStore, y.Volume());
    } else if (x.Nspin() == 1) {//staggered
      SpinorTexture<float2,float2,short2,3,0> xTex(x);
      SpinorTexture<float2,float2,short2,3,1> yTex(y);
      Spinor<float2,float2,short2,3> yStore(y);
      Functor<float2, float2> f(make_float2(a.x, a.y), make_float2(b.x, b.y));
      blasKernel<float2, 3> <<<blasGrid, blasBlock>>>(xTex, yTex, f, yStore, y.Volume());
    } else { errorQuda("ERROR: nSpin=%d is not supported\n", x.Nspin()); }
    quda::blas_bytes += Functor<double2,double2>::streams()*x.Volume()*sizeof(float);
  }
  quda::blas_bytes += Functor<double2,double2>::streams()*x.RealLength()*x.Precision();
  quda::blas_flops += Functor<double2,double2>::flops()*x.RealLength();

  if (!blasTuning) checkCudaError();
}

/**
   Functor to perform the operation y = a*x + b*y
*/
template <typename Float2, typename FloatN>
struct axpby {
  const Float2 a;
  const Float2 b;
  axpby(const Float2 &a, const Float2 &b) : a(a), b(b) { ; }
  __device__ void operator()(const FloatN &x, FloatN &y) { y = a.x*x + b.x*y; }
  static int streams() { return 3; } //! total number of input and output streams
  static int flops() { return 3; } //! flops per element
};

void axpbyCuda(const double &a, cudaColorSpinorField &x, const double &b, cudaColorSpinorField &y) {
  const int kernel = 2;
  blasCuda<axpby>(kernel, make_double2(a, 0.0), x, make_double2(b, 0.0), y);
}

template <typename Float2, typename FloatN>
struct xpy {
  xpy(const Float2 &a, const Float2 &b) { ; }
  __device__ void operator()(const FloatN &x, FloatN &y) { y += x ; }
  static int streams() { return 3; } //! total number of input and output streams
  static int flops() { return 1; } //! flops per element
};

void xpyCuda(cudaColorSpinorField &x, cudaColorSpinorField &y) {
  const int kernel = 3;
  blasCuda<xpy>(kernel, make_double2(1.0, 0.0), x, make_double2(1.0, 0.0), y);
}

template <typename Float2, typename FloatN>
struct axpy {
  const Float2 a;
  axpy(const Float2 &a, const Float2 &b) : a(a) { ; }
  __device__ void operator()(const FloatN &x, FloatN &y) { y = a.x*x + y; }
  static int streams() { return 3; } //! total number of input and output streams
  static int flops() { return 2; } //! flops per element
};

void axpyCuda(const double &a, cudaColorSpinorField &x, cudaColorSpinorField &y) {
  const int kernel = 4;
  blasCuda<axpy>(kernel, make_double2(a, 0.0), x, make_double2(1.0, 0.0), y);
}

template <typename Float2, typename FloatN>
struct xpay {
  const Float2 a;
  xpay(const Float2 &a, const Float2 &b) : a(a) { ; }
  __device__ void operator()(const FloatN &x, FloatN &y) { y = x + a.x*y; }
  static int streams() { return 3; } //! total number of input and output streams
  static int flops() { return 2; } //! flops per element
};

void xpayCuda(cudaColorSpinorField &x, const double &a, cudaColorSpinorField &y) {
  const int kernel = 5;
  blasCuda<xpay>(kernel, make_double2(a,0.0), x, make_double2(0.0, 0.0), y);
}

template <typename Float2, typename FloatN>
struct mxpy {
  mxpy(const Float2 &a, const Float2 &b) { ; }
  __device__ void operator()(const FloatN &x, FloatN &y) { y -= x; }
  static int streams() { return 3; } //! total number of input and output streams
  static int flops() { return 1; } //! flops per element
};

void mxpyCuda(cudaColorSpinorField &x, cudaColorSpinorField &y) {
  const int kernel = 6;
  blasCuda<mxpy>(kernel, make_double2(1.0, 0.0), x, make_double2(1.0, 0.0), y);
}

template <typename Float2, typename FloatN>
struct ax {
  const Float2 a;
  ax(const Float2 &a, const Float2 &b) : a(a) { ; }
  __device__ void operator()(const FloatN &x, FloatN &y) { y *= a.x; }
  static int streams() { return 2; } //! total number of input and output streams
  static int flops() { return 1; } //! flops per element
};

void axCuda(const double &a, cudaColorSpinorField &x) {
  const int kernel = 7;
  blasCuda<ax>(kernel, make_double2(a, 0.0), x, make_double2(0.0, 0.0), x);
}

template <typename Float2, typename FloatN>
struct caxpy {
  const Float2 a;
  caxpy(const Float2 &a, const Float2 &b) : a(a) { ; }
  __device__ void operator()(const FloatN &x, FloatN &y) { caxpy_(a, x, y); }
  static int streams() { return 3; } //! total number of input and output streams
  static int flops() { return 4; } //! flops per element
};

void caxpyCuda(const quda::Complex &a, cudaColorSpinorField &x, cudaColorSpinorField &y) {
  const int kernel = 8;
  blasCuda<caxpy>(kernel, make_double2(real(a),imag(a)), x, make_double2(0.0, 0.0), y);
}

template <typename Float2, typename FloatN>
struct caxpby {
  const Float2 a;
  const Float2 b;
  caxpby(const Float2 &a, const Float2 &b) : a(a), b(b) { ; }
  __device__ void operator()(const FloatN &x, FloatN &y) { caxpby_(a, x, b, y); }
  static int streams() { return 3; } //! total number of input and output streams
  static int flops() { return 7; } //! flops per element
};

void caxpbyCuda(const quda::Complex &a, cudaColorSpinorField &x, const quda::Complex &b, cudaColorSpinorField &y) {
  const int kernel = 9;
  blasCuda<caxpby>(kernel, make_double2(a.real(),a.imag()), x, make_double2(b.real(), b.imag()), y);
}


/**
   Generic blas kernel with three loads and two stores.
 */
template <typename FloatN, int M, typename InputX, typename InputY, typename InputZ, 
	  typename InputW, typename OutputY, typename OutputZ, typename Functor>
__global__ void blasKernel(InputX X, InputY Y, InputZ Z, InputW W, Functor f, 
			   OutputY YY, OutputZ ZZ, int length) {
  unsigned int i = blockIdx.x*(blockDim.x) + threadIdx.x;
  unsigned int gridSize = gridDim.x*blockDim.x;
  while (i < length) {
    FloatN x[M], y[M], z[M], w[M];
    X.load(x, i);
    Y.load(y, i);
    Z.load(z, i);
    W.load(w, i);

#pragma unroll
    for (int j=0; j<M; j++) f(x[j], y[j], z[j], w[j]);

    YY.save(y, i);
    ZZ.save(z, i);
    i += gridSize;
  }
}

/**
   Driver for generic blas routine with four loads and two store.
 */
template <template <typename Float, typename FloatN> class Functor>
void blasCuda(const int kernel, const double2 &a, const double2 &b, const double2 &c,
	      cudaColorSpinorField &x, cudaColorSpinorField &y, 
	      cudaColorSpinorField &z, cudaColorSpinorField &w) {
  setBlock(kernel, x.Length(), x.Precision());
  checkSpinor(x, y);
  checkSpinor(x, z);
  checkSpinor(x, w);

  if (x.SiteSubset() == QUDA_FULL_SITE_SUBSET) {
    blasCuda<Functor>(kernel, a, b, c, x.Even(), y.Even(), z.Even(), w.Even());
    blasCuda<Functor>(kernel, a, b, c, x.Odd(), y.Odd(), z.Odd(), w.Even());
    return;
  }

  if (x.Precision() == QUDA_DOUBLE_PRECISION) {
    Spinor<double2,double2,double2,1> X(x);
    Spinor<double2,double2,double2,1> Y(y);
    Spinor<double2,double2,double2,1> Z(z);
    Spinor<double2,double2,double2,1> W(w);
    Functor<double2, double2> f(a,b,c);
    blasKernel<double2,1><<<blasGrid, blasBlock>>>(X, Y, Z, W, f, Y, Z, x.Length()/2);
  } else if (x.Precision() == QUDA_SINGLE_PRECISION) {
    Spinor<float4,float4,float4,1> X(x);
    Spinor<float4,float4,float4,1> Y(y);
    Spinor<float4,float4,float4,1> Z(z);
    Spinor<float4,float4,float4,1> W(w);
    Functor<float2, float4> f(make_float2(a.x, a.y), make_float2(b.x, b.y), make_float2(c.x, c.y));
    blasKernel<float4,1><<<blasGrid, blasBlock>>>(X, Y, Z, W, f, Y, Z, x.Length()/4);
  } else {
    if (x.Nspin() == 4){ //wilson
      SpinorTexture<float4,float4,short4,6,0> xTex(x);
      SpinorTexture<float4,float4,short4,6,1> yTex(y);
      SpinorTexture<float4,float4,short4,6,2> zTex(z);
      SpinorTexture<float4,float4,short4,6,2> wTex(w);
      Spinor<float4,float4,short4,6> yStore(y);
      Spinor<float4,float4,short4,6> zStore(z);
      Functor<float2, float4> f(make_float2(a.x, a.y), make_float2(b.x, b.y), make_float2(c.x, c.y));
      blasKernel<float4, 6> <<<blasGrid, blasBlock>>> (xTex, yTex, zTex, wTex, f, yStore, zStore, y.Volume());
    } else if (x.Nspin() == 1) {//staggered
      SpinorTexture<float2,float2,short2,3,0> xTex(x);
      SpinorTexture<float2,float2,short2,3,1> yTex(y);
      SpinorTexture<float2,float2,short2,3,2> zTex(z);
      SpinorTexture<float2,float2,short2,3,2> wTex(w);
      Spinor<float2,float2,short2,3> yStore(y);
      Spinor<float2,float2,short2,3> zStore(z);
      Functor<float2, float2> f(make_float2(a.x, a.y), make_float2(b.x, b.y), make_float2(c.x, c.y));
      blasKernel<float2, 3> <<<blasGrid, blasBlock>>>(xTex, yTex, zTex, wTex, f, yStore, zStore, y.Volume());
    } else { errorQuda("ERROR: nSpin=%d is not supported\n", x.Nspin()); }
    quda::blas_bytes += Functor<double2,double2>::streams()*x.Volume()*sizeof(float);
  }
  quda::blas_bytes += Functor<double2,double2>::streams()*x.RealLength()*x.Precision();
  quda::blas_flops += Functor<double2,double2>::flops()*x.RealLength();

  if (!blasTuning) checkCudaError();
}

/**
   Performs the operation z[i] = x[i] + a*y[i] + b*z[i]
*/
template <typename Float2, typename FloatN>
struct cxpaypbz {
  const Float2 a;
  const Float2 b;
  const Float2 c;
  cxpaypbz(const Float2 &a, const Float2 &b, const Float2 &c) : a(a), b(b), c(c) { ; }
  __device__ void operator()(const FloatN &x, const FloatN &y, FloatN &z, FloatN &w) 
  { cxpaypbz_(x, a, y, b, z); }
  static int streams() { return 4; } //! total number of input and output streams
  static int flops() { return 8; } //! flops per element
};

void cxpaypbzCuda(cudaColorSpinorField &x, const quda::Complex &a, cudaColorSpinorField &y, 
		  const quda::Complex &b, cudaColorSpinorField &z) {
  const int kernel = 10;
  blasCuda<cxpaypbz>(kernel, make_double2(a.real(),a.imag()), make_double2(b.real(), b.imag()), 
		     make_double2(0.0, 0.0), x, y, z, z);
}

/**
   Performs the operations: y[i] = a*x[i] + y[i]; x[i] = b*z[i] + c*x[i]
*/
template <typename Float2, typename FloatN>
struct axpyBzpcx {
  const Float2 a;
  const Float2 b;
  const Float2 c;
  axpyBzpcx(const Float2 &a, const Float2 &b, const Float2 &c) : a(a), b(b), c(c) { ; }
  __device__ void operator()(const FloatN &x, FloatN &y, FloatN &z, FloatN &w)
  { y += a.x*z; z = b.x*x + c.x*z; }
  static int streams() { return 5; } //! total number of input and output streams
  static int flops() { return 10; } //! flops per element
};

void axpyBzpcxCuda(const double &a, cudaColorSpinorField& x, cudaColorSpinorField& y, const double &b, 
		   cudaColorSpinorField& z, const double &c) {
  const int kernel = 11;
  // swap arguments around 
  blasCuda<axpyBzpcx>(kernel, make_double2(a,0.0), make_double2(b,0.0), make_double2(c,0.0), 
		      z, y, x, x);
}

/**
   Performs the operations: {y[i] = a*x[i] + y[i]; x[i] = z[i] + b*x[i]}
*/
template <typename Float2, typename FloatN>
struct axpyZpbx {
  const Float2 a;
  const Float2 b;
  axpyZpbx(const Float2 &a, const Float2 &b, const Float2 &c) : a(a), b(b) { ; }
  __device__ void operator()(const FloatN &x, FloatN &y, FloatN &z, FloatN &w)
  { y += a.x*z; z = x + b.x*z; }
  static int streams() { return 5; } //! total number of input and output streams
  static int flops() { return 8; } //! flops per element
};

void axpyZpbxCuda(const double &a, cudaColorSpinorField& x, cudaColorSpinorField& y,
		  cudaColorSpinorField& z, const double &b) {
  const int kernel = 12;
  // swap arguments around 
  blasCuda<axpyZpbx>(kernel, make_double2(a,0.0), make_double2(b,0.0), make_double2(0.0,0.0),
		     z, y, x, x);
}

/**
   Performs the operation z[i] = a*x[i] + b*y[i] + z[i] and y[i] -= b*w[i]
*/
template <typename Float2, typename FloatN>
struct caxpbypzYmbw {
  const Float2 a;
  const Float2 b;
  caxpbypzYmbw(const Float2 &a, const Float2 &b, const Float2 &c) : a(a), b(b) { ; }
  __device__ void operator()(const FloatN &x, FloatN &y, FloatN &z, FloatN &w)
  { caxpbypz_(a, x, b, y, z); caxpy_(-b, w, y); }

  static int streams() { return 6; } //! total number of input and output streams
  static int flops() { return 12; } //! flops per element
};

void caxpbypzYmbwCuda(const quda::Complex &a, cudaColorSpinorField &x, const quda::Complex &b, 
		      cudaColorSpinorField &y, cudaColorSpinorField &z, cudaColorSpinorField &w) {
  const int kernel = 12;
  // swap arguments around 
  blasCuda<caxpbypzYmbw>(kernel, make_double2(a.real(),a.imag()), make_double2(a.real(), b.imag()), 
			 make_double2(0.0,0.0), x, y, z, w);
}

/**
   Performs the operation y[i] += a*b*x[i], x[i] *= a
*/
template <typename Float2, typename FloatN>
struct cabxpyAx {
  const Float2 a;
  const Float2 b;
  cabxpyAx(const Float2 &a, const Float2 &b, const Float2 &c) : a(a), b(b) { ; }
  __device__ void operator()(const FloatN &x, FloatN &y, FloatN &z, FloatN &w) 
  { z *= a.x; caxpy_(b, z, y); }
  static int streams() { return 4; } //! total number of input and output streams
  static int flops() { return 5; } //! flops per element
};

void cabxpyAxCuda(const double &a, const quda::Complex &b, 
		  cudaColorSpinorField &x, cudaColorSpinorField &y) {
  const int kernel = 14;
  // swap arguments around 
  blasCuda<cabxpyAx>(kernel, make_double2(a,0.0), make_double2(b.real(),b.imag()), 
		     make_double2(0.0,0.0), x, y, x, x);
}

/**
   Performs the operation z[i] = a*x[i] + b*y[i] + z[i]
*/
template <typename Float2, typename FloatN>
struct caxpbypz {
  const Float2 a;
  const Float2 b;
  caxpbypz(const Float2 &a, const Float2 &b, const Float2 &c) : a(a), b(b) { ; }
  __device__ void operator()(const FloatN &x, FloatN &y, FloatN &z, FloatN &w) 
  { caxpbypz_(a, x, b, y, z); }
  static int streams() { return 4; } //! total number of input and output streams
  static int flops() { return 5; } //! flops per element
};

void caxpbypzCuda(const quda::Complex &a, cudaColorSpinorField &x, const quda::Complex &b, 
		  cudaColorSpinorField &y, cudaColorSpinorField &z) {
  const int kernel = 15;
  blasCuda<caxpbypz>(kernel, make_double2(a.real(),a.imag()), make_double2(b.real(),b.imag()), 
		     make_double2(0.0,0.0), x, y, z, z);
}

/**
   Performs the operation z[i] = a*x[i] + b*y[i] + c*z[i] + w[i]
*/
template <typename Float2, typename FloatN>
struct caxpbypczpw {
  const Float2 a;
  const Float2 b;
  const Float2 c;
  caxpbypczpw(const Float2 &a, const Float2 &b, const Float2 &c) : a(a), b(b), c(c) { ; }
  __device__ void operator()(const FloatN &x, FloatN &y, FloatN &z, FloatN &w) 
  { caxpy_(a, x, w); caxpy_(b, y, w); caxpy_(c, z, w); }

  static int streams() { return 4; } //! total number of input and output streams
  static int flops() { return 5; } //! flops per element
};

void caxpbypczpwCuda(const quda::Complex &a, cudaColorSpinorField &x, const quda::Complex &b, 
		     cudaColorSpinorField &y, const quda::Complex &c, cudaColorSpinorField &z, 
		     cudaColorSpinorField &w) {
  const int kernel = 16;
  // swap arguments around 
  blasCuda<caxpbypczpw>(kernel, make_double2(a.real(),a.imag()), make_double2(b.real(),b.imag()), 
			make_double2(c.real(), c.imag()), x, y, z, w);
}

__device__ double norm2(double a) { return a*a; }
__device__ float norm2(float2 a) { return a.x*a.x + a.y*a.y; }
__device__ float norm2(float4 a) { return a.x*a.x + a.y*a.y + a.z*a.z + a.w*a.w; }

template <unsigned int reduce_threads, typename Float>
#define REDUCE_FUNC_NAME(suffix) norm##suffix
#define REDUCE_TYPES Float *a
#define REDUCE_PARAMS a
#define REDUCE_AUXILIARY(i)
#define REDUCE_OPERATION(i) norm2(a[i])
#include "reduce_core.h"
#undef REDUCE_FUNC_NAME
#undef REDUCE_TYPES
#undef REDUCE_PARAMS
#undef REDUCE_AUXILIARY
#undef REDUCE_OPERATION

//
// double normHCuda(char *, int n) {}
//
template <unsigned int reduce_threads, typename Float>
#define REDUCE_FUNC_NAME(suffix) normH##suffix
#define REDUCE_TYPES Float *aN, int stride // dummy type
#define REDUCE_PARAMS aN, stride
#define REDUCE_AUXILIARY(i)						\
  READ_HALF_SPINOR(a, texHalf1, stride);				\
  REAL_DOT_FLOAT4(norm0, a0, a0);					\
  REAL_DOT_FLOAT4(norm1, a1, a1);					\
  REAL_DOT_FLOAT4(norm2, a2, a2);					\
  REAL_DOT_FLOAT4(norm3, a3, a3);					\
  REAL_DOT_FLOAT4(norm4, a4, a4);					\
  REAL_DOT_FLOAT4(norm5, a5, a5);					\
  norm0 += norm1; norm2 += norm3; norm4 += norm5; norm0 += norm2, norm0 += norm4;
#define REDUCE_OPERATION(i) (ac*ac*norm0)
#include "reduce_core.h"
#undef REDUCE_FUNC_NAME
#undef REDUCE_TYPES
#undef REDUCE_PARAMS
#undef REDUCE_AUXILIARY
#undef REDUCE_OPERATION

template <unsigned int reduce_threads, typename Float>
#define REDUCE_FUNC_NAME(suffix) normHSt##suffix
#define REDUCE_TYPES Float *aN, int stride // dummy type
#define REDUCE_PARAMS aN, stride
#define REDUCE_AUXILIARY(i)						\
    READ_HALF_SPINOR_ST(a, texHalfSt1, stride);				\
    REAL_DOT_FLOAT2(norm0, a0, a0);					\
    REAL_DOT_FLOAT2(norm1, a1, a1);					\
    REAL_DOT_FLOAT2(norm2, a2, a2);					\
    norm0 += norm1; norm0 += norm2; 
#define REDUCE_OPERATION(i) (ac*ac*norm0)
#include "reduce_core.h"
#undef REDUCE_FUNC_NAME
#undef REDUCE_TYPES
#undef REDUCE_PARAMS
#undef REDUCE_AUXILIARY
#undef REDUCE_OPERATION

double normCuda(const cudaColorSpinorField &a) {
  if (a.SiteSubset() == QUDA_FULL_SITE_SUBSET) return normCuda(a.Even()) + normCuda(a.Odd());
  const int id = 17;
  quda::blas_flops += 2*a.RealLength();
  quda::blas_bytes += a.RealLength()*a.Precision();
  if (a.Precision() == QUDA_DOUBLE_PRECISION) {
    return normCuda((double*)a.V(), a.Length(), id, a.Precision());
  } else if (a.Precision() == QUDA_SINGLE_PRECISION) {
    return normCuda((float2*)a.V(), a.Length()/2, id, a.Precision());
  } else {
    bindTexture(&a);
    quda::blas_bytes += (a.RealLength()*a.Precision()) / (a.Ncolor() * a.Nspin());
    if (a.Nspin() == 4){ //wilson
      return normHCuda((float*)a.Norm(), a.Stride(), a.Volume(), id, a.Precision());
    }else if (a.Nspin() == 1) { //staggered
      return normHStCuda((float*)a.Norm(), a.Stride(), a.Volume(), id, a.Precision());	
    }else{
      errorQuda("%s: nSpin(%d) is not supported\n", __FUNCTION__, a.Nspin());
      return 0;
    }
  }

}

__device__ double dot(double a, double b) { return a*b; }
__device__ float dot(float2 a, float2 b) { return a.x*b.x + a.y*b.y; }

//
// double reDotProductFCuda(float *a, float *b, int n) {}
//
template <unsigned int reduce_threads, typename Float>
#define REDUCE_FUNC_NAME(suffix) reDotProduct##suffix
#define REDUCE_TYPES Float *a, Float *b
#define REDUCE_PARAMS a, b
#define REDUCE_AUXILIARY(i)
#define REDUCE_OPERATION(i) dot(a[i], b[i])
#include "reduce_core.h"
#undef REDUCE_FUNC_NAME
#undef REDUCE_TYPES
#undef REDUCE_PARAMS
#undef REDUCE_AUXILIARY
#undef REDUCE_OPERATION

//
// double reDotProductHCuda(float *a, float *b, int n) {}
//
template <unsigned int reduce_threads, typename Float>
#define REDUCE_FUNC_NAME(suffix) reDotProductH##suffix
#define REDUCE_TYPES Float *aN, Float *bN, int stride
#define REDUCE_PARAMS aN, bN, stride
#define REDUCE_AUXILIARY(i)						\
  READ_HALF_SPINOR(a, texHalf1, stride);				\
  READ_HALF_SPINOR(b, texHalf2, stride);				\
  REAL_DOT_FLOAT4(rdot0, a0, b0);					\
  REAL_DOT_FLOAT4(rdot1, a1, b1);					\
  REAL_DOT_FLOAT4(rdot2, a2, b2);					\
  REAL_DOT_FLOAT4(rdot3, a3, b3);					\
  REAL_DOT_FLOAT4(rdot4, a4, b4);					\
  REAL_DOT_FLOAT4(rdot5, a5, b5);					\
  rdot0 += rdot1; rdot2 += rdot3; rdot4 += rdot5; rdot0 += rdot2; rdot0 += rdot4;
#define REDUCE_OPERATION(i) (ac*bc*rdot0)
#include "reduce_core.h"
#undef REDUCE_FUNC_NAME
#undef REDUCE_TYPES
#undef REDUCE_PARAMS
#undef REDUCE_AUXILIARY
#undef REDUCE_OPERATION

template <unsigned int reduce_threads, typename Float>
#define REDUCE_FUNC_NAME(suffix) reDotProductHSt##suffix
#define REDUCE_TYPES Float *aN, Float *bN, int stride
#define REDUCE_PARAMS aN, bN, stride
#define REDUCE_AUXILIARY(i)						\
  READ_HALF_SPINOR_ST(a, texHalfSt1, stride);				\
  READ_HALF_SPINOR_ST(b, texHalfSt2, stride);				\
  REAL_DOT_FLOAT2(rdot0, a0, b0);					\
  REAL_DOT_FLOAT2(rdot1, a1, b1);					\
  REAL_DOT_FLOAT2(rdot2, a2, b2);					\
  rdot0 += rdot1; rdot0 += rdot2; 
#define REDUCE_OPERATION(i) (ac*bc*rdot0)
#include "reduce_core.h"
#undef REDUCE_FUNC_NAME
#undef REDUCE_TYPES
#undef REDUCE_PARAMS
#undef REDUCE_AUXILIARY
#undef REDUCE_OPERATION  

double reDotProductCuda(cudaColorSpinorField &a, cudaColorSpinorField &b) {
  if (a.SiteSubset() == QUDA_FULL_SITE_SUBSET) {
    return reDotProductCuda(a.Even(), b.Even()) + reDotProductCuda(a.Odd(), b.Odd());
  }
  const int id = 18;
  quda::blas_flops += 2*a.RealLength();
  checkSpinor(a, b);
  quda::blas_bytes += 2*a.RealLength()*a.Precision();
  if (a.Precision() == QUDA_DOUBLE_PRECISION) {
    return reDotProductCuda((double*)a.V(), (double*)b.V(), a.Length(), id, a.Precision());
  } else if (a.Precision() == QUDA_SINGLE_PRECISION) {
    return reDotProductCuda((float2*)a.V(), (float2*)b.V(), a.Length()/2, id, a.Precision());
  } else {
    quda::blas_bytes += 2*a.Volume()*sizeof(float);
    bindTexture(&a, &b);
    if (a.Nspin() == 4){ //wilson
      return reDotProductHCuda((float*)a.Norm(), (float*)b.Norm(), a.Stride(), a.Volume(), id, a.Precision());
    }else if (a.Nspin() == 1){ //staggered
      return reDotProductHStCuda((float*)a.Norm(), (float*)b.Norm(), a.Stride(), a.Volume(), id, a.Precision());
    }else{
      errorQuda("%s: nSpin(%d) is not supported\n", __FUNCTION__, a.Nspin());      
      return 0;
    }
  }

}

//
// double axpyNormCuda(float a, float *x, float *y, n){}
//
// First performs the operation y[i] = a*x[i] + y[i]
// Second returns the norm of y
//

template <unsigned int reduce_threads, typename Float>
#define REDUCE_FUNC_NAME(suffix) axpyNormF##suffix
#define REDUCE_TYPES Float a, Float *x, Float *y
#define REDUCE_PARAMS a, x, y
#define REDUCE_AUXILIARY(i) y[i] = a*x[i] + y[i]
#define REDUCE_OPERATION(i) (y[i]*y[i])
#include "reduce_core.h"
#undef REDUCE_FUNC_NAME
#undef REDUCE_TYPES
#undef REDUCE_PARAMS
#undef REDUCE_AUXILIARY
#undef REDUCE_OPERATION

template <unsigned int reduce_threads, typename Float>
#define REDUCE_FUNC_NAME(suffix) axpyNormH##suffix
#define REDUCE_TYPES Float a, short4 *yH, float *yN, int stride
#define REDUCE_PARAMS a, yH, yN, stride
#define REDUCE_AUXILIARY(i)						\
  RECONSTRUCT_HALF_SPINOR(x, texHalf1, texNorm1, stride);		\
  RECONSTRUCT_HALF_SPINOR(y, texHalf2, texNorm2, stride);		\
  AXPY_FLOAT4(a, x0, y0);						\
  REAL_DOT_FLOAT4(norm0, y0, y0);					\
  AXPY_FLOAT4(a, x1, y1);						\
  REAL_DOT_FLOAT4(norm1, y1, y1);					\
  AXPY_FLOAT4(a, x2, y2);						\
  REAL_DOT_FLOAT4(norm2, y2, y2);					\
  AXPY_FLOAT4(a, x3, y3);						\
  REAL_DOT_FLOAT4(norm3, y3, y3);					\
  AXPY_FLOAT4(a, x4, y4);						\
  REAL_DOT_FLOAT4(norm4, y4, y4);					\
  AXPY_FLOAT4(a, x5, y5);						\
  REAL_DOT_FLOAT4(norm5, y5, y5);					\
  norm0 += norm1; norm2 += norm3; norm4 += norm5; norm0 += norm2; norm0 += norm4; \
  CONSTRUCT_HALF_SPINOR_FROM_SINGLE(yH, yN, y, stride);
#define REDUCE_OPERATION(i) (norm0)
#include "reduce_core.h"
#undef REDUCE_FUNC_NAME
#undef REDUCE_TYPES
#undef REDUCE_PARAMS
#undef REDUCE_AUXILIARY
#undef REDUCE_OPERATION

template <unsigned int reduce_threads, typename Float>
#define REDUCE_FUNC_NAME(suffix) axpyNormH##suffix
#define REDUCE_TYPES Float a, short2 *yH, float *yN, int stride
#define REDUCE_PARAMS a, yH, yN, stride
#define REDUCE_AUXILIARY(i)						\
  RECONSTRUCT_HALF_SPINOR_ST(x, texHalfSt1, texNorm1, stride);		\
  RECONSTRUCT_HALF_SPINOR_ST(y, texHalfSt2, texNorm2, stride);		\
  AXPY_FLOAT2(a, x0, y0);						\
  REAL_DOT_FLOAT2(norm0, y0, y0);					\
  AXPY_FLOAT2(a, x1, y1);						\
  REAL_DOT_FLOAT2(norm1, y1, y1);					\
  AXPY_FLOAT2(a, x2, y2);						\
  REAL_DOT_FLOAT2(norm2, y2, y2);					\
  norm0 += norm1; norm0 += norm2;					\
  CONSTRUCT_HALF_SPINOR_FROM_SINGLE_ST(yH, yN, y, stride);
#define REDUCE_OPERATION(i) (norm0)
#include "reduce_core.h"
#undef REDUCE_FUNC_NAME
#undef REDUCE_TYPES
#undef REDUCE_PARAMS
#undef REDUCE_AUXILIARY
#undef REDUCE_OPERATION

double axpyNormCuda(const double &a, cudaColorSpinorField &x, cudaColorSpinorField &y) {
  if (x.SiteSubset() == QUDA_FULL_SITE_SUBSET) 
    return axpyNormCuda(a, x.Even(), y.Even()) + axpyNormCuda(a, x.Odd(), y.Odd());

  const int id = 19;
  quda::blas_flops += 4*x.RealLength();
  checkSpinor(x,y);
  quda::blas_bytes += 3*x.RealLength()*x.Precision();
  if (x.Precision() == QUDA_DOUBLE_PRECISION) {
    return axpyNormFCuda(a, (double*)x.V(), (double*)y.V(), x.Length(), id, x.Precision());
  } else if (x.Precision() == QUDA_SINGLE_PRECISION) {
    return axpyNormFCuda((float)a, (float*)x.V(), (float*)y.V(), x.Length(), id, x.Precision());
  } else {
    bindTexture(&x, &y);
    quda::blas_bytes += 3*x.Volume()*sizeof(float);
    
    if (x.Nspin() == 4){ //wilson
      return axpyNormHCuda((float)a, (short4*)y.V(), (float*)y.Norm(), x.Stride(), x.Volume(), id, x.Precision());
    }else if (x.Nspin() == 1){ //staggered
      return axpyNormHCuda((float)a, (short2*)y.V(), (float*)y.Norm(), x.Stride(), x.Volume(), id, x.Precision());
    }else{
      errorQuda("%s: nSpin(%d) is not supported\n", __FUNCTION__, x.Nspin());            
      return 0;
    }
  }

}

//
// double xmyNormCuda(float a, float *x, float *y, n){}
//
// First performs the operation y[i] = x[i] - y[i]
// Second returns the norm of y
//

template <unsigned int reduce_threads, typename Float>
#define REDUCE_FUNC_NAME(suffix) xmyNormF##suffix
#define REDUCE_TYPES Float *x, Float *y
#define REDUCE_PARAMS x, y
#define REDUCE_AUXILIARY(i) y[i] = x[i] - y[i]
#define REDUCE_OPERATION(i) (y[i]*y[i])
#include "reduce_core.h"
#undef REDUCE_FUNC_NAME
#undef REDUCE_TYPES
#undef REDUCE_PARAMS
#undef REDUCE_AUXILIARY
#undef REDUCE_OPERATION

template <unsigned int reduce_threads, typename Float>
#define REDUCE_FUNC_NAME(suffix) xmyNormH##suffix
#define REDUCE_TYPES Float *d1, Float *d2, short4 *yH, float *yN, int stride
#define REDUCE_PARAMS d1, d2, yH, yN, stride
#define REDUCE_AUXILIARY(i)						\
  RECONSTRUCT_HALF_SPINOR(x, texHalf1, texNorm1, stride);		\
  RECONSTRUCT_HALF_SPINOR(y, texHalf2, texNorm2, stride);		\
  XMY_FLOAT4(x0, y0);							\
  REAL_DOT_FLOAT4(norm0, y0, y0);					\
  XMY_FLOAT4(x1, y1);							\
  REAL_DOT_FLOAT4(norm1, y1, y1);					\
  XMY_FLOAT4(x2, y2);							\
  REAL_DOT_FLOAT4(norm2, y2, y2);					\
  XMY_FLOAT4(x3, y3);							\
  REAL_DOT_FLOAT4(norm3, y3, y3);					\
  XMY_FLOAT4(x4, y4);							\
  REAL_DOT_FLOAT4(norm4, y4, y4);					\
  XMY_FLOAT4(x5, y5);							\
  REAL_DOT_FLOAT4(norm5, y5, y5);					\
  norm0 += norm1; norm2 += norm3; norm4 += norm5; norm0 += norm2; norm0 += norm4; \
  CONSTRUCT_HALF_SPINOR_FROM_SINGLE(yH, yN, y, stride);
#define REDUCE_OPERATION(i) (norm0)
#include "reduce_core.h"
#undef REDUCE_FUNC_NAME
#undef REDUCE_TYPES
#undef REDUCE_PARAMS
#undef REDUCE_AUXILIARY
#undef REDUCE_OPERATION

template <unsigned int reduce_threads, typename Float>
#define REDUCE_FUNC_NAME(suffix) xmyNormH##suffix
#define REDUCE_TYPES Float *d1, Float *d2, short2 *yH, float *yN, int stride
#define REDUCE_PARAMS d1, d2, yH, yN, stride
#define REDUCE_AUXILIARY(i)						\
  RECONSTRUCT_HALF_SPINOR_ST(x, texHalfSt1, texNorm1, stride);		\
  RECONSTRUCT_HALF_SPINOR_ST(y, texHalfSt2, texNorm2, stride);		\
  XMY_FLOAT2(x0, y0);							\
  REAL_DOT_FLOAT2(norm0, y0, y0);					\
  XMY_FLOAT2(x1, y1);							\
  REAL_DOT_FLOAT2(norm1, y1, y1);					\
  XMY_FLOAT2(x2, y2);							\
  REAL_DOT_FLOAT2(norm2, y2, y2);					\
  norm0 += norm1; norm0 += norm2;					\
  CONSTRUCT_HALF_SPINOR_FROM_SINGLE_ST(yH, yN, y, stride);
#define REDUCE_OPERATION(i) (norm0)
#include "reduce_core.h"
#undef REDUCE_FUNC_NAME
#undef REDUCE_TYPES
#undef REDUCE_PARAMS
#undef REDUCE_AUXILIARY
#undef REDUCE_OPERATION


double xmyNormCuda(cudaColorSpinorField &x, cudaColorSpinorField &y) {
  if (x.SiteSubset() == QUDA_FULL_SITE_SUBSET) 
    return xmyNormCuda(x.Even(), y.Even()) + xmyNormCuda(x.Odd(), y.Odd());

  const int id = 20;
  quda::blas_flops += 3*x.RealLength();
  checkSpinor(x,y);
  quda::blas_bytes += 3*x.RealLength()*x.Precision();
  if (x.Precision() == QUDA_DOUBLE_PRECISION) {
    return xmyNormFCuda((double*)x.V(), (double*)y.V(), x.Length(), id, x.Precision());
  } else if (x.Precision() == QUDA_SINGLE_PRECISION) {
    return xmyNormFCuda((float*)x.V(), (float*)y.V(), x.Length(), id, x.Precision());
  } else { 
    bindTexture(&x, &y);
    quda::blas_bytes += 3*x.Volume()*sizeof(float);
    
    if (x.Nspin() ==4 ){ //wilsin
      return xmyNormHCuda((char*)0, (char*)0, (short4*)y.V(), (float*)y.Norm(), y.Stride(), y.Volume(), id, x.Precision());
    }else if (x.Nspin() == 1){
      return xmyNormHCuda((char*)0, (char*)0, (short2*)y.V(), (float*)y.Norm(), y.Stride(), y.Volume(), id, x.Precision());
    }else{
      errorQuda("%s: nSpin(%d) is not supported\n", __FUNCTION__, x.Nspin());                  
    }
  }

  exit(-1);
}

//
// double caxpyNormCuda(float a, float *x, float *y, n){}
//
// First performs the operation y[i] = a*x[i] + y[i]
// Second returns the norm of y
//

template <unsigned int reduce_threads, typename Float>
#define REDUCE_FUNC_NAME(suffix) caxpyNormF##suffix
#define REDUCE_TYPES Float a, Float *x, Float *y
#define REDUCE_PARAMS a, x, y
#define REDUCE_AUXILIARY(i)					\
  y[i].x += a.x*x[i].x - a.y*x[i].y;				\
  y[i].y += a.y*x[i].x + a.x*x[i].y
#define REDUCE_OPERATION(i) (y[i].x*y[i].x + y[i].y*y[i].y)
#include "reduce_core.h"
#undef REDUCE_FUNC_NAME
#undef REDUCE_TYPES
#undef REDUCE_PARAMS
#undef REDUCE_AUXILIARY
#undef REDUCE_OPERATION

template <unsigned int reduce_threads, typename Float>
#define REDUCE_FUNC_NAME(suffix) caxpyNormH##suffix
#define REDUCE_TYPES Float a, short4 *yH, float *yN, int stride
#define REDUCE_PARAMS a, yH, yN, stride
#define REDUCE_AUXILIARY(i)						\
  RECONSTRUCT_HALF_SPINOR(x, texHalf1, texNorm1, stride);		\
  RECONSTRUCT_HALF_SPINOR(y, texHalf2, texNorm2, stride);		\
  CAXPY_FLOAT4(a, x0, y0);						\
  REAL_DOT_FLOAT4(norm0, y0, y0);					\
  CAXPY_FLOAT4(a, x1, y1);						\
  REAL_DOT_FLOAT4(norm1, y1, y1);					\
  CAXPY_FLOAT4(a, x2, y2);						\
  REAL_DOT_FLOAT4(norm2, y2, y2);					\
  CAXPY_FLOAT4(a, x3, y3);						\
  REAL_DOT_FLOAT4(norm3, y3, y3);					\
  CAXPY_FLOAT4(a, x4, y4);						\
  REAL_DOT_FLOAT4(norm4, y4, y4);					\
  CAXPY_FLOAT4(a, x5, y5);						\
  REAL_DOT_FLOAT4(norm5, y5, y5);					\
  norm0 += norm1; norm2 += norm3; norm4 += norm5; norm0 += norm2; norm0 += norm4; \
  CONSTRUCT_HALF_SPINOR_FROM_SINGLE(yH, yN, y, stride);
#define REDUCE_OPERATION(i) (norm0)
#include "reduce_core.h"
#undef REDUCE_FUNC_NAME
#undef REDUCE_TYPES
#undef REDUCE_PARAMS
#undef REDUCE_AUXILIARY
#undef REDUCE_OPERATION

template <unsigned int reduce_threads, typename Float>
#define REDUCE_FUNC_NAME(suffix) caxpyNormH##suffix
#define REDUCE_TYPES Float a, short2 *yH, float *yN, int stride
#define REDUCE_PARAMS a, yH, yN, stride
#define REDUCE_AUXILIARY(i)						\
  RECONSTRUCT_HALF_SPINOR_ST(x, texHalfSt1, texNorm1, stride);		\
  RECONSTRUCT_HALF_SPINOR_ST(y, texHalfSt2, texNorm2, stride);		\
  CAXPY_FLOAT2(a, x0, y0);						\
  REAL_DOT_FLOAT2(norm0, y0, y0);					\
  CAXPY_FLOAT2(a, x1, y1);						\
  REAL_DOT_FLOAT2(norm1, y1, y1);					\
  CAXPY_FLOAT2(a, x2, y2);						\
  REAL_DOT_FLOAT2(norm2, y2, y2);					\
  norm0 += norm1; norm0 += norm2;					\
  CONSTRUCT_HALF_SPINOR_FROM_SINGLE_ST(yH, yN, y, stride);
#define REDUCE_OPERATION(i) (norm0)
#include "reduce_core.h"
#undef REDUCE_FUNC_NAME
#undef REDUCE_TYPES
#undef REDUCE_PARAMS
#undef REDUCE_AUXILIARY
#undef REDUCE_OPERATION

double caxpyNormCuda(const quda::Complex &a, cudaColorSpinorField &x, cudaColorSpinorField &y) {
  if (x.SiteSubset() == QUDA_FULL_SITE_SUBSET) 
    return caxpyNormCuda(a, x.Even(), y.Even()) + caxpyNormCuda(a, x.Odd(), y.Odd());

  const int id = 21;
  quda::blas_flops += 6*x.RealLength();
  checkSpinor(x,y);
  quda::blas_bytes += 3*x.RealLength()*x.Precision();
  if (x.Precision() == QUDA_DOUBLE_PRECISION) {
    double2 a2 = make_double2(real(a), imag(a));
    return caxpyNormFCuda(a2, (double2*)x.V(), (double2*)y.V(), x.Length()/2, id, x.Precision());
  } else if (x.Precision() == QUDA_SINGLE_PRECISION) {
    float2 a2 = make_float2(real(a), imag(a));
    return caxpyNormFCuda(a2, (float2*)x.V(), (float2*)y.V(), x.Length()/2, id, x.Precision());
  } else {
    bindTexture(&x, &y);
    quda::blas_bytes += 3*x.Volume()*sizeof(float);
    
    float2 a2 = make_float2(real(a), imag(a));
    if (x.Nspin() == 4){ //wilson
      return caxpyNormHCuda(a2, (short4*)y.V(), (float*)y.Norm(), x.Stride(), x.Volume(), id, x.Precision());
    }else if (x.Nspin() == 1){ //staggered
      return caxpyNormHCuda(a2, (short2*)y.V(), (float*)y.Norm(), x.Stride(), x.Volume(), id, x.Precision());
    }else{
      errorQuda("%s: nSpin(%d) is not supported\n", __FUNCTION__, x.Nspin());            
      return 0;
    }
  }

}

//
// double caxpyXmayNormCuda(float a, float *x, float *y, n){}
//
// First performs the operation y[i] = a*x[i] + y[i]
// Second performs the operator x[i] -= a*z[i]
// Third returns the norm of x
//

template <unsigned int reduce_threads, typename Float>
#define REDUCE_FUNC_NAME(suffix) caxpyXmazNormXF##suffix
#define REDUCE_TYPES Float a, Float *x, Float *y, Float *z
#define REDUCE_PARAMS a, x, y, z
#define REDUCE_AUXILIARY(i)					\
  y[i].x += a.x*x[i].x - a.y*x[i].y;				\
  y[i].y += a.y*x[i].x + a.x*x[i].y;				\
  x[i].x += a.y*z[i].y - a.x*z[i].x;				\
  x[i].y -= (a.x*z[i].y + a.y*z[i].x);
#define REDUCE_OPERATION(i) (x[i].x*x[i].x + x[i].y*x[i].y)
#include "reduce_core.h"
#undef REDUCE_FUNC_NAME
#undef REDUCE_TYPES
#undef REDUCE_PARAMS
#undef REDUCE_AUXILIARY
#undef REDUCE_OPERATION

template <unsigned int reduce_threads, typename Float>
#define REDUCE_FUNC_NAME(suffix) caxpyXmazNormXH##suffix
#define REDUCE_TYPES Float a, short4 *xH, float *xN, short4 *yH, float *yN, int stride
#define REDUCE_PARAMS a, xH, xN, yH, yN, stride
#define REDUCE_AUXILIARY(i)						\
  RECONSTRUCT_HALF_SPINOR(x, texHalf1, texNorm1, stride);		\
  RECONSTRUCT_HALF_SPINOR(y, texHalf2, texNorm2, stride);		\
  RECONSTRUCT_HALF_SPINOR(z, texHalf3, texNorm3, stride);		\
  CAXPY_FLOAT4(a, x0, y0);						\
  CMAXPY_FLOAT4(a, z0, x0);						\
  REAL_DOT_FLOAT4(norm0, x0, x0);					\
  CAXPY_FLOAT4(a, x1, y1);						\
  CMAXPY_FLOAT4(a, z1, x1);						\
  REAL_DOT_FLOAT4(norm1, x1, x1);					\
  CAXPY_FLOAT4(a, x2, y2);						\
  CMAXPY_FLOAT4(a, z2, x2);						\
  REAL_DOT_FLOAT4(norm2, x2, x2);					\
  CAXPY_FLOAT4(a, x3, y3);						\
  CMAXPY_FLOAT4(a, z3, x3);						\
  REAL_DOT_FLOAT4(norm3, x3, x3);					\
  CAXPY_FLOAT4(a, x4, y4);						\
  CMAXPY_FLOAT4(a, z4, x4);						\
  REAL_DOT_FLOAT4(norm4, x4, x4);					\
  CAXPY_FLOAT4(a, x5, y5);						\
  CMAXPY_FLOAT4(a, z5, x5);						\
  REAL_DOT_FLOAT4(norm5, x5, x5);					\
  norm0 += norm1; norm2 += norm3;					\
  norm4 += norm5; norm0 += norm2; norm0 += norm4;			\
  CONSTRUCT_HALF_SPINOR_FROM_SINGLE(yH, yN, y, stride);			\
  CONSTRUCT_HALF_SPINOR_FROM_SINGLE(xH, xN, x, stride);
#define REDUCE_OPERATION(i) (norm0)
#include "reduce_core.h"
#undef REDUCE_FUNC_NAME
#undef REDUCE_TYPES
#undef REDUCE_PARAMS
#undef REDUCE_AUXILIARY
#undef REDUCE_OPERATION

template <unsigned int reduce_threads, typename Float>
#define REDUCE_FUNC_NAME(suffix) caxpyXmazNormXH##suffix
#define REDUCE_TYPES Float a, short2 *xH, float *xN, short2 *yH, float *yN, int stride
#define REDUCE_PARAMS a, xH, xN, yH, yN, stride
#define REDUCE_AUXILIARY(i)						\
  RECONSTRUCT_HALF_SPINOR_ST(x, texHalfSt1, texNorm1, stride);		\
  RECONSTRUCT_HALF_SPINOR_ST(y, texHalfSt2, texNorm2, stride);		\
  RECONSTRUCT_HALF_SPINOR_ST(z, texHalfSt3, texNorm3, stride);		\
  CAXPY_FLOAT2(a, x0, y0);						\
  CMAXPY_FLOAT2(a, z0, x0);						\
  REAL_DOT_FLOAT2(norm0, x0, x0);					\
  CAXPY_FLOAT2(a, x1, y1);						\
  CMAXPY_FLOAT2(a, z1, x1);						\
  REAL_DOT_FLOAT2(norm1, x1, x1);					\
  CAXPY_FLOAT2(a, x2, y2);						\
  CMAXPY_FLOAT2(a, z2, x2);						\
  REAL_DOT_FLOAT2(norm2, x2, x2);					\
  norm0 += norm1; norm0 += norm2;					\
  CONSTRUCT_HALF_SPINOR_FROM_SINGLE_ST(yH, yN, y, stride);		\
  CONSTRUCT_HALF_SPINOR_FROM_SINGLE_ST(xH, xN, x, stride);
#define REDUCE_OPERATION(i) (norm0)
#include "reduce_core.h"
#undef REDUCE_FUNC_NAME
#undef REDUCE_TYPES
#undef REDUCE_PARAMS
#undef REDUCE_AUXILIARY
#undef REDUCE_OPERATION

double caxpyXmazNormXCuda(const quda::Complex &a, cudaColorSpinorField &x, 
			  cudaColorSpinorField &y, cudaColorSpinorField &z) {
  if (x.SiteSubset() == QUDA_FULL_SITE_SUBSET) 
    return caxpyXmazNormXCuda(a, x.Even(), y.Even(), z.Even()) + 
      caxpyXmazNormXCuda(a, x.Odd(), y.Odd(), z.Odd());

  const int id = 22;
  quda::blas_flops += 10*x.RealLength();
  checkSpinor(x,y);
  quda::blas_bytes += 5*x.RealLength()*x.Precision();
  if (x.Precision() == QUDA_DOUBLE_PRECISION) {
    double2 a2 = make_double2(real(a), imag(a));
    return caxpyXmazNormXFCuda(a2, (double2*)x.V(), (double2*)y.V(), (double2*)z.V(), x.Length()/2, id, x.Precision());
  } else if (x.Precision() == QUDA_SINGLE_PRECISION) {
    float2 a2 = make_float2(real(a), imag(a));
    return caxpyXmazNormXFCuda(a2, (float2*)x.V(), (float2*)y.V(), (float2*)z.V(), x.Length()/2, id, x.Precision());
  } else {
    bindTexture(&x, &y, &z);
    quda::blas_bytes += 3*x.Volume()*sizeof(float);
    
    float2 a2 = make_float2(real(a), imag(a));
    if (x.Nspin() == 4){ //wilson
      return caxpyXmazNormXHCuda(a2, (short4*)x.V(), (float*)x.Norm(), (short4*)y.V(), (float*)y.Norm(), x.Stride(), x.Volume(), id, x.Precision());
    }else if (x.Nspin() == 1){ //staggered
      return caxpyXmazNormXHCuda(a2, (short2*)x.V(), (float*)x.Norm(), (short2*)y.V(), (float*)y.Norm(), x.Stride(), x.Volume(), id, x.Precision());
    }else{
      errorQuda("%s: nSpin(%d) is not supported\n", __FUNCTION__, x.Nspin());            
      return 0;
    }
  }

}


//
// double cabxpyAxNormCuda(float a, float *x, float *y, n){}
//
// First performs the operation y[i] = a*x[i] + y[i]
// Second returns the norm of y
//

template <unsigned int reduce_threads, typename Float>
#define REDUCE_FUNC_NAME(suffix) cabxpyAxNormF##suffix
#define REDUCE_TYPES Float a, Float b, Float *x, Float *y
#define REDUCE_PARAMS a, b, x, y
#define REDUCE_AUXILIARY(i)						\
  x[i].x *= a.x;							\
  x[i].y *= a.x;							\
  y[i].x += b.x*x[i].x - b.y*x[i].y;					\
  y[i].y += b.y*x[i].x + b.x*x[i].y;
#define REDUCE_OPERATION(i) (y[i].x*y[i].x + y[i].y*y[i].y)
#include "reduce_core.h"
#undef REDUCE_FUNC_NAME
#undef REDUCE_TYPES
#undef REDUCE_PARAMS
#undef REDUCE_AUXILIARY
#undef REDUCE_OPERATION

template <unsigned int reduce_threads, typename Float>
#define REDUCE_FUNC_NAME(suffix) cabxpyAxNormH##suffix
#define REDUCE_TYPES Float a, Float b, short4 *xH, float *xN, short4 *yH, float *yN, int stride
#define REDUCE_PARAMS a, b, xH, xN, yH, yN, stride
#define REDUCE_AUXILIARY(i)						\
  RECONSTRUCT_HALF_SPINOR(x, texHalf1, texNorm1, stride);		\
  RECONSTRUCT_HALF_SPINOR(y, texHalf2, texNorm2, stride);		\
  AX_FLOAT4(a.x, x0);							\
  AX_FLOAT4(a.x, x1);							\
  AX_FLOAT4(a.x, x2);							\
  AX_FLOAT4(a.x, x3);							\
  AX_FLOAT4(a.x, x4);							\
  AX_FLOAT4(a.x, x5);							\
  CONSTRUCT_HALF_SPINOR_FROM_SINGLE(xH, xN, x, stride);			\
  CAXPY_FLOAT4(b, x0, y0);						\
  REAL_DOT_FLOAT4(norm0, y0, y0);					\
  CAXPY_FLOAT4(b, x1, y1);						\
  REAL_DOT_FLOAT4(norm1, y1, y1);					\
  CAXPY_FLOAT4(b, x2, y2);						\
  REAL_DOT_FLOAT4(norm2, y2, y2);					\
  CAXPY_FLOAT4(b, x3, y3);						\
  REAL_DOT_FLOAT4(norm3, y3, y3);					\
  CAXPY_FLOAT4(b, x4, y4);						\
  REAL_DOT_FLOAT4(norm4, y4, y4);					\
  CAXPY_FLOAT4(b, x5, y5);						\
  REAL_DOT_FLOAT4(norm5, y5, y5);					\
  norm0 += norm1; norm2 += norm3; norm4 += norm5; norm0 += norm2; norm0 += norm4; \
  CONSTRUCT_HALF_SPINOR_FROM_SINGLE(yH, yN, y, stride);
#define REDUCE_OPERATION(i) (norm0)
#include "reduce_core.h"
#undef REDUCE_FUNC_NAME
#undef REDUCE_TYPES
#undef REDUCE_PARAMS
#undef REDUCE_AUXILIARY
#undef REDUCE_OPERATION

template <unsigned int reduce_threads, typename Float>
#define REDUCE_FUNC_NAME(suffix) cabxpyAxNormH##suffix
#define REDUCE_TYPES Float a, Float b, short2 *xH, float *xN, short2 *yH, float *yN, int stride
#define REDUCE_PARAMS a, b, xH, xN, yH, yN, stride
#define REDUCE_AUXILIARY(i)						\
  RECONSTRUCT_HALF_SPINOR_ST(x, texHalfSt1, texNorm1, stride);		\
  RECONSTRUCT_HALF_SPINOR_ST(y, texHalfSt2, texNorm2, stride);		\
  AX_FLOAT2(a.x, x0);							\
  AX_FLOAT2(a.x, x1);							\
  AX_FLOAT2(a.x, x2);							\
  CONSTRUCT_HALF_SPINOR_FROM_SINGLE_ST(xH, xN, x, stride);		\
  CAXPY_FLOAT2(b, x0, y0);						\
  REAL_DOT_FLOAT2(norm0, y0, y0);					\
  CAXPY_FLOAT2(b, x1, y1);						\
  REAL_DOT_FLOAT2(norm1, y1, y1);					\
  CAXPY_FLOAT2(b, x2, y2);						\
  REAL_DOT_FLOAT2(norm2, y2, y2);					\
  norm0 += norm1; norm0 += norm2;					\
  CONSTRUCT_HALF_SPINOR_FROM_SINGLE_ST(yH, yN, y, stride);
#define REDUCE_OPERATION(i) (norm0)
#include "reduce_core.h"
#undef REDUCE_FUNC_NAME
#undef REDUCE_TYPES
#undef REDUCE_PARAMS
#undef REDUCE_AUXILIARY
#undef REDUCE_OPERATION

double cabxpyAxNormCuda(const double &a, const quda::Complex &b, cudaColorSpinorField &x, cudaColorSpinorField &y) {
  if (x.SiteSubset() == QUDA_FULL_SITE_SUBSET) 
    return cabxpyAxNormCuda(a, b, x.Even(), y.Even()) + cabxpyAxNormCuda(a, b, x.Odd(), y.Odd());

  const int id = 23;
  quda::blas_flops += 7*x.RealLength();
  checkSpinor(x,y);
  quda::blas_bytes += 4*x.RealLength()*x.Precision();
  if (x.Precision() == QUDA_DOUBLE_PRECISION) {
    double2 a2 = make_double2(a, 0);
    double2 b2 = make_double2(real(b), imag(b));
    return cabxpyAxNormFCuda(a2, b2, (double2*)x.V(), (double2*)y.V(), x.Length()/2, id, x.Precision());
  } else if (x.Precision() == QUDA_SINGLE_PRECISION) {
    float2 a2 = make_float2(a, 0);
    float2 b2 = make_float2(real(b), imag(b));
    return cabxpyAxNormFCuda(a2, b2, (float2*)x.V(), (float2*)y.V(), x.Length()/2, id, x.Precision());
  } else {
    bindTexture(&x, &y);
    quda::blas_bytes += 3*x.Volume()*sizeof(float);
    
    float2 a2 = make_float2(a, 0);
    float2 b2 = make_float2(real(b), imag(b));
    if (x.Nspin() == 4){ //wilson
      return cabxpyAxNormHCuda(a2, b2, (short4*)x.V(), (float*)x.Norm(), (short4*)y.V(), (float*)y.Norm(), x.Stride(), x.Volume(), id, x.Precision());
    }else if (x.Nspin() == 1){ //staggered
      return cabxpyAxNormHCuda(a2, b2, (short2*)x.V(), (float*)x.Norm(), (short2*)y.V(), (float*)y.Norm(), x.Stride(), x.Volume(), id, x.Precision());
    }else{
      errorQuda("%s: nSpin(%d) is not supported\n", __FUNCTION__, x.Nspin());            
      return 0;
    }
  }

}


//
// double2 cDotProductCuda(float2 *x, float2 *y, int n) {}
//
template <unsigned int reduce_threads, typename Float, typename Float2>
#define REDUCE_FUNC_NAME(suffix) cDotProductD##suffix
#define REDUCE_TYPES Float2 *x, Float2 *y, Float c
#define REDUCE_PARAMS x, y, c
#define REDUCE_REAL_AUXILIARY(i) Float2 a = READ_DOUBLE2_TEXTURE(x, i);
#define REDUCE_IMAG_AUXILIARY(i) Float2 b = READ_DOUBLE2_TEXTURE(y, i);
#define REDUCE_REAL_OPERATION(i) (a.x*b.x + a.y*b.y)
#define REDUCE_IMAG_OPERATION(i) (a.x*b.y - a.y*b.x)
#include "reduce_complex_core.h"
#undef REDUCE_FUNC_NAME
#undef REDUCE_TYPES
#undef REDUCE_PARAMS
#undef REDUCE_REAL_AUXILIARY
#undef REDUCE_IMAG_AUXILIARY
#undef REDUCE_REAL_OPERATION
#undef REDUCE_IMAG_OPERATION

template <unsigned int reduce_threads, typename Float, typename Float2>
#define REDUCE_FUNC_NAME(suffix) cDotProductS##suffix
#define REDUCE_TYPES Float2 *x, Float2 *y, Float c
#define REDUCE_PARAMS x, y, c
#define REDUCE_REAL_AUXILIARY(i) Float2 a = read_Float2(x, i);
#define REDUCE_IMAG_AUXILIARY(i) Float2 b = read_Float2(y, i);
#define REDUCE_REAL_OPERATION(i) (a.x*b.x + a.y*b.y)
#define REDUCE_IMAG_OPERATION(i) (a.x*b.y - a.y*b.x)
#include "reduce_complex_core.h"
#undef REDUCE_FUNC_NAME
#undef REDUCE_TYPES
#undef REDUCE_PARAMS
#undef REDUCE_REAL_AUXILIARY
#undef REDUCE_IMAG_AUXILIARY
#undef REDUCE_REAL_OPERATION
#undef REDUCE_IMAG_OPERATION

template <unsigned int reduce_threads, typename Float, typename Float2>
#define REDUCE_FUNC_NAME(suffix) cDotProductH##suffix
#define REDUCE_TYPES Float *aN, Float2 *bN, int stride
#define REDUCE_PARAMS aN, bN, stride
#define REDUCE_REAL_AUXILIARY(i)					\
  READ_HALF_SPINOR(a, texHalf1, stride);				\
  READ_HALF_SPINOR(b, texHalf2, stride);				\
  REAL_DOT_FLOAT4(rdot0, a0, b0);					\
  REAL_DOT_FLOAT4(rdot1, a1, b1);					\
  REAL_DOT_FLOAT4(rdot2, a2, b2);					\
  REAL_DOT_FLOAT4(rdot3, a3, b3);					\
  REAL_DOT_FLOAT4(rdot4, a4, b4);					\
  REAL_DOT_FLOAT4(rdot5, a5, b5);					\
  rdot0 += rdot1; rdot2 += rdot3; rdot4 += rdot5; rdot0 += rdot2; rdot0 += rdot4;
#define REDUCE_IMAG_AUXILIARY(i)					\
  IMAG_DOT_FLOAT4(idot0, a0, b0);					\
  IMAG_DOT_FLOAT4(idot1, a1, b1);					\
  IMAG_DOT_FLOAT4(idot2, a2, b2);					\
  IMAG_DOT_FLOAT4(idot3, a3, b3);					\
  IMAG_DOT_FLOAT4(idot4, a4, b4);					\
  IMAG_DOT_FLOAT4(idot5, a5, b5);					\
  idot0 += idot1; idot2 += idot3; idot4 += idot5; idot0 += idot2; idot0 += idot4;
#define REDUCE_REAL_OPERATION(i) (ac*bc*rdot0)
#define REDUCE_IMAG_OPERATION(i) (ac*bc*idot0)
#include "reduce_complex_core.h"
#undef REDUCE_FUNC_NAME
#undef REDUCE_TYPES
#undef REDUCE_PARAMS
#undef REDUCE_REAL_AUXILIARY
#undef REDUCE_IMAG_AUXILIARY
#undef REDUCE_REAL_OPERATION
#undef REDUCE_IMAG_OPERATION

template <unsigned int reduce_threads, typename Float, typename Float2>
#define REDUCE_FUNC_NAME(suffix) cDotProductHSt##suffix
#define REDUCE_TYPES Float *aN, Float2 *bN, int stride
#define REDUCE_PARAMS aN, bN, stride
#define REDUCE_REAL_AUXILIARY(i)					\
  READ_HALF_SPINOR_ST(a, texHalfSt1, stride);				\
  READ_HALF_SPINOR_ST(b, texHalfSt2, stride);				\
  REAL_DOT_FLOAT2(rdot0, a0, b0);					\
  REAL_DOT_FLOAT2(rdot1, a1, b1);					\
  REAL_DOT_FLOAT2(rdot2, a2, b2);					\
  rdot0 += rdot1; rdot0 += rdot2;
#define REDUCE_IMAG_AUXILIARY(i)					\
  IMAG_DOT_FLOAT2(idot0, a0, b0);					\
  IMAG_DOT_FLOAT2(idot1, a1, b1);					\
  IMAG_DOT_FLOAT2(idot2, a2, b2);					\
  idot0 += idot1; idot0 += idot2;
#define REDUCE_REAL_OPERATION(i) (ac*bc*rdot0)
#define REDUCE_IMAG_OPERATION(i) (ac*bc*idot0)
#include "reduce_complex_core.h"
#undef REDUCE_FUNC_NAME
#undef REDUCE_TYPES
#undef REDUCE_PARAMS
#undef REDUCE_REAL_AUXILIARY
#undef REDUCE_IMAG_AUXILIARY
#undef REDUCE_REAL_OPERATION
#undef REDUCE_IMAG_OPERATION

quda::Complex cDotProductCuda(cudaColorSpinorField &x, cudaColorSpinorField &y) {
  if (x.SiteSubset() == QUDA_FULL_SITE_SUBSET) 
    return cDotProductCuda(x.Even(), y.Even()) + cDotProductCuda(x.Odd(), y.Odd());

  const int id = 24;
  quda::blas_flops += 4*x.RealLength();
  checkSpinor(x,y);
  int length = x.Length()/2;
  quda::blas_bytes += 2*x.RealLength()*x.Precision();
  double2 dot;
  if (x.Precision() == QUDA_DOUBLE_PRECISION) {
    char c = 0;
    bindTexture(&x, &y);
    dot = cDotProductDCuda((double2*)x.V(), (double2*)y.V(), c, length, id, x.Precision());
  } else if (x.Precision() == QUDA_SINGLE_PRECISION) {
    char c = 0;
    // FIXME: bindTexture() will incorrectly bind this as float4 since it queries the spin
    int spinor_bytes = x.Length()*sizeof(float);
    cudaBindTexture(0, xTexSingle2, x.V(), spinor_bytes); 
    cudaBindTexture(0, yTexSingle2, y.V(), spinor_bytes); 
    dot = cDotProductSCuda((float2*)x.V(), (float2*)y.V(), c, length, id, x.Precision());
  } else {
    bindTexture(&x, &y);
    quda::blas_bytes += 2*x.Volume()*sizeof(float);
    if (x.Nspin() == 4){
      dot = cDotProductHCuda((float*)x.Norm(), (float*)y.Norm(), x.Stride(), x.Volume(), id, x.Precision());
    } else if (x.Nspin() == 1){
      dot = cDotProductHStCuda((float*)x.Norm(), (float*)y.Norm(), x.Stride(), x.Volume(), id, x.Precision());
    }else{
      errorQuda("%s: nSpin(%d) is not supported\n", __FUNCTION__, x.Nspin());      
    } 
  }

  return quda::Complex(dot.x, dot.y);
}

//
// double2 xpaycDotzyCuda(float2 *x, float a, float2 *y, float2 *z, int n) {}
//
// First performs the operation y = x + a*y
// Second returns complex dot product (z,y)
//

template <unsigned int reduce_threads, typename Float, typename Float2>
#define REDUCE_FUNC_NAME(suffix) xpaycDotzyD##suffix
#define REDUCE_TYPES Float2 *x, Float a, Float2 *y, Float2 *z
#define REDUCE_PARAMS x, a, y, z
#define REDUCE_REAL_AUXILIARY(i)		\
  Float2 X = READ_DOUBLE2_TEXTURE(x, i);	\
  Float2 Y = READ_DOUBLE2_TEXTURE(y, i);	\
  Float2 Z = READ_DOUBLE2_TEXTURE(z, i);
#define REDUCE_IMAG_AUXILIARY(i) y[i].x = X.x + a*Y.x; y[i].y = X.y + a*Y.y
#define REDUCE_REAL_OPERATION(i) (Z.x*y[i].x + Z.y*y[i].y)
#define REDUCE_IMAG_OPERATION(i) (Z.x*y[i].y - Z.y*y[i].x)
#include "reduce_complex_core.h"
#undef REDUCE_FUNC_NAME
#undef REDUCE_TYPES
#undef REDUCE_PARAMS
#undef REDUCE_REAL_AUXILIARY
#undef REDUCE_IMAG_AUXILIARY
#undef REDUCE_REAL_OPERATION
#undef REDUCE_IMAG_OPERATION

template <unsigned int reduce_threads, typename Float, typename Float2>
#define REDUCE_FUNC_NAME(suffix) xpaycDotzyS##suffix
#define REDUCE_TYPES Float2 *x, Float a, Float2 *y, Float2 *z
#define REDUCE_PARAMS x, a, y, z
#define REDUCE_REAL_AUXILIARY(i) y[i].x = x[i].x + a*y[i].x
#define REDUCE_IMAG_AUXILIARY(i) y[i].y = x[i].y + a*y[i].y
#define REDUCE_REAL_OPERATION(i) (z[i].x*y[i].x + z[i].y*y[i].y)
#define REDUCE_IMAG_OPERATION(i) (z[i].x*y[i].y - z[i].y*y[i].x)
#include "reduce_complex_core.h"
#undef REDUCE_FUNC_NAME
#undef REDUCE_TYPES
#undef REDUCE_PARAMS
#undef REDUCE_REAL_AUXILIARY
#undef REDUCE_IMAG_AUXILIARY
#undef REDUCE_REAL_OPERATION
#undef REDUCE_IMAG_OPERATION

template <unsigned int reduce_threads, typename Float, typename Float2>
#define REDUCE_FUNC_NAME(suffix) xpaycDotzyH##suffix
#define REDUCE_TYPES Float a, short4 *yH, Float2 *yN, int stride
#define REDUCE_PARAMS a, yH, yN, stride
#define REDUCE_REAL_AUXILIARY(i)					\
  RECONSTRUCT_HALF_SPINOR(x, texHalf1, texNorm1, stride);		\
  RECONSTRUCT_HALF_SPINOR(y, texHalf2, texNorm2, stride);		\
  RECONSTRUCT_HALF_SPINOR(z, texHalf3, texNorm3, stride);		\
  XPAY_FLOAT4(x0, a, y0);						\
  XPAY_FLOAT4(x1, a, y1);						\
  XPAY_FLOAT4(x2, a, y2);						\
  XPAY_FLOAT4(x3, a, y3);						\
  XPAY_FLOAT4(x4, a, y4);						\
  XPAY_FLOAT4(x5, a, y5);						\
  REAL_DOT_FLOAT4(rdot0, z0, y0);					\
  REAL_DOT_FLOAT4(rdot1, z1, y1);					\
  REAL_DOT_FLOAT4(rdot2, z2, y2);					\
  REAL_DOT_FLOAT4(rdot3, z3, y3);					\
  REAL_DOT_FLOAT4(rdot4, z4, y4);					\
  REAL_DOT_FLOAT4(rdot5, z5, y5);					\
  rdot0 += rdot1; rdot2 += rdot3; rdot4 += rdot5; rdot0 += rdot2; rdot0 += rdot4;
#define REDUCE_IMAG_AUXILIARY(i)					\
  IMAG_DOT_FLOAT4(idot0, z0, y0);					\
  IMAG_DOT_FLOAT4(idot1, z1, y1);					\
  IMAG_DOT_FLOAT4(idot2, z2, y2);					\
  IMAG_DOT_FLOAT4(idot3, z3, y3);					\
  IMAG_DOT_FLOAT4(idot4, z4, y4);					\
  IMAG_DOT_FLOAT4(idot5, z5, y5);					\
  idot0 += idot1; idot2 += idot3; idot4 += idot5; idot0 += idot2; idot0 += idot4; \
  CONSTRUCT_HALF_SPINOR_FROM_SINGLE(yH, yN, y, stride);
#define REDUCE_REAL_OPERATION(i) (rdot0)
#define REDUCE_IMAG_OPERATION(i) (idot0)
#include "reduce_complex_core.h"
#undef REDUCE_FUNC_NAME
#undef REDUCE_TYPES
#undef REDUCE_PARAMS
#undef REDUCE_REAL_AUXILIARY
#undef REDUCE_IMAG_AUXILIARY
#undef REDUCE_REAL_OPERATION
#undef REDUCE_IMAG_OPERATION

template <unsigned int reduce_threads, typename Float, typename Float2>
#define REDUCE_FUNC_NAME(suffix) xpaycDotzyH##suffix
#define REDUCE_TYPES Float a, short2 *yH, Float2 *yN, int stride
#define REDUCE_PARAMS a, yH, yN, stride
#define REDUCE_REAL_AUXILIARY(i)					\
  RECONSTRUCT_HALF_SPINOR_ST(x, texHalfSt1, texNorm1, stride);		\
  RECONSTRUCT_HALF_SPINOR_ST(y, texHalfSt2, texNorm2, stride);		\
  RECONSTRUCT_HALF_SPINOR_ST(z, texHalfSt3, texNorm3, stride);		\
  XPAY_FLOAT2(x0, a, y0);						\
  XPAY_FLOAT2(x1, a, y1);						\
  XPAY_FLOAT2(x2, a, y2);						\
  REAL_DOT_FLOAT2(rdot0, z0, y0);					\
  REAL_DOT_FLOAT2(rdot1, z1, y1);					\
  REAL_DOT_FLOAT2(rdot2, z2, y2);					\
  rdot0 += rdot1; rdot0 += rdot2;
#define REDUCE_IMAG_AUXILIARY(i)					\
  IMAG_DOT_FLOAT2(idot0, z0, y0);					\
  IMAG_DOT_FLOAT2(idot1, z1, y1);					\
  IMAG_DOT_FLOAT2(idot2, z2, y2);					\
  idot0 += idot1; idot0 += idot2;					\
  CONSTRUCT_HALF_SPINOR_FROM_SINGLE_ST(yH, yN, y, stride);
#define REDUCE_REAL_OPERATION(i) (rdot0)
#define REDUCE_IMAG_OPERATION(i) (idot0)
#include "reduce_complex_core.h"
#undef REDUCE_FUNC_NAME
#undef REDUCE_TYPES
#undef REDUCE_PARAMS
#undef REDUCE_REAL_AUXILIARY
#undef REDUCE_IMAG_AUXILIARY
#undef REDUCE_REAL_OPERATION
#undef REDUCE_IMAG_OPERATION

quda::Complex xpaycDotzyCuda(cudaColorSpinorField &x, const double &a, cudaColorSpinorField &y, cudaColorSpinorField &z) {
  if (x.SiteSubset() == QUDA_FULL_SITE_SUBSET) 
    return xpaycDotzyCuda(x.Even(), a, y.Even(), z.Even()) + xpaycDotzyCuda(x.Odd(), a, y.Odd(), z.Odd());

  const int id = 25;
  quda::blas_flops += 6*x.RealLength();
  checkSpinor(x,y);
  checkSpinor(x,z);
  int length = x.Length()/2;
  quda::blas_bytes += 4*x.RealLength()*x.Precision();
  double2 dot;
  if (x.Precision() == QUDA_DOUBLE_PRECISION) {
    bindTexture(&x, &y, &z);
    dot = xpaycDotzyDCuda((double2*)x.V(), a, (double2*)y.V(), (double2*)z.V(), length, id, x.Precision());
  } else if (x.Precision() == QUDA_SINGLE_PRECISION) {
    dot = xpaycDotzySCuda((float2*)x.V(), (float)a, (float2*)y.V(), (float2*)z.V(), length, id, x.Precision());
  } else {
    bindTexture(&x, &y, &z);
    quda::blas_bytes += 4*x.Volume()*sizeof(float);
    if (x.Nspin() ==4 ){//wilson
      dot = xpaycDotzyHCuda((float)a, (short4*)y.V(), (float*)y.Norm(), x.Stride(), x.Volume(), id, x.Precision());
    } else if (x.Nspin() ==1 ){//wilson
      dot = xpaycDotzyHCuda((float)a, (short2*)y.V(), (float*)y.Norm(), x.Stride(), x.Volume(), id, x.Precision());
    }else{
      errorQuda("%s: nSpin(%d) is not supported\n", __FUNCTION__, x.Nspin());            
    }
  }
  return quda::Complex(dot.x, dot.y);
}



//
// double caxpyDotzyCuda(float a, float *x, float *y, float *z, n){}
//
// First performs the operation y[i] = a*x[i] + y[i]
// Second returns the dot product (z,y)
//

template <unsigned int reduce_threads, typename Float, typename Float2>
#define REDUCE_FUNC_NAME(suffix) caxpyDotzyF##suffix
#define REDUCE_TYPES Float2 a, Float2 *x, Float2 *y, Float2 *z, Float c
#define REDUCE_PARAMS a, x, y, z, c
#define REDUCE_REAL_AUXILIARY(i) y[i].x += a.x*x[i].x - a.y*x[i].y;
#define REDUCE_IMAG_AUXILIARY(i) y[i].y += a.y*x[i].x + a.x*x[i].y;
#define REDUCE_REAL_OPERATION(i) (z[i].x*y[i].x + z[i].y*y[i].y)
#define REDUCE_IMAG_OPERATION(i) (z[i].x*y[i].y - z[i].y*y[i].x)
#include "reduce_complex_core.h"
#undef REDUCE_FUNC_NAME
#undef REDUCE_TYPES
#undef REDUCE_PARAMS
#undef REDUCE_REAL_AUXILIARY
#undef REDUCE_IMAG_AUXILIARY
#undef REDUCE_REAL_OPERATION
#undef REDUCE_IMAG_OPERATION

template <unsigned int reduce_threads, typename Float, typename Float2>
#define REDUCE_FUNC_NAME(suffix) caxpyDotzyH##suffix
#define REDUCE_TYPES Float2 a, short4 *yH, Float *yN, int stride
#define REDUCE_PARAMS a, yH, yN, stride
#define REDUCE_REAL_AUXILIARY(i)					\
  RECONSTRUCT_HALF_SPINOR(x, texHalf1, texNorm1, stride);		\
  RECONSTRUCT_HALF_SPINOR(y, texHalf2, texNorm2, stride);		\
  RECONSTRUCT_HALF_SPINOR(z, texHalf3, texNorm3, stride);		\
  CAXPY_FLOAT4(a, x0, y0);						\
  CAXPY_FLOAT4(a, x1, y1);						\
  CAXPY_FLOAT4(a, x2, y2);						\
  CAXPY_FLOAT4(a, x3, y3);						\
  CAXPY_FLOAT4(a, x4, y4);						\
  CAXPY_FLOAT4(a, x5, y5);						\
  CONSTRUCT_HALF_SPINOR_FROM_SINGLE(yH, yN, y, stride);			
#define REDUCE_IMAG_AUXILIARY(i)					\
  REAL_DOT_FLOAT4(rdot0, z0, y0);					\
  REAL_DOT_FLOAT4(rdot1, z1, y1);					\
  REAL_DOT_FLOAT4(rdot2, z2, y2);					\
  REAL_DOT_FLOAT4(rdot3, z3, y3);					\
  REAL_DOT_FLOAT4(rdot4, z4, y4);					\
  REAL_DOT_FLOAT4(rdot5, z5, y5);					\
  IMAG_DOT_FLOAT4(idot0, z0, y0);					\
  IMAG_DOT_FLOAT4(idot1, z1, y1);					\
  IMAG_DOT_FLOAT4(idot2, z2, y2);					\
  IMAG_DOT_FLOAT4(idot3, z3, y3);					\
  IMAG_DOT_FLOAT4(idot4, z4, y4);					\
  IMAG_DOT_FLOAT4(idot5, z5, y5);					\
  rdot0 += rdot1; rdot2 += rdot3; rdot4 += rdot5; rdot0 += rdot2; rdot0 += rdot4; \
  idot0 += idot1; idot2 += idot3; idot4 += idot5; idot0 += idot2; idot0 += idot4;
#define REDUCE_REAL_OPERATION(i) (rdot0)
#define REDUCE_IMAG_OPERATION(i) (idot0)
#include "reduce_complex_core.h"
#undef REDUCE_FUNC_NAME
#undef REDUCE_TYPES
#undef REDUCE_PARAMS
#undef REDUCE_REAL_AUXILIARY
#undef REDUCE_IMAG_AUXILIARY
#undef REDUCE_REAL_OPERATION
#undef REDUCE_IMAG_OPERATION

template <unsigned int reduce_threads, typename Float, typename Float2>
#define REDUCE_FUNC_NAME(suffix) caxpyDotzyH##suffix
#define REDUCE_TYPES Float2 a, short2 *yH, Float *yN, int stride
#define REDUCE_PARAMS a, yH, yN, stride
#define REDUCE_REAL_AUXILIARY(i)					\
  RECONSTRUCT_HALF_SPINOR_ST(x, texHalfSt1, texNorm1, stride);		\
  RECONSTRUCT_HALF_SPINOR_ST(y, texHalfSt2, texNorm2, stride);		\
  RECONSTRUCT_HALF_SPINOR_ST(z, texHalfSt3, texNorm3, stride);		\
  CAXPY_FLOAT2(a, x0, y0);						\
  CAXPY_FLOAT2(a, x1, y1);						\
  CAXPY_FLOAT2(a, x2, y2);						\
  CONSTRUCT_HALF_SPINOR_FROM_SINGLE_ST(yH, yN, y, stride);		
#define REDUCE_IMAG_AUXILIARY(i)					\
  REAL_DOT_FLOAT2(rdot0, z0, y0);					\
  REAL_DOT_FLOAT2(rdot1, z1, y1);					\
  REAL_DOT_FLOAT2(rdot2, z2, y2);					\
  IMAG_DOT_FLOAT2(idot0, z0, y0);					\
  IMAG_DOT_FLOAT2(idot1, z1, y1);					\
  IMAG_DOT_FLOAT2(idot2, z2, y2);					\
  rdot0 += rdot1; rdot0 += rdot2;					\
  idot0 += idot1; idot0 += idot2; 
#define REDUCE_REAL_OPERATION(i) (rdot0)
#define REDUCE_IMAG_OPERATION(i) (idot0)	
#include "reduce_complex_core.h"
#undef REDUCE_FUNC_NAME
#undef REDUCE_TYPES
#undef REDUCE_PARAMS
#undef REDUCE_REAL_AUXILIARY
#undef REDUCE_IMAG_AUXILIARY
#undef REDUCE_REAL_OPERATION
#undef REDUCE_IMAG_OPERATION

quda::Complex caxpyDotzyCuda(const quda::Complex &a, cudaColorSpinorField &x, cudaColorSpinorField &y,
		       cudaColorSpinorField &z) {
  if (x.SiteSubset() == QUDA_FULL_SITE_SUBSET) 
    return caxpyDotzyCuda(a, x.Even(), y.Even(), z.Even()) + 
      caxpyDotzyCuda(a, x.Odd(), y.Odd(), z.Odd());

  const int id = 26;
  quda::blas_flops += 8*x.RealLength();
  checkSpinor(x,y);
  quda::blas_bytes += 4*x.RealLength()*x.Precision();
  double2 dot;
  if (x.Precision() == QUDA_DOUBLE_PRECISION) {
    char c = 0;
    double2 a2 = make_double2(real(a), imag(a));
    dot = caxpyDotzyFCuda(a2, (double2*)x.V(), (double2*)y.V(), (double2*)z.V(), c, x.Length()/2, id, x.Precision());
  } else if (x.Precision() == QUDA_SINGLE_PRECISION) {
    char c = 0;
    float2 a2 = make_float2(real(a), imag(a));
    dot = caxpyDotzyFCuda(a2, (float2*)x.V(), (float2*)y.V(), (float2*)z.V(), c, x.Length()/2, id, x.Precision());
  } else {
    bindTexture(&x, &y, &z);
    quda::blas_bytes += 3*x.Volume()*sizeof(float);
    float2 a2 = make_float2(real(a), imag(a));
    
    if (x.Nspin() == 4){ //wilson
      dot = caxpyDotzyHCuda(a2, (short4*)y.V(), (float*)y.Norm(), x.Stride(), x.Volume(), id, x.Precision());
    }else if (x.Nspin() == 1){ //staggered
      dot = caxpyDotzyHCuda(a2, (short2*)y.V(), (float*)y.Norm(), x.Stride(), x.Volume(), id, x.Precision());
    }else{
      errorQuda("%s: nSpin(%d) is not supported\n", __FUNCTION__, x.Nspin());            
    }
  }

  return quda::Complex(dot.x, dot.y);
}


//
// double3 cDotProductNormACuda(float2 *a, float2 *b, int n) {}
//
template <unsigned int reduce_threads, typename Float2>
#define REDUCE_FUNC_NAME(suffix) cDotProductNormAD##suffix
#define REDUCE_TYPES Float2 *x, Float2 *y
#define REDUCE_PARAMS x, y
#define REDUCE_X_AUXILIARY(i) Float2 a = READ_DOUBLE2_TEXTURE(x, i);
#define REDUCE_Y_AUXILIARY(i) Float2 b = READ_DOUBLE2_TEXTURE(y, i);
#define REDUCE_Z_AUXILIARY(i)
#define REDUCE_X_OPERATION(i) (a.x*b.x + a.y*b.y)
#define REDUCE_Y_OPERATION(i) (a.x*b.y - a.y*b.x)
#define REDUCE_Z_OPERATION(i) (a.x*a.x + a.y*a.y)
#include "reduce_triple_core.h"
#undef REDUCE_FUNC_NAME
#undef REDUCE_TYPES
#undef REDUCE_PARAMS
#undef REDUCE_X_AUXILIARY
#undef REDUCE_Y_AUXILIARY
#undef REDUCE_Z_AUXILIARY
#undef REDUCE_X_OPERATION
#undef REDUCE_Y_OPERATION
#undef REDUCE_Z_OPERATION

template <unsigned int reduce_threads, typename Float2>
#define REDUCE_FUNC_NAME(suffix) cDotProductNormAS##suffix
#define REDUCE_TYPES Float2 *a, Float2 *b
#define REDUCE_PARAMS a, b
#define REDUCE_X_AUXILIARY(i)
#define REDUCE_Y_AUXILIARY(i)
#define REDUCE_Z_AUXILIARY(i)
#define REDUCE_X_OPERATION(i) (a[i].x*b[i].x + a[i].y*b[i].y)
#define REDUCE_Y_OPERATION(i) (a[i].x*b[i].y - a[i].y*b[i].x)
#define REDUCE_Z_OPERATION(i) (a[i].x*a[i].x + a[i].y*a[i].y)
#include "reduce_triple_core.h"
#undef REDUCE_FUNC_NAME
#undef REDUCE_TYPES
#undef REDUCE_PARAMS
#undef REDUCE_X_AUXILIARY
#undef REDUCE_Y_AUXILIARY
#undef REDUCE_Z_AUXILIARY
#undef REDUCE_X_OPERATION
#undef REDUCE_Y_OPERATION
#undef REDUCE_Z_OPERATION

template <unsigned int reduce_threads, typename Float2>
#define REDUCE_FUNC_NAME(suffix) cDotProductNormAH##suffix
#define REDUCE_TYPES Float2 *xN, Float2 *yN, int stride
#define REDUCE_PARAMS xN, yN, stride
#define REDUCE_X_AUXILIARY(i)						\
  READ_HALF_SPINOR(x, texHalf1, stride);				\
  READ_HALF_SPINOR(y, texHalf2, stride);				\
  REAL_DOT_FLOAT4(norm0, x0, x0);					\
  REAL_DOT_FLOAT4(norm1, x1, x1);					\
  REAL_DOT_FLOAT4(norm2, x2, x2);					\
  REAL_DOT_FLOAT4(norm3, x3, x3);					\
  REAL_DOT_FLOAT4(norm4, x4, x4);					\
  REAL_DOT_FLOAT4(norm5, x5, x5);					\
  norm0 += norm1; norm2 += norm3; norm4 += norm5; norm0 += norm2, norm0 += norm4;
#define REDUCE_Y_AUXILIARY(i)						\
  REAL_DOT_FLOAT4(rdot0, x0, y0);					\
  REAL_DOT_FLOAT4(rdot1, x1, y1);					\
  REAL_DOT_FLOAT4(rdot2, x2, y2);					\
  REAL_DOT_FLOAT4(rdot3, x3, y3);					\
  REAL_DOT_FLOAT4(rdot4, x4, y4);					\
  REAL_DOT_FLOAT4(rdot5, x5, y5);					\
  rdot0 += rdot1; rdot2 += rdot3; rdot4 += rdot5; rdot0 += rdot2; rdot0 += rdot4;
#define REDUCE_Z_AUXILIARY(i)						\
  IMAG_DOT_FLOAT4(idot0, x0, y0);					\
  IMAG_DOT_FLOAT4(idot1, x1, y1);					\
  IMAG_DOT_FLOAT4(idot2, x2, y2);					\
  IMAG_DOT_FLOAT4(idot3, x3, y3);					\
  IMAG_DOT_FLOAT4(idot4, x4, y4);					\
  IMAG_DOT_FLOAT4(idot5, x5, y5);					\
  idot0 += idot1; idot2 += idot3; idot4 += idot5; idot0 += idot2; idot0 += idot4;  
#define REDUCE_X_OPERATION(i) (xc*yc*rdot0)
#define REDUCE_Y_OPERATION(i) (xc*yc*idot0)
#define REDUCE_Z_OPERATION(i) (xc*xc*norm0)
#include "reduce_triple_core.h"
#undef REDUCE_FUNC_NAME
#undef REDUCE_TYPES
#undef REDUCE_PARAMS
#undef REDUCE_X_AUXILIARY
#undef REDUCE_Y_AUXILIARY
#undef REDUCE_Z_AUXILIARY
#undef REDUCE_X_OPERATION
#undef REDUCE_Y_OPERATION
#undef REDUCE_Z_OPERATION

template <unsigned int reduce_threads, typename Float2>
#define REDUCE_FUNC_NAME(suffix) cDotProductNormAHSt##suffix
#define REDUCE_TYPES Float2 *xN, Float2 *yN, int stride
#define REDUCE_PARAMS xN, yN, stride
#define REDUCE_X_AUXILIARY(i)						\
  READ_HALF_SPINOR_ST(x, texHalfSt1, stride);				\
  READ_HALF_SPINOR_ST(y, texHalfSt2, stride);				\
  REAL_DOT_FLOAT2(norm0, x0, x0);					\
  REAL_DOT_FLOAT2(norm1, x1, x1);					\
  REAL_DOT_FLOAT2(norm2, x2, x2);					\
  norm0 += norm1; norm0 += norm2;
#define REDUCE_Y_AUXILIARY(i)						\
  REAL_DOT_FLOAT2(rdot0, x0, y0);					\
  REAL_DOT_FLOAT2(rdot1, x1, y1);					\
  REAL_DOT_FLOAT2(rdot2, x2, y2);					\
  rdot0 += rdot1; rdot0 += rdot2;
#define REDUCE_Z_AUXILIARY(i)						\
  IMAG_DOT_FLOAT2(idot0, x0, y0);					\
  IMAG_DOT_FLOAT2(idot1, x1, y1);					\
  IMAG_DOT_FLOAT2(idot2, x2, y2);					\
  idot0 += idot1; idot0 += idot2;
#define REDUCE_X_OPERATION(i) (xc*yc*rdot0)
#define REDUCE_Y_OPERATION(i) (xc*yc*idot0)
#define REDUCE_Z_OPERATION(i) (xc*xc*norm0)
#include "reduce_triple_core.h"
#undef REDUCE_FUNC_NAME
#undef REDUCE_TYPES
#undef REDUCE_PARAMS
#undef REDUCE_X_AUXILIARY
#undef REDUCE_Y_AUXILIARY
#undef REDUCE_Z_AUXILIARY
#undef REDUCE_X_OPERATION
#undef REDUCE_Y_OPERATION
#undef REDUCE_Z_OPERATION

double3 cDotProductNormACuda(cudaColorSpinorField &x, cudaColorSpinorField &y) {
  if (x.SiteSubset() == QUDA_FULL_SITE_SUBSET) 
    return cDotProductNormACuda(x.Even(), y.Even()) + cDotProductNormACuda(x.Odd(), y.Odd());

  const int id = 27;
  quda::blas_flops += 6*x.RealLength();
  checkSpinor(x,y);
  int length = x.Length()/2;
  quda::blas_bytes += 2*x.RealLength()*x.Precision();
  if (x.Precision() == QUDA_DOUBLE_PRECISION) {
    bindTexture(&x, &y);
    return cDotProductNormADCuda((double2*)x.V(), (double2*)y.V(), length, id, x.Precision());
  } else if (x.Precision() == QUDA_SINGLE_PRECISION) {
    return cDotProductNormASCuda((float2*)x.V(), (float2*)y.V(), length, id, x.Precision());
  } else {
    bindTexture(&x, &y);
    quda::blas_bytes += 2*x.Volume()*sizeof(float);
    if (x.Nspin() == 4){ //wilson
      return cDotProductNormAHCuda((float*)x.Norm(), (float*)y.Norm(), x.Stride(), x.Volume(), id, x.Precision());
    } else if (x.Nspin() == 1){ //staggered
      return cDotProductNormAHStCuda((float*)x.Norm(), (float*)y.Norm(), x.Stride(), x.Volume(), id, x.Precision());
    }else{
      errorQuda("%s: nSpin(%d) is not supported\n", __FUNCTION__, x.Nspin());            
    }
  }

  exit(-1);
}

//
// double3 cDotProductNormBCuda(float2 *a, float2 *b, int n) {}
//
template <unsigned int reduce_threads, typename Float2>
#define REDUCE_FUNC_NAME(suffix) cDotProductNormBD##suffix
#define REDUCE_TYPES Float2 *x, Float2 *y
#define REDUCE_PARAMS x, y
#define REDUCE_X_AUXILIARY(i) Float2 a = READ_DOUBLE2_TEXTURE(x, i);
#define REDUCE_Y_AUXILIARY(i) Float2 b = READ_DOUBLE2_TEXTURE(y, i);
#define REDUCE_Z_AUXILIARY(i)
#define REDUCE_X_OPERATION(i) (a.x*b.x + a.y*b.y)
#define REDUCE_Y_OPERATION(i) (a.x*b.y - a.y*b.x)
#define REDUCE_Z_OPERATION(i) (b.x*b.x + b.y*b.y)
#include "reduce_triple_core.h"
#undef REDUCE_FUNC_NAME
#undef REDUCE_TYPES
#undef REDUCE_PARAMS
#undef REDUCE_X_AUXILIARY
#undef REDUCE_Y_AUXILIARY
#undef REDUCE_Z_AUXILIARY
#undef REDUCE_X_OPERATION
#undef REDUCE_Y_OPERATION
#undef REDUCE_Z_OPERATION

template <unsigned int reduce_threads, typename Float2>
#define REDUCE_FUNC_NAME(suffix) cDotProductNormBS##suffix
#define REDUCE_TYPES Float2 *a, Float2 *b
#define REDUCE_PARAMS a, b
#define REDUCE_X_AUXILIARY(i)
#define REDUCE_Y_AUXILIARY(i)
#define REDUCE_Z_AUXILIARY(i)
#define REDUCE_X_OPERATION(i) (a[i].x*b[i].x + a[i].y*b[i].y)
#define REDUCE_Y_OPERATION(i) (a[i].x*b[i].y - a[i].y*b[i].x)
#define REDUCE_Z_OPERATION(i) (b[i].x*b[i].x + b[i].y*b[i].y)
#include "reduce_triple_core.h"
#undef REDUCE_FUNC_NAME
#undef REDUCE_TYPES
#undef REDUCE_PARAMS
#undef REDUCE_X_AUXILIARY
#undef REDUCE_Y_AUXILIARY
#undef REDUCE_Z_AUXILIARY
#undef REDUCE_X_OPERATION
#undef REDUCE_Y_OPERATION
#undef REDUCE_Z_OPERATION

template <unsigned int reduce_threads, typename Float2>
#define REDUCE_FUNC_NAME(suffix) cDotProductNormBH##suffix
#define REDUCE_TYPES Float2 *xN, Float2 *yN, int stride
#define REDUCE_PARAMS xN, yN, stride
#define REDUCE_X_AUXILIARY(i)						\
  READ_HALF_SPINOR(x, texHalf1, stride);				\
  READ_HALF_SPINOR(y, texHalf2, stride);				\
  REAL_DOT_FLOAT4(norm0, y0, y0);					\
  REAL_DOT_FLOAT4(norm1, y1, y1);					\
  REAL_DOT_FLOAT4(norm2, y2, y2);					\
  REAL_DOT_FLOAT4(norm3, y3, y3);					\
  REAL_DOT_FLOAT4(norm4, y4, y4);					\
  REAL_DOT_FLOAT4(norm5, y5, y5);					\
  norm0 += norm1; norm2 += norm3; norm4 += norm5; norm0 += norm2, norm0 += norm4;
#define REDUCE_Y_AUXILIARY(i)						\
  REAL_DOT_FLOAT4(rdot0, x0, y0);					\
  REAL_DOT_FLOAT4(rdot1, x1, y1);					\
  REAL_DOT_FLOAT4(rdot2, x2, y2);					\
  REAL_DOT_FLOAT4(rdot3, x3, y3);					\
  REAL_DOT_FLOAT4(rdot4, x4, y4);					\
  REAL_DOT_FLOAT4(rdot5, x5, y5);					\
  rdot0 += rdot1; rdot2 += rdot3; rdot4 += rdot5; rdot0 += rdot2; rdot0 += rdot4;
#define REDUCE_Z_AUXILIARY(i)						\
  IMAG_DOT_FLOAT4(idot0, x0, y0);					\
  IMAG_DOT_FLOAT4(idot1, x1, y1);					\
  IMAG_DOT_FLOAT4(idot2, x2, y2);					\
  IMAG_DOT_FLOAT4(idot3, x3, y3);					\
  IMAG_DOT_FLOAT4(idot4, x4, y4);					\
  IMAG_DOT_FLOAT4(idot5, x5, y5);					\
  idot0 += idot1; idot2 += idot3; idot4 += idot5; idot0 += idot2; idot0 += idot4;
#define REDUCE_X_OPERATION(i) (xc*yc*rdot0)
#define REDUCE_Y_OPERATION(i) (xc*yc*idot0)
#define REDUCE_Z_OPERATION(i) (yc*yc*norm0)
#include "reduce_triple_core.h"
#undef REDUCE_FUNC_NAME
#undef REDUCE_TYPES
#undef REDUCE_PARAMS
#undef REDUCE_X_AUXILIARY
#undef REDUCE_Y_AUXILIARY
#undef REDUCE_Z_AUXILIARY
#undef REDUCE_X_OPERATION
#undef REDUCE_Y_OPERATION
#undef REDUCE_Z_OPERATION

template <unsigned int reduce_threads, typename Float2>
#define REDUCE_FUNC_NAME(suffix) cDotProductNormBHSt##suffix
#define REDUCE_TYPES Float2 *xN, Float2 *yN, int stride
#define REDUCE_PARAMS xN, yN, stride
#define REDUCE_X_AUXILIARY(i)						\
  READ_HALF_SPINOR_ST(x, texHalfSt1, stride);				\
  READ_HALF_SPINOR_ST(y, texHalfSt2, stride);				\
  REAL_DOT_FLOAT2(norm0, y0, y0);					\
  REAL_DOT_FLOAT2(norm1, y1, y1);					\
  REAL_DOT_FLOAT2(norm2, y2, y2);					\
  norm0 += norm1; norm0 += norm2;
#define REDUCE_Y_AUXILIARY(i)						\
  REAL_DOT_FLOAT2(rdot0, x0, y0);					\
  REAL_DOT_FLOAT2(rdot1, x1, y1);					\
  REAL_DOT_FLOAT2(rdot2, x2, y2);					\
  rdot0 += rdot1; rdot0 += rdot2;
#define REDUCE_Z_AUXILIARY(i)						\
  IMAG_DOT_FLOAT2(idot0, x0, y0);					\
  IMAG_DOT_FLOAT2(idot1, x1, y1);					\
  IMAG_DOT_FLOAT2(idot2, x2, y2);					\
  idot0 += idot1; idot0 += idot2;
#define REDUCE_X_OPERATION(i) (xc*yc*rdot0)
#define REDUCE_Y_OPERATION(i) (xc*yc*idot0)
#define REDUCE_Z_OPERATION(i) (yc*yc*norm0)
#include "reduce_triple_core.h"
#undef REDUCE_FUNC_NAME
#undef REDUCE_TYPES
#undef REDUCE_PARAMS
#undef REDUCE_X_AUXILIARY
#undef REDUCE_Y_AUXILIARY
#undef REDUCE_Z_AUXILIARY
#undef REDUCE_X_OPERATION
#undef REDUCE_Y_OPERATION
#undef REDUCE_Z_OPERATION

double3 cDotProductNormBCuda(cudaColorSpinorField &x, cudaColorSpinorField &y) {
  if (x.SiteSubset() == QUDA_FULL_SITE_SUBSET) 
    return cDotProductNormBCuda(x.Even(), y.Even()) + cDotProductNormBCuda(x.Odd(), y.Odd());

  const int id = 28;
  quda::blas_flops += 6*x.RealLength();
  checkSpinor(x,y);
  int length = x.Length()/2;
  quda::blas_bytes += 2*x.RealLength()*x.Precision();
  if (x.Precision() == QUDA_DOUBLE_PRECISION) {
    bindTexture(&x, &y);
    return cDotProductNormBDCuda((double2*)x.V(), (double2*)y.V(), length, id, x.Precision());
  } else if (x.Precision() == QUDA_SINGLE_PRECISION) {
    return cDotProductNormBSCuda((float2*)x.V(), (float2*)y.V(), length, id, x.Precision());
  } else {
    bindTexture(&x, &y);
    quda::blas_bytes += 2*x.Volume()*sizeof(float);
    if (x.Nspin() == 4){ //wilson
      return cDotProductNormBHCuda((float*)x.Norm(), (float*)y.Norm(), x.Stride(), x.Volume(), id, x.Precision());
    } else if (x.Nspin() == 1){ //staggered
      return cDotProductNormBHStCuda((float*)x.Norm(), (float*)y.Norm(), x.Stride(), x.Volume(), id, x.Precision());
    }else{
      errorQuda("%s: nSpin(%d) is not supported\n", __FUNCTION__, x.Nspin());            
    }
  }

  exit(-1);
}


//
// double3 caxpbypzYmbwcDotProductWYNormYCuda(float2 a, float2 *x, float2 b, float2 *y, 
// float2 *z, float2 *w, float2 *u, int len)
//
template <unsigned int reduce_threads, typename Float2>
#define REDUCE_FUNC_NAME(suffix) caxpbypzYmbwcDotProductUYNormYD##suffix
#define REDUCE_TYPES Float2 a, Float2 *x, Float2 b, Float2 *y, Float2 *z, Float2 *w, Float2 *u
#define REDUCE_PARAMS a, x, b, y, z, w, u
#define REDUCE_X_AUXILIARY(i)				\
  Float2 X = READ_DOUBLE2_TEXTURE(x, i);		\
  Float2 Y = READ_DOUBLE2_TEXTURE(y, i);		\
  Float2 W = READ_DOUBLE2_TEXTURE(w, i);				
#define REDUCE_Y_AUXILIARY(i)			\
  Float2 Z = read_Float2(z, i);			\
  Z.x += a.x*X.x - a.y*X.y;			\
  Z.y += a.y*X.x + a.x*X.y;			\
  Z.x += b.x*Y.x - b.y*Y.y;			\
  Z.y += b.y*Y.x + b.x*Y.y;			\
  Y.x -= b.x*W.x - b.y*W.y;			\
  Y.y -= b.y*W.x + b.x*W.y;	
#define REDUCE_Z_AUXILIARY(i)	      \
  z[i] = make_Float2(Z);	      \
  y[i] = make_Float2(Y);	      
#define REDUCE_X_OPERATION(i) (u[i].x*y[i].x + u[i].y*y[i].y)
#define REDUCE_Y_OPERATION(i) (u[i].x*y[i].y - u[i].y*y[i].x)
#define REDUCE_Z_OPERATION(i) (y[i].x*y[i].x + y[i].y*y[i].y)
#include "reduce_triple_core.h"
#undef REDUCE_FUNC_NAME
#undef REDUCE_TYPES
#undef REDUCE_PARAMS
#undef REDUCE_X_AUXILIARY
#undef REDUCE_Y_AUXILIARY
#undef REDUCE_Z_AUXILIARY
#undef REDUCE_X_OPERATION
#undef REDUCE_Y_OPERATION
#undef REDUCE_Z_OPERATION

template <unsigned int reduce_threads, typename Float2>
#define REDUCE_FUNC_NAME(suffix) caxpbypzYmbwcDotProductUYNormYS##suffix
#define REDUCE_TYPES Float2 a, Float2 *x, Float2 b, Float2 *y, Float2 *z, Float2 *w, Float2 *u
#define REDUCE_PARAMS a, x, b, y, z, w, u
#define REDUCE_X_AUXILIARY(i)				\
  Float2 X = read_Float2(x, i);		\
  Float2 Y = read_Float2(y, i);		\
  Float2 W = read_Float2(w, i);		
#define REDUCE_Y_AUXILIARY(i)			\
  Float2 Z = read_Float2(z, i);	\
  Z.x += a.x*X.x - a.y*X.y;			\
  Z.y += a.y*X.x + a.x*X.y;			\
  Z.x += b.x*Y.x - b.y*Y.y;			\
  Z.y += b.y*Y.x + b.x*Y.y;			\
  Y.x -= b.x*W.x - b.y*W.y;			\
  Y.y -= b.y*W.x + b.x*W.y;	
#define REDUCE_Z_AUXILIARY(i)	      \
  z[i] = make_Float2(Z);	      \
  y[i] = make_Float2(Y);	      
#define REDUCE_X_OPERATION(i) (u[i].x*y[i].x + u[i].y*y[i].y)
#define REDUCE_Y_OPERATION(i) (u[i].x*y[i].y - u[i].y*y[i].x)
#define REDUCE_Z_OPERATION(i) (y[i].x*y[i].x + y[i].y*y[i].y)
#include "reduce_triple_core.h"
#undef REDUCE_FUNC_NAME
#undef REDUCE_TYPES
#undef REDUCE_PARAMS
#undef REDUCE_X_AUXILIARY
#undef REDUCE_Y_AUXILIARY
#undef REDUCE_Z_AUXILIARY
#undef REDUCE_X_OPERATION
#undef REDUCE_Y_OPERATION
#undef REDUCE_Z_OPERATION

//
// double3 caxpbypzYmbwcDotProductWYNormYCuda(float2 a, float2 *x, float2 b, float2 *y, 
// float2 *z, float2 *w, float2 *u, int len)
//
template <unsigned int reduce_threads, typename Float2>
#define REDUCE_FUNC_NAME(suffix) caxpbypzYmbwcDotProductUYNormYH##suffix
#define REDUCE_TYPES Float2 a, Float2 b, short4 *yH, float *yN, short4 *zH, float *zN, float *wN, float *uN, int stride
#define REDUCE_PARAMS a, b, yH, yN, zH, zN, wN, uN, stride
#define REDUCE_X_AUXILIARY(i)						\
  RECONSTRUCT_HALF_SPINOR(x, texHalf1, texNorm1, stride);		\
  RECONSTRUCT_HALF_SPINOR(y, texHalf2, texNorm2, stride);		\
  RECONSTRUCT_HALF_SPINOR(z, texHalf3, texNorm3, stride);		\
  CAXPBYPZ_FLOAT4(a, x0, b, y0, z0);					\
  CAXPBYPZ_FLOAT4(a, x1, b, y1, z1);					\
  CAXPBYPZ_FLOAT4(a, x2, b, y2, z2);					\
  CAXPBYPZ_FLOAT4(a, x3, b, y3, z3);					\
  CAXPBYPZ_FLOAT4(a, x4, b, y4, z4);					\
  CAXPBYPZ_FLOAT4(a, x5, b, y5, z5);					\
  CONSTRUCT_HALF_SPINOR_FROM_SINGLE(zH, zN, z, stride);			\
  READ_HALF_SPINOR(w, texHalf4, stride);				\
  float2 bwc = -wc*b;							\
  CAXPY_FLOAT4(bwc, w0, y0);						\
  CAXPY_FLOAT4(bwc, w1, y1);						\
  CAXPY_FLOAT4(bwc, w2, y2);						\
  CAXPY_FLOAT4(bwc, w3, y3);						\
  CAXPY_FLOAT4(bwc, w4, y4);						\
  CAXPY_FLOAT4(bwc, w5, y5);						\
  REAL_DOT_FLOAT4(norm0, y0, y0);					\
  REAL_DOT_FLOAT4(norm1, y1, y1);					\
  REAL_DOT_FLOAT4(norm2, y2, y2);					\
  REAL_DOT_FLOAT4(norm3, y3, y3);					\
  REAL_DOT_FLOAT4(norm4, y4, y4);					\
  REAL_DOT_FLOAT4(norm5, y5, y5);					\
  CONSTRUCT_HALF_SPINOR_FROM_SINGLE(yH, yN, y, stride);			
#define REDUCE_Y_AUXILIARY(i)						\
  READ_HALF_SPINOR(u, texHalf5, stride);				\
  REAL_DOT_FLOAT4(rdot0, u0, y0);					\
  REAL_DOT_FLOAT4(rdot1, u1, y1);					\
  REAL_DOT_FLOAT4(rdot2, u2, y2);					\
  REAL_DOT_FLOAT4(rdot3, u3, y3);					\
  REAL_DOT_FLOAT4(rdot4, u4, y4);					\
  REAL_DOT_FLOAT4(rdot5, u5, y5);					\
  IMAG_DOT_FLOAT4(idot0, u0, y0);					\
  IMAG_DOT_FLOAT4(idot1, u1, y1);					\
  IMAG_DOT_FLOAT4(idot2, u2, y2);					\
  IMAG_DOT_FLOAT4(idot3, u3, y3);					\
  IMAG_DOT_FLOAT4(idot4, u4, y4);					\
  IMAG_DOT_FLOAT4(idot5, u5, y5);					
#define REDUCE_Z_AUXILIARY(i)						\
  norm0 += norm1; norm2 += norm3; norm4 += norm5; norm0 += norm2, norm0 += norm4; \
  rdot0 += rdot1; rdot2 += rdot3; rdot4 += rdot5; rdot0 += rdot2; rdot0 += rdot4; \
  idot0 += idot1; idot2 += idot3; idot4 += idot5; idot0 += idot2; idot0 += idot4;

#define REDUCE_X_OPERATION(i) (uc*rdot0)
#define REDUCE_Y_OPERATION(i) (uc*idot0)
#define REDUCE_Z_OPERATION(i) (norm0)

#include "reduce_triple_core.h"
#undef REDUCE_FUNC_NAME
#undef REDUCE_TYPES
#undef REDUCE_PARAMS
#undef REDUCE_X_AUXILIARY
#undef REDUCE_Y_AUXILIARY
#undef REDUCE_Z_AUXILIARY
#undef REDUCE_X_OPERATION
#undef REDUCE_Y_OPERATION
#undef REDUCE_Z_OPERATION


template <unsigned int reduce_threads, typename Float2>
#define REDUCE_FUNC_NAME(suffix) caxpbypzYmbwcDotProductUYNormYH##suffix
#define REDUCE_TYPES Float2 a, Float2 b, short2 *yH, float *yN, short2 *zH, float *zN, float *wN, float *uN, int stride
#define REDUCE_PARAMS a, b, yH, yN, zH, zN, wN, uN, stride
#define REDUCE_X_AUXILIARY(i)						\
  RECONSTRUCT_HALF_SPINOR_ST(x, texHalfSt1, texNorm1, stride);		\
  RECONSTRUCT_HALF_SPINOR_ST(y, texHalfSt2, texNorm2, stride);		\
  RECONSTRUCT_HALF_SPINOR_ST(z, texHalfSt3, texNorm3, stride);		\
  CAXPBYPZ_FLOAT2(a, x0, b, y0, z0);					\
  CAXPBYPZ_FLOAT2(a, x1, b, y1, z1);					\
  CAXPBYPZ_FLOAT2(a, x2, b, y2, z2);					\
  CONSTRUCT_HALF_SPINOR_FROM_SINGLE_ST(zH, zN, z, stride);			\
  READ_HALF_SPINOR_ST(w, texHalfSt4, stride);				\
  float2 bwc = -wc*b;							\
  CAXPY_FLOAT2(bwc, w0, y0);						\
  CAXPY_FLOAT2(bwc, w1, y1);						\
  CAXPY_FLOAT2(bwc, w2, y2);						\
  REAL_DOT_FLOAT2(norm0, y0, y0);					\
  REAL_DOT_FLOAT2(norm1, y1, y1);					\
  REAL_DOT_FLOAT2(norm2, y2, y2);					\
  CONSTRUCT_HALF_SPINOR_FROM_SINGLE_ST(yH, yN, y, stride);			
#define REDUCE_Y_AUXILIARY(i)						\
  READ_HALF_SPINOR_ST(u, texHalfSt5, stride);				\
  REAL_DOT_FLOAT2(rdot0, u0, y0);					\
  REAL_DOT_FLOAT2(rdot1, u1, y1);					\
  REAL_DOT_FLOAT2(rdot2, u2, y2);					\
  IMAG_DOT_FLOAT2(idot0, u0, y0);					\
  IMAG_DOT_FLOAT2(idot1, u1, y1);					\
  IMAG_DOT_FLOAT2(idot2, u2, y2);					
#define REDUCE_Z_AUXILIARY(i)						\
  norm0 += norm1; norm0 += norm2; \
  rdot0 += rdot1; rdot0 += rdot2; \
  idot0 += idot1; idot0 += idot2; 

#define REDUCE_X_OPERATION(i) (uc*rdot0)
#define REDUCE_Y_OPERATION(i) (uc*idot0)
#define REDUCE_Z_OPERATION(i) (norm0)

#include "reduce_triple_core.h"
#undef REDUCE_FUNC_NAME
#undef REDUCE_TYPES
#undef REDUCE_PARAMS
#undef REDUCE_X_AUXILIARY
#undef REDUCE_Y_AUXILIARY
#undef REDUCE_Z_AUXILIARY
#undef REDUCE_X_OPERATION
#undef REDUCE_Y_OPERATION
#undef REDUCE_Z_OPERATION

// This convoluted kernel does the following: z += a*x + b*y, y -= b*w, norm = (y,y), dot = (u, y)
double3 caxpbypzYmbwcDotProductUYNormYCuda(const quda::Complex &a, cudaColorSpinorField &x, const quda::Complex &b, cudaColorSpinorField &y,
					   cudaColorSpinorField &z, cudaColorSpinorField &w, cudaColorSpinorField &u) {
  if (x.SiteSubset() == QUDA_FULL_SITE_SUBSET) 
    return caxpbypzYmbwcDotProductUYNormYCuda(a, x.Even(), b, y.Even(), z.Even(), w.Even(), u.Even()) + 
      caxpbypzYmbwcDotProductUYNormYCuda(a, x.Odd(), b, y.Odd(), z.Odd(), w.Odd(), u.Odd());

  const int id = 29;
  quda::blas_flops += 18*x.RealLength();
  checkSpinor(x,y);
  checkSpinor(x,z);
  checkSpinor(x,w);
  checkSpinor(x,u);
  int length = x.Length()/2;
  quda::blas_bytes += 7*x.RealLength()*x.Precision();
  if (x.Precision() == QUDA_DOUBLE_PRECISION) {
    double2 a2 = make_double2(real(a), imag(a));
    double2 b2 = make_double2(real(b), imag(b));
    bindTexture(&x, &y, &z, &w, &u);
    return caxpbypzYmbwcDotProductUYNormYDCuda(a2, (double2*)x.V(), b2, (double2*)y.V(), (double2*)z.V(), 
					       (double2*)w.V(), (double2*)u.V(), length, id, x.Precision());
  } else if (x.Precision() == QUDA_SINGLE_PRECISION) {
    float2 a2 = make_float2(real(a), imag(a));
    float2 b2 = make_float2(real(b), imag(b));
    return caxpbypzYmbwcDotProductUYNormYSCuda(a2, (float2*)x.V(), b2, (float2*)y.V(), (float2*)z.V(),
					       (float2*)w.V(), (float2*)u.V(), length, id, x.Precision());
  } else {
    // fused nSpin=4 kernel is slow on Fermi
    // N.B. this introduces an extra half truncation so will affect convergence (for the better?)
    if (!blasTuning && (__COMPUTE_CAPABILITY__ >= 200) && x.Nspin() == 4) {
      caxpbypzYmbwCuda(a, x, b, y, z, w);
      return cDotProductNormBCuda(u, y);
    }
     
    bindTexture(&x, &y, &z, &w, &u);
    quda::blas_bytes += 7*x.Volume()*sizeof(float);
    float2 a2 = make_float2(real(a), imag(a));
    float2 b2 = make_float2(real(b), imag(b));
    if (x.Nspin() == 4) { // wilson
      return caxpbypzYmbwcDotProductUYNormYHCuda(a2, b2, (short4*)y.V(), (float*)y.Norm(), 
						 (short4*)z.V(), (float*)z.Norm(), (float*)w.Norm(), (float*)u.Norm(), 
						 y.Stride(), y.Volume(), id, x.Precision());
    } else if (x.Nspin() == 1){ // staggered
      return caxpbypzYmbwcDotProductUYNormYHCuda(a2, b2, (short2*)y.V(), (float*)y.Norm(), 
						 (short2*)z.V(), (float*)z.Norm(), (float*)w.Norm(), (float*)u.Norm(), 
						 y.Stride(), y.Volume(), id, x.Precision());
    } else {
      errorQuda("%s: nSpin(%d) is not supported\n", __FUNCTION__, x.Nspin());            
    }
  }

  exit(-1);
}

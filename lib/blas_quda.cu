#include <stdlib.h>
#include <stdio.h>

#include <quda_internal.h>
#include <blas_quda.h>
#include <color_spinor_field.h>
#include <face_quda.h> // this is where the MPI / QMP depdendent code is

#include <cuComplex.h>

#define REDUCE_MAX_BLOCKS 65536

#if (__COMPUTE_CAPABILITY__ >= 130)
#define QudaSumFloat double
#define QudaSumFloat2 double2
#define QudaSumFloat3 double3
#else
#define QudaSumFloat doublesingle
#define QudaSumFloat doublesingle2
#define QudaSumFloat doublesingle3
#endif

#include <double_single.h>

// These are used for reduction kernels
static QudaSumFloat *d_reduce=0;
static QudaSumFloat *h_reduce=0;

namespace quda {
  unsigned long long blas_flops;
  unsigned long long blas_bytes;
}

static dim3 blasBlock;
static dim3 blasGrid;

// generated by blas_test
#include <blas_param.h>

#include <float_vector.h>

void zeroCuda(cudaColorSpinorField &a) { a.zero(); }

// blasTuning = 1 turns off error checking
static QudaTune blasTuning = QUDA_TUNE_NO;

namespace quda {

void initBlas(void)
{  
  if (!d_reduce) {
    if (cudaMalloc((void**) &d_reduce, 3*REDUCE_MAX_BLOCKS*sizeof(QudaSumFloat)) == cudaErrorMemoryAllocation) {
      errorQuda("Error allocating device reduction array");
    }
  }

  if (!h_reduce) {
    printf("Host reduce size %lu\n", 3*REDUCE_MAX_BLOCKS*sizeof(QudaSumFloat));
    if (cudaMallocHost((void**) &h_reduce, 3*REDUCE_MAX_BLOCKS*sizeof(QudaSumFloat)) == cudaErrorMemoryAllocation) {
      errorQuda("Error allocating host reduction array");
    }
  }

}


void endBlas(void)
{
  if (d_reduce) {
    cudaFree(d_reduce);
    d_reduce = 0;
  }
  if (h_reduce) {
    cudaFreeHost(h_reduce);
    h_reduce = 0;
  }
}

void setBlasTuning(QudaTune tune)
{
  blasTuning = tune;
}

void setBlasParam(int kernel, int prec, int threads, int blocks)
{
  blas_threads[kernel][prec] = threads;
  blas_blocks[kernel][prec] = blocks;
}

} 

// FIXME: this should be queried from the device
#if (__COMPUTE_CAPACITY__ < 200)
#define MAX_BLOCK 512
#else
#define MAX_BLOCK 1024
#endif

void setBlock(int kernel, int length, QudaPrecision precision)
{
  int prec;
  switch(precision) {
  case QUDA_HALF_PRECISION:
    prec = 0;
    break;
  case QUDA_SINGLE_PRECISION:
    prec = 1;
    break;
  case QUDA_DOUBLE_PRECISION:
    prec = 2;
    break;
  }

  //printf("threads %d %d %d\n", kernel, precision, blas_threads[kernel][prec]);
  blasBlock.x = min(MAX_BLOCK, blas_threads[kernel][prec]);
  blasBlock.y = 1;
  blasBlock.z = 1;

  // the min here prevents creating too big a grid
  int grid = min(blas_blocks[kernel][prec], max(length/blas_threads[kernel][prec], 1));
  blasGrid.x = grid;
  blasGrid.y = 1;
  blasGrid.z = 1;

  //printf("Setting block = (%d, %d, %d), grid = (%d, %d, %d)\n", blasBlock.x,
  //	 blasBlock.y, blasBlock.z, blasGrid.x, blasGrid.y, blasGrid.z);
}

#if (__COMPUTE_CAPABILITY__ >= 130)
__inline__ __device__ double2 fetch_double2(texture<int4, 1> t, int i)
{
  int4 v = tex1Dfetch(t,i);
  return make_double2(__hiloint2double(v.y, v.x), __hiloint2double(v.w, v.z));
}
#else
__inline__ __device__ double2 fetch_double2(texture<int4, 1> t, int i)
{
  // do nothing
  return make_double2(0.0, 0.0);
}
#endif

#include <texture.h>



float2 __device__ read_Float2(float2 *x, int i) {
  return make_float2(x[i].x, x[i].y);
}

double2 __device__ read_Float2(double2 *x, int i) {
  return make_double2(x[i].x, x[i].y);
}

#if FERMI_NO_DBLE_TEX
#define READ_DOUBLE2_TEXTURE(x, i) \
  read_Float2(x, i)
#else
#define READ_DOUBLE2_TEXTURE(x, i) \
  fetch_double2(x##TexDouble2, i)
#endif

#define READ_FLOAT2_TEXTURE(x, i) \
  tex1Dfetch(x##TexSingle2, i)

float2 __device__ make_Float2(float2 x) {
  return make_float2(x.x, x.y);
}

double2 __device__ make_Float2(double2 x) {
  return make_double2(x.x, x.y);
}

__device__ void caxpy_(const float2 &a, const float4 &x, float4 &y) {
  y.x += a.x*x.x; y.x -= a.y*x.y;
  y.y += a.y*x.x; y.y += a.x*x.y;
  y.z += a.x*x.z; y.z -= a.y*x.w;
  y.w += a.y*x.z; y.w += a.x*x.w;
}

__device__ void caxpy_(const float2 &a, const float2 &x, float2 &y) {
  y.x += a.x*x.x; y.x -= a.y*x.y;
  y.y += a.y*x.x; y.y += a.x*x.y;
}

__device__ void caxpy_(const double2 &a, const double2 &x, double2 &y) {
  y.x += a.x*x.x; y.x -= a.y*x.y;
  y.y += a.y*x.x; y.y += a.x*x.y;
}

__device__ void caxpby_(const float2 &a, const float4 &x, const float2 &b, float4 &y)					
  { float4 yy;								
  yy.x = a.x*x.x; yy.x -= a.y*x.y; yy.x += b.x*y.x; yy.x -= b.y*y.y;	
  yy.y = a.y*x.x; yy.y += a.x*x.y; yy.y += b.y*y.x; yy.y += b.x*y.y;	
  yy.z = a.x*x.z; yy.z -= a.y*x.w; yy.z += b.x*y.z; yy.z -= b.y*y.w;	
  yy.w = a.y*x.z; yy.w += a.x*x.w; yy.w += b.y*y.z; yy.w += b.x*y.w;	
  y = yy; }

__device__ void caxpby_(const float2 &a, const float2 &x, const float2 &b, float2 &y)
  { float2 yy;								
  yy.x = a.x*x.x; yy.x -= a.y*x.y; yy.x += b.x*y.x; yy.x -= b.y*y.y;	
  yy.y = a.y*x.x; yy.y += a.x*x.y; yy.y += b.y*y.x; yy.y += b.x*y.y;	
  y = yy; }

__device__ void caxpby_(const double2 &a, const double2 &x, const double2 &b, double2 &y)				 
  { double2 yy;								
  yy.x = a.x*x.x; yy.x -= a.y*x.y; yy.x += b.x*y.x; yy.x -= b.y*y.y;	
  yy.y = a.y*x.x; yy.y += a.x*x.y; yy.y += b.y*y.x; yy.y += b.x*y.y;	
  y = yy; }

__device__ void cxpaypbz_(const float4 &x, const float2 &a, const float4 &y, const float2 &b, float4 &z) {
  float4 zz;
  zz.x = x.x + a.x*y.x; zz.x -= a.y*y.y; zz.x += b.x*z.x; zz.x -= b.y*z.y;
  zz.y = x.y + a.y*y.x; zz.y += a.x*y.y; zz.y += b.y*z.x; zz.y += b.x*z.y;
  zz.z = x.z + a.x*y.z; zz.z -= a.y*y.w; zz.z += b.x*z.z; zz.z -= b.y*z.w;
  zz.w = x.w + a.y*y.z; zz.w += a.x*y.w; zz.w += b.y*z.z; zz.w += b.x*z.w;
  z = zz;
}


__device__ void cxpaypbz_(const float2 &x, const float2 &a, const float2 &y, const float2 &b, float2 &z) {
  float2 zz;
  zz.x = x.x + a.x*y.x; zz.x -= a.y*y.y; zz.x += b.x*z.x; zz.x -= b.y*z.y;
  zz.y = x.y + a.y*y.x; zz.y += a.x*y.y; zz.y += b.y*z.x; zz.y += b.x*z.y;
  z = zz;
}

__device__ void cxpaypbz_(const double2 &x, const double2 &a, const double2 &y, const double2 &b, double2 &z) {
  double2 zz;
  zz.x = x.x + a.x*y.x; zz.x -= a.y*y.y; zz.x += b.x*z.x; zz.x -= b.y*z.y;
  zz.y = x.y + a.y*y.x; zz.y += a.x*y.y; zz.y += b.y*z.x; zz.y += b.x*z.y;
  z = zz;
}

__device__ void caxpbypz_(const float2 &a, const float4 &x, const float2 &b, const float4 &y, float4 &z) {
  z.x += a.x*x.x - a.y*x.y + b.x*y.x - b.y*y.y;   
  z.y += a.y*x.x + a.x*x.y + b.y*y.x + b.x*y.y;  
  z.z += a.x*x.z - a.y*x.w + b.x*y.z - b.y*y.w;  
  z.w += a.y*x.z + a.x*x.w + b.y*y.z + b.x*y.w;
}

__device__ void caxpbypz_(const float2 &a, const float2 &x, const float2 &b, const float2 &y, float2 &z) {
  z.x += a.x*x.x - a.y*x.y + b.x*y.x - b.y*y.y;   
  z.y += a.y*x.x + a.x*x.y + b.y*y.x + b.x*y.y;  
}

__device__ void caxpbypz_(const double2 &a, const double2 &x, const double2 &b, const double2 &y, double2 &z) {
  z.x += a.x*x.x - a.y*x.y + b.x*y.x - b.y*y.y;   
  z.y += a.y*x.x + a.x*x.y + b.y*y.x + b.x*y.y;  
}


__forceinline__ __device__ float max_fabs(const float4 &c) {
  float a = fmaxf(fabsf(c.x), fabsf(c.y));
  float b = fmaxf(fabsf(c.z), fabsf(c.w));
  return fmaxf(a, b);
};

__forceinline__ __device__ float max_fabs(const float2 &b) {
  return fmaxf(fabsf(b.x), fabsf(b.y));
};

__forceinline__ __device__ float2 make_FloatN(const double2 &a) {
  return make_float2(a.x, a.y);
}

__forceinline__ __device__ float4 make_FloatN(const double4 &a) {
  return make_float4(a.x, a.y, a.z, a.w);
}

__forceinline__ __device__ double2 make_FloatN(const float2 &a) {
  return make_double2(a.x, a.y);
}

__forceinline__ __device__ double4 make_FloatN(const float4 &a) {
  return make_double4(a.x, a.y, a.z, a.w);
}

__forceinline__ __device__ short4 make_shortN(const float4 &a) {
  return make_short4(a.x, a.y, a.z, a.w);
}

__forceinline__ __device__ short2 make_shortN(const float2 &a) {
  return make_short2(a.x, a.y);
}

__forceinline__ __device__ double max_fabs(const double4 &c) {
  double a = fmaxf(fabsf(c.x), fabsf(c.y));
  double b = fmaxf(fabsf(c.z), fabsf(c.w));
  return fmaxf(a, b);
};

__forceinline__ __device__ double max_fabs(const double2 &b) {
  return fmaxf(fabsf(b.x), fabsf(b.y));
};

__forceinline__ __device__ short4 make_shortN(const double4 &a) {
  return make_short4(a.x, a.y, a.z, a.w);
}

__forceinline__ __device__ short2 make_shortN(const double2 &a) {
  return make_short2(a.x, a.y);
}

#define checkSpinor(a, b)						\
  {									\
    if (a.Precision() != b.Precision())					\
      errorQuda("precisions do not match: %d %d", a.Precision(), b.Precision()); \
    if (a.Length() != b.Length())					\
      errorQuda("lengths do not match: %d %d", a.Length(), b.Length());	\
    if (a.Stride() != b.Stride())					\
      errorQuda("strides do not match: %d %d", a.Stride(), b.Stride());	\
  }

// For kernels with precision conversion built in
#define checkSpinorLength(a, b)						\
  {									\
    if (a.Length() != b.Length()) {					\
      errorQuda("engths do not match: %d %d", a.Length(), b.Length());	\
    }									


template <typename FloatN, int N, typename Output, typename Input>
__global__ void copyKernel(Output Y, Input X, int length) {
  unsigned int i = blockIdx.x*(blockDim.x) + threadIdx.x;
  unsigned int gridSize = gridDim.x*blockDim.x;

  while (i < length) {
    FloatN x[N];
    X.load(x, i);
    Y.save(x, i);
    i += gridSize;
  }
}


void copyCuda(cudaColorSpinorField &dst, const cudaColorSpinorField &src) {

  if (&src == &dst) return; // aliasing fields
  if (src.Nspin() != 1 && src.Nspin() != 4) errorQuda("nSpin(%d) not supported\n", src.Nspin());

  if (dst.SiteSubset() == QUDA_FULL_SITE_SUBSET || src.SiteSubset() == QUDA_FULL_SITE_SUBSET) {
    copyCuda(dst.Even(), src.Even());
    copyCuda(dst.Odd(), src.Odd());
    return;
  }

  // For a given dst precision, there are two non-trivial possibilities for the
  // src precision.  The higher one corresponds to kernel index 0 (in the table
  // of block and grid dimensions), while the lower one corresponds to index 1.
  int id;
  if (src.Precision() == QUDA_DOUBLE_PRECISION ||
      dst.Precision() == QUDA_DOUBLE_PRECISION && src.Precision() == QUDA_SINGLE_PRECISION) {
    id = 0;
  } else {
    id = 1;
  }
  setBlock(id, dst.Stride(), dst.Precision());

  quda::blas_bytes += src.RealLength()*((int)src.Precision() + (int)dst.Precision());

  if (dst.Precision() == QUDA_DOUBLE_PRECISION && src.Precision() == QUDA_SINGLE_PRECISION) {
    if (src.Nspin() == 4){
      SpinorTexture<float4, float4, float4, 6, 0> src_tex(src);
      Spinor<float4, float2, double2, 6> dst_spinor(dst);
      copyKernel<float4, 6><<<blasGrid, blasBlock>>>(dst_spinor, src_tex, src.Volume());
    } else { //src.Nspin() == 1
      SpinorTexture<float2, float2, float2, 3, 0> src_tex(src);
      Spinor<float2, float2, double2, 3> dst_spinor(dst);
      copyKernel<float2, 3><<<blasGrid, blasBlock>>>(dst_spinor, src_tex, src.Volume());
    }

  } else if (dst.Precision() == QUDA_SINGLE_PRECISION && src.Precision() == QUDA_DOUBLE_PRECISION) {
    if (src.Nspin() == 4){
      SpinorTexture<float4, float2, double2, 6, 0> src_tex(src);
      Spinor<float4, float4, float4, 6> dst_spinor(dst);
      copyKernel<float4, 6><<<blasGrid, blasBlock>>>(dst_spinor, src_tex, src.Volume());
    } else { //src.Nspin() ==1
      Spinor<float2, float2, double2, 3> src_tex(src);
      Spinor<float2, float2, float2, 3> dst_spinor(dst);
      copyKernel<float2, 3><<<blasGrid, blasBlock>>>(dst_spinor, src_tex, src.Volume());
    }
  } else if (dst.Precision() == QUDA_SINGLE_PRECISION && src.Precision() == QUDA_HALF_PRECISION) {
    quda::blas_bytes += src.Volume()*sizeof(float);
    if (src.Nspin() == 4){      
      SpinorTexture<float4, float4, short4, 6, 0> src_tex(src);
      Spinor<float4, float4, float4, 6> dst_spinor(dst);
      copyKernel<float4, 6><<<blasGrid, blasBlock>>>(dst_spinor, src_tex, src.Volume());
    } else { //nSpin== 1;
      SpinorTexture<float2, float2, short2, 3, 0> src_tex(src);
      Spinor<float2, float2, float2, 3> dst_spinor(dst);
      copyKernel<float2, 3><<<blasGrid, blasBlock>>>(dst_spinor, src_tex, src.Volume());
    }
  } else if (dst.Precision() == QUDA_HALF_PRECISION && src.Precision() == QUDA_SINGLE_PRECISION) {
    quda::blas_bytes += dst.Volume()*sizeof(float);
    if (src.Nspin() == 4){
      SpinorTexture<float4, float4, float4, 6, 0> src_tex(src);
      Spinor<float4, float4, short4, 6> dst_spinor(dst);
      copyKernel<float4, 6><<<blasGrid, blasBlock>>>(dst_spinor, src_tex, src.Volume());
    } else { //nSpin == 1
      SpinorTexture<float2, float2, float2, 3, 0> src_tex(src);
      Spinor<float2, float2, short2, 3> dst_spinor(dst);
      copyKernel<float2, 3><<<blasGrid, blasBlock>>>(dst_spinor, src_tex, src.Volume());
    }
  } else if (dst.Precision() == QUDA_DOUBLE_PRECISION && src.Precision() == QUDA_HALF_PRECISION) {
    quda::blas_bytes += src.Volume()*sizeof(float);
    if (src.Nspin() == 4){
      SpinorTexture<double2, float4, short4, 12, 0> src_tex(src);
      Spinor<double2, double2, double2, 12> dst_spinor(dst);
      copyKernel<double2, 12><<<blasGrid, blasBlock>>>(dst_spinor, src_tex, src.Volume());
    } else { //nSpin == 1
      SpinorTexture<double2, float2, short2, 3, 0> src_tex(src);
      Spinor<double2, double2, double2, 3> dst_spinor(dst);
      copyKernel<double2, 3><<<blasGrid, blasBlock>>>(dst_spinor, src_tex, src.Volume());
    }
  } else if (dst.Precision() == QUDA_HALF_PRECISION && src.Precision() == QUDA_DOUBLE_PRECISION) {
    quda::blas_bytes += dst.Volume()*sizeof(float);
    if (src.Nspin() == 4){
      SpinorTexture<double2, double2, double2, 12, 0> src_tex(src);
      Spinor<double2, double4, short4, 12> dst_spinor(dst);
      copyKernel<double2, 12><<<blasGrid, blasBlock>>>(dst_spinor, src_tex, src.Volume());
    } else { //nSpin == 1
      SpinorTexture<double2, double2, double2, 3, 0> src_tex(src);
      Spinor<double2, double2, short2, 3> dst_spinor(dst);
      copyKernel<double2, 3><<<blasGrid, blasBlock>>>(dst_spinor, src_tex, src.Volume());
    }
  } else {
    cudaMemcpy(dst.V(), src.V(), dst.Bytes(), cudaMemcpyDeviceToDevice);
    if (dst.Precision() == QUDA_HALF_PRECISION) {
      cudaMemcpy(dst.Norm(), src.Norm(), dst.NormBytes(), cudaMemcpyDeviceToDevice);
      quda::blas_bytes += 2*dst.RealLength()*sizeof(float);
    }
  }
  
  if (!blasTuning) checkCudaError();
}

/**
   Generic blas kernel with two loads and one store.
 */
template <typename FloatN, int M, typename InputX, typename InputY, typename OutputY, typename Functor>
__global__ void blasKernel(InputX X, InputY Y, Functor f, OutputY YY, int length) {
  unsigned int i = blockIdx.x*(blockDim.x) + threadIdx.x;
  unsigned int gridSize = gridDim.x*blockDim.x;
  while (i < length) {
    FloatN x[M], y[M];
    X.load(x, i);
    Y.load(y, i);

#pragma unroll
    for (int j=0; j<M; j++) f(x[j], y[j]);

    YY.save(y, i);
    i += gridSize;
  }
}

/**
   Driver for generic blas routine with two loads and one store.
 */
template <template <typename Float, typename FloatN> class Functor>
void blasCuda(const int kernel, const double2 &a, cudaColorSpinorField &x, 
	      const double2 &b, cudaColorSpinorField &y) {
  setBlock(kernel, x.Length(), x.Precision());
  checkSpinor(x, y);

  if (x.SiteSubset() == QUDA_FULL_SITE_SUBSET) {
    blasCuda<Functor>(kernel, a, x.Even(), b, y.Even());
    blasCuda<Functor>(kernel, a, x.Odd(), b, y.Odd());
    return;
  }

  if (x.Precision() == QUDA_DOUBLE_PRECISION) {
    Spinor<double2,double2,double2,1> X(x);
    Spinor<double2,double2,double2,1> Y(y);
    Functor<double2, double2> f(a,b);
    blasKernel<double2,1><<<blasGrid, blasBlock>>>(X, Y, f, Y, x.Length()/2);
  } else if (x.Precision() == QUDA_SINGLE_PRECISION) {
    Spinor<float4,float4,float4,1> X(x);
    Spinor<float4,float4,float4,1> Y(y);
    Functor<float2, float4> f(make_float2(a.x, a.y), make_float2(b.x, b.y));
    blasKernel<float4,1><<<blasGrid, blasBlock>>>(X, Y, f, Y, x.Length()/4);
  } else {
    if (x.Nspin() == 4){ //wilson
      SpinorTexture<float4,float4,short4,6,0> xTex(x);
      SpinorTexture<float4,float4,short4,6,1> yTex(y);
      Spinor<float4,float4,short4,6> yStore(y);
      Functor<float2, float4> f(make_float2(a.x, a.y), make_float2(b.x, b.y));
      blasKernel<float4, 6> <<<blasGrid, blasBlock>>> (xTex, yTex, f, yStore, y.Volume());
    } else if (x.Nspin() == 1) {//staggered
      SpinorTexture<float2,float2,short2,3,0> xTex(x);
      SpinorTexture<float2,float2,short2,3,1> yTex(y);
      Spinor<float2,float2,short2,3> yStore(y);
      Functor<float2, float2> f(make_float2(a.x, a.y), make_float2(b.x, b.y));
      blasKernel<float2, 3> <<<blasGrid, blasBlock>>>(xTex, yTex, f, yStore, y.Volume());
    } else { errorQuda("ERROR: nSpin=%d is not supported\n", x.Nspin()); }
    quda::blas_bytes += Functor<double2,double2>::streams()*x.Volume()*sizeof(float);
  }
  quda::blas_bytes += Functor<double2,double2>::streams()*x.RealLength()*x.Precision();
  quda::blas_flops += Functor<double2,double2>::flops()*x.RealLength();

  if (!blasTuning) checkCudaError();
}

/**
   Functor to perform the operation y = a*x + b*y
*/
template <typename Float2, typename FloatN>
struct axpby {
  const Float2 a;
  const Float2 b;
  axpby(const Float2 &a, const Float2 &b) : a(a), b(b) { ; }
  __device__ void operator()(const FloatN &x, FloatN &y) { y = a.x*x + b.x*y; }
  static int streams() { return 3; } //! total number of input and output streams
  static int flops() { return 3; } //! flops per element
};

void axpbyCuda(const double &a, cudaColorSpinorField &x, const double &b, cudaColorSpinorField &y) {
  const int kernel = 2;
  blasCuda<axpby>(kernel, make_double2(a, 0.0), x, make_double2(b, 0.0), y);
}

template <typename Float2, typename FloatN>
struct xpy {
  xpy(const Float2 &a, const Float2 &b) { ; }
  __device__ void operator()(const FloatN &x, FloatN &y) { y += x ; }
  static int streams() { return 3; } //! total number of input and output streams
  static int flops() { return 1; } //! flops per element
};

void xpyCuda(cudaColorSpinorField &x, cudaColorSpinorField &y) {
  const int kernel = 3;
  blasCuda<xpy>(kernel, make_double2(1.0, 0.0), x, make_double2(1.0, 0.0), y);
}

template <typename Float2, typename FloatN>
struct axpy {
  const Float2 a;
  axpy(const Float2 &a, const Float2 &b) : a(a) { ; }
  __device__ void operator()(const FloatN &x, FloatN &y) { y = a.x*x + y; }
  static int streams() { return 3; } //! total number of input and output streams
  static int flops() { return 2; } //! flops per element
};

void axpyCuda(const double &a, cudaColorSpinorField &x, cudaColorSpinorField &y) {
  const int kernel = 4;
  blasCuda<axpy>(kernel, make_double2(a, 0.0), x, make_double2(1.0, 0.0), y);
}

template <typename Float2, typename FloatN>
struct xpay {
  const Float2 a;
  xpay(const Float2 &a, const Float2 &b) : a(a) { ; }
  __device__ void operator()(const FloatN &x, FloatN &y) { y = x + a.x*y; }
  static int streams() { return 3; } //! total number of input and output streams
  static int flops() { return 2; } //! flops per element
};

void xpayCuda(cudaColorSpinorField &x, const double &a, cudaColorSpinorField &y) {
  const int kernel = 5;
  blasCuda<xpay>(kernel, make_double2(a,0.0), x, make_double2(0.0, 0.0), y);
}

template <typename Float2, typename FloatN>
struct mxpy {
  mxpy(const Float2 &a, const Float2 &b) { ; }
  __device__ void operator()(const FloatN &x, FloatN &y) { y -= x; }
  static int streams() { return 3; } //! total number of input and output streams
  static int flops() { return 1; } //! flops per element
};

void mxpyCuda(cudaColorSpinorField &x, cudaColorSpinorField &y) {
  const int kernel = 6;
  blasCuda<mxpy>(kernel, make_double2(1.0, 0.0), x, make_double2(1.0, 0.0), y);
}

template <typename Float2, typename FloatN>
struct ax {
  const Float2 a;
  ax(const Float2 &a, const Float2 &b) : a(a) { ; }
  __device__ void operator()(const FloatN &x, FloatN &y) { y *= a.x; }
  static int streams() { return 2; } //! total number of input and output streams
  static int flops() { return 1; } //! flops per element
};

void axCuda(const double &a, cudaColorSpinorField &x) {
  const int kernel = 7;
  blasCuda<ax>(kernel, make_double2(a, 0.0), x, make_double2(0.0, 0.0), x);
}

template <typename Float2, typename FloatN>
struct caxpy {
  const Float2 a;
  caxpy(const Float2 &a, const Float2 &b) : a(a) { ; }
  __device__ void operator()(const FloatN &x, FloatN &y) { caxpy_(a, x, y); }
  static int streams() { return 3; } //! total number of input and output streams
  static int flops() { return 4; } //! flops per element
};

void caxpyCuda(const quda::Complex &a, cudaColorSpinorField &x, cudaColorSpinorField &y) {
  const int kernel = 8;
  blasCuda<caxpy>(kernel, make_double2(real(a),imag(a)), x, make_double2(0.0, 0.0), y);
}

template <typename Float2, typename FloatN>
struct caxpby {
  const Float2 a;
  const Float2 b;
  caxpby(const Float2 &a, const Float2 &b) : a(a), b(b) { ; }
  __device__ void operator()(const FloatN &x, FloatN &y) { caxpby_(a, x, b, y); }
  static int streams() { return 3; } //! total number of input and output streams
  static int flops() { return 7; } //! flops per element
};

void caxpbyCuda(const quda::Complex &a, cudaColorSpinorField &x, const quda::Complex &b, cudaColorSpinorField &y) {
  const int kernel = 9;
  blasCuda<caxpby>(kernel, make_double2(a.real(),a.imag()), x, make_double2(b.real(), b.imag()), y);
}


/**
   Generic blas kernel with three loads and two stores.
 */
template <typename FloatN, int M, typename InputX, typename InputY, typename InputZ, 
	  typename InputW, typename OutputY, typename OutputZ, typename Functor>
__global__ void blasKernel(InputX X, InputY Y, InputZ Z, InputW W, Functor f, 
			   OutputY YY, OutputZ ZZ, int length) {
  unsigned int i = blockIdx.x*(blockDim.x) + threadIdx.x;
  unsigned int gridSize = gridDim.x*blockDim.x;
  while (i < length) {
    FloatN x[M], y[M], z[M], w[M];
    X.load(x, i);
    Y.load(y, i);
    Z.load(z, i);
    W.load(w, i);

#pragma unroll
    for (int j=0; j<M; j++) f(x[j], y[j], z[j], w[j]);

    YY.save(y, i);
    ZZ.save(z, i);
    i += gridSize;
  }
}

/**
   Driver for generic blas routine with four loads and two store.
 */
template <template <typename Float, typename FloatN> class Functor>
void blasCuda(const int kernel, const double2 &a, const double2 &b, const double2 &c,
	      cudaColorSpinorField &x, cudaColorSpinorField &y, 
	      cudaColorSpinorField &z, cudaColorSpinorField &w) {
  setBlock(kernel, x.Length(), x.Precision());
  checkSpinor(x, y);
  checkSpinor(x, z);
  checkSpinor(x, w);

  if (x.SiteSubset() == QUDA_FULL_SITE_SUBSET) {
    blasCuda<Functor>(kernel, a, b, c, x.Even(), y.Even(), z.Even(), w.Even());
    blasCuda<Functor>(kernel, a, b, c, x.Odd(), y.Odd(), z.Odd(), w.Even());
    return;
  }

  if (x.Precision() == QUDA_DOUBLE_PRECISION) {
    Spinor<double2,double2,double2,1> X(x);
    Spinor<double2,double2,double2,1> Y(y);
    Spinor<double2,double2,double2,1> Z(z);
    Spinor<double2,double2,double2,1> W(w);
    Functor<double2, double2> f(a,b,c);
    blasKernel<double2,1><<<blasGrid, blasBlock>>>(X, Y, Z, W, f, Y, Z, x.Length()/2);
  } else if (x.Precision() == QUDA_SINGLE_PRECISION) {
    Spinor<float4,float4,float4,1> X(x);
    Spinor<float4,float4,float4,1> Y(y);
    Spinor<float4,float4,float4,1> Z(z);
    Spinor<float4,float4,float4,1> W(w);
    Functor<float2, float4> f(make_float2(a.x, a.y), make_float2(b.x, b.y), make_float2(c.x, c.y));
    blasKernel<float4,1><<<blasGrid, blasBlock>>>(X, Y, Z, W, f, Y, Z, x.Length()/4);
  } else {
    if (x.Nspin() == 4){ //wilson
      SpinorTexture<float4,float4,short4,6,0> xTex(x);
      SpinorTexture<float4,float4,short4,6,1> yTex(y);
      SpinorTexture<float4,float4,short4,6,2> zTex(z);
      SpinorTexture<float4,float4,short4,6,2> wTex(w);
      Spinor<float4,float4,short4,6> yStore(y);
      Spinor<float4,float4,short4,6> zStore(z);
      Functor<float2, float4> f(make_float2(a.x, a.y), make_float2(b.x, b.y), make_float2(c.x, c.y));
      blasKernel<float4, 6> <<<blasGrid, blasBlock>>> (xTex, yTex, zTex, wTex, f, yStore, zStore, y.Volume());
    } else if (x.Nspin() == 1) {//staggered
      SpinorTexture<float2,float2,short2,3,0> xTex(x);
      SpinorTexture<float2,float2,short2,3,1> yTex(y);
      SpinorTexture<float2,float2,short2,3,2> zTex(z);
      SpinorTexture<float2,float2,short2,3,2> wTex(w);
      Spinor<float2,float2,short2,3> yStore(y);
      Spinor<float2,float2,short2,3> zStore(z);
      Functor<float2, float2> f(make_float2(a.x, a.y), make_float2(b.x, b.y), make_float2(c.x, c.y));
      blasKernel<float2, 3> <<<blasGrid, blasBlock>>>(xTex, yTex, zTex, wTex, f, yStore, zStore, y.Volume());
    } else { errorQuda("ERROR: nSpin=%d is not supported\n", x.Nspin()); }
    quda::blas_bytes += Functor<double2,double2>::streams()*x.Volume()*sizeof(float);
  }
  quda::blas_bytes += Functor<double2,double2>::streams()*x.RealLength()*x.Precision();
  quda::blas_flops += Functor<double2,double2>::flops()*x.RealLength();

  if (!blasTuning) checkCudaError();
}

/**
   Performs the operation z[i] = x[i] + a*y[i] + b*z[i]
*/
template <typename Float2, typename FloatN>
struct cxpaypbz {
  const Float2 a;
  const Float2 b;
  const Float2 c;
  cxpaypbz(const Float2 &a, const Float2 &b, const Float2 &c) : a(a), b(b), c(c) { ; }
  __device__ void operator()(const FloatN &x, const FloatN &y, FloatN &z, FloatN &w) 
  { cxpaypbz_(x, a, y, b, z); }
  static int streams() { return 4; } //! total number of input and output streams
  static int flops() { return 8; } //! flops per element
};

void cxpaypbzCuda(cudaColorSpinorField &x, const quda::Complex &a, cudaColorSpinorField &y, 
		  const quda::Complex &b, cudaColorSpinorField &z) {
  const int kernel = 10;
  blasCuda<cxpaypbz>(kernel, make_double2(a.real(),a.imag()), make_double2(b.real(), b.imag()), 
		     make_double2(0.0, 0.0), x, y, z, z);
}

/**
   Performs the operations: y[i] = a*x[i] + y[i]; x[i] = b*z[i] + c*x[i]
*/
template <typename Float2, typename FloatN>
struct axpyBzpcx {
  const Float2 a;
  const Float2 b;
  const Float2 c;
  axpyBzpcx(const Float2 &a, const Float2 &b, const Float2 &c) : a(a), b(b), c(c) { ; }
  __device__ void operator()(const FloatN &x, FloatN &y, FloatN &z, FloatN &w)
  { y += a.x*z; z = b.x*x + c.x*z; }
  static int streams() { return 5; } //! total number of input and output streams
  static int flops() { return 10; } //! flops per element
};

void axpyBzpcxCuda(const double &a, cudaColorSpinorField& x, cudaColorSpinorField& y, const double &b, 
		   cudaColorSpinorField& z, const double &c) {
  const int kernel = 11;
  // swap arguments around 
  blasCuda<axpyBzpcx>(kernel, make_double2(a,0.0), make_double2(b,0.0), make_double2(c,0.0), 
		      z, y, x, x);
}

/**
   Performs the operations: {y[i] = a*x[i] + y[i]; x[i] = z[i] + b*x[i]}
*/
template <typename Float2, typename FloatN>
struct axpyZpbx {
  const Float2 a;
  const Float2 b;
  axpyZpbx(const Float2 &a, const Float2 &b, const Float2 &c) : a(a), b(b) { ; }
  __device__ void operator()(const FloatN &x, FloatN &y, FloatN &z, FloatN &w)
  { y += a.x*z; z = x + b.x*z; }
  static int streams() { return 5; } //! total number of input and output streams
  static int flops() { return 8; } //! flops per element
};

void axpyZpbxCuda(const double &a, cudaColorSpinorField& x, cudaColorSpinorField& y,
		  cudaColorSpinorField& z, const double &b) {
  const int kernel = 12;
  // swap arguments around 
  blasCuda<axpyZpbx>(kernel, make_double2(a,0.0), make_double2(b,0.0), make_double2(0.0,0.0),
		     z, y, x, x);
}

/**
   Performs the operation z[i] = a*x[i] + b*y[i] + z[i] and y[i] -= b*w[i]
*/
template <typename Float2, typename FloatN>
struct caxpbypzYmbw {
  const Float2 a;
  const Float2 b;
  caxpbypzYmbw(const Float2 &a, const Float2 &b, const Float2 &c) : a(a), b(b) { ; }
  __device__ void operator()(const FloatN &x, FloatN &y, FloatN &z, FloatN &w)
  { caxpbypz_(a, x, b, y, z); caxpy_(-b, w, y); }

  static int streams() { return 6; } //! total number of input and output streams
  static int flops() { return 12; } //! flops per element
};

void caxpbypzYmbwCuda(const quda::Complex &a, cudaColorSpinorField &x, const quda::Complex &b, 
		      cudaColorSpinorField &y, cudaColorSpinorField &z, cudaColorSpinorField &w) {
  const int kernel = 12;
  // swap arguments around 
  blasCuda<caxpbypzYmbw>(kernel, make_double2(a.real(),a.imag()), make_double2(a.real(), b.imag()), 
			 make_double2(0.0,0.0), x, y, z, w);
}

/**
   Performs the operation y[i] += a*b*x[i], x[i] *= a
*/
template <typename Float2, typename FloatN>
struct cabxpyAx {
  const Float2 a;
  const Float2 b;
  cabxpyAx(const Float2 &a, const Float2 &b, const Float2 &c) : a(a), b(b) { ; }
  __device__ void operator()(const FloatN &x, FloatN &y, FloatN &z, FloatN &w) 
  { z *= a.x; caxpy_(b, z, y); }
  static int streams() { return 4; } //! total number of input and output streams
  static int flops() { return 5; } //! flops per element
};

void cabxpyAxCuda(const double &a, const quda::Complex &b, 
		  cudaColorSpinorField &x, cudaColorSpinorField &y) {
  const int kernel = 14;
  // swap arguments around 
  blasCuda<cabxpyAx>(kernel, make_double2(a,0.0), make_double2(b.real(),b.imag()), 
		     make_double2(0.0,0.0), x, y, x, x);
}

/**
   Performs the operation z[i] = a*x[i] + b*y[i] + z[i]
*/
template <typename Float2, typename FloatN>
struct caxpbypz {
  const Float2 a;
  const Float2 b;
  caxpbypz(const Float2 &a, const Float2 &b, const Float2 &c) : a(a), b(b) { ; }
  __device__ void operator()(const FloatN &x, FloatN &y, FloatN &z, FloatN &w) 
  { caxpbypz_(a, x, b, y, z); }
  static int streams() { return 4; } //! total number of input and output streams
  static int flops() { return 5; } //! flops per element
};

void caxpbypzCuda(const quda::Complex &a, cudaColorSpinorField &x, const quda::Complex &b, 
		  cudaColorSpinorField &y, cudaColorSpinorField &z) {
  const int kernel = 15;
  blasCuda<caxpbypz>(kernel, make_double2(a.real(),a.imag()), make_double2(b.real(),b.imag()), 
		     make_double2(0.0,0.0), x, y, z, z);
}

/**
   Performs the operation z[i] = a*x[i] + b*y[i] + c*z[i] + w[i]
*/
template <typename Float2, typename FloatN>
struct caxpbypczpw {
  const Float2 a;
  const Float2 b;
  const Float2 c;
  caxpbypczpw(const Float2 &a, const Float2 &b, const Float2 &c) : a(a), b(b), c(c) { ; }
  __device__ void operator()(const FloatN &x, FloatN &y, FloatN &z, FloatN &w) 
  { caxpy_(a, x, w); caxpy_(b, y, w); caxpy_(c, z, w); }

  static int streams() { return 4; } //! total number of input and output streams
  static int flops() { return 5; } //! flops per element
};

void caxpbypczpwCuda(const quda::Complex &a, cudaColorSpinorField &x, const quda::Complex &b, 
		     cudaColorSpinorField &y, const quda::Complex &c, cudaColorSpinorField &z, 
		     cudaColorSpinorField &w) {
  const int kernel = 16;
  // swap arguments around 
  blasCuda<caxpbypczpw>(kernel, make_double2(a.real(),a.imag()), make_double2(b.real(),b.imag()), 
			make_double2(c.real(), c.imag()), x, y, z, w);
}

__host__ __device__ void zero(double &x) { x = 0.0; }
__host__ __device__ void zero(double2 &x) { x.x = 0.0; x.y = 0.0; }
__host__ __device__ void zero(double3 &x) { x.x = 0.0; x.y = 0.0; x.z = 0.0; }
__device__ void copytoshared(double *s, int i, double x, int block) { s[i] = x; }
__device__ void copytoshared(double *s, int i, double2 x, int block) { s[i] = x.x; s[i+block] = x.y; }
__device__ void copytoshared(double *s, int i, double3 x, int block) 
{ s[i] = x.x; s[i+block] = x.y; s[i+2*block] = x.z; }
__device__ void copyfromshared(double &x, double *s, int i, int block) { x = s[i]; }
__device__ void copyfromshared(double2 &x, double *s, int i, int block) { x.x = s[i]; x.y = s[i+block]; }
__device__ void copyfromshared(double3 &x, double *s, int i, int block) 
{ x.x = s[i]; x.y = s[i+block]; x.z = s[i+2*block]; }

__device__ void add(double *s, int i, int j) { s[i] += s[j]; }
__device__ void add(volatile double *s, int i, int j) { s[i] += s[j]; }

/**
   Generic reduction kernel with up to four loads and three saves.
 */
template <int reduce_threads, typename ReduceType, typename ReduceSimpleType, 
	  typename FloatN, int M, int writeX, int writeY, int writeZ,
	  typename InputX, typename InputY, typename InputZ, typename InputW, typename InputV,
	  typename OutputX, typename OutputY, typename OutputZ, typename Reducer>
__global__ void reduceKernel(InputX X, InputY Y, InputZ Z, InputW W, InputV V, Reducer r, 
			     ReduceType *reduce, 
			     OutputX XX, OutputY YY, OutputZ ZZ, int length) {
  unsigned int tid = threadIdx.x;
  unsigned int i = blockIdx.x*(blockDim.x) + threadIdx.x;
  unsigned int gridSize = gridDim.x*blockDim.x;

  ReduceType sum;
  zero(sum); 
  while (i < length) {
    FloatN x[M], y[M], z[M], w[M], v[M];
    X.load(x, i);
    Y.load(y, i);
    Z.load(z, i);
    W.load(w, i);
    V.load(v, i);
#pragma unroll
    for (int j=0; j<M; j++) r(sum, x[j], y[j], z[j], w[j], v[j]);

    if (writeX) XX.save(x, i);
    if (writeY) YY.save(y, i);
    if (writeZ) ZZ.save(z, i);

    i += gridSize;
  }

  extern __shared__ ReduceSimpleType sdata[];
  ReduceSimpleType *s = sdata + tid;  
  copytoshared(s, 0, sum, reduce_threads);
  __syncthreads();
  
  // do reduction in shared mem
  if (reduce_threads >= 1024) { if (tid < 512) { add(s, 0, 512); } __syncthreads(); }
  if (reduce_threads >= 512) { if (tid < 256) { add(s, 0, 256); } __syncthreads(); }
  if (reduce_threads >= 256) { if (tid < 128) { add(s, 0, 128); } __syncthreads(); }
  if (reduce_threads >= 128) { if (tid <  64) { add(s, 0, 64); } __syncthreads(); }
  
  if (tid < 32) {
    volatile ReduceSimpleType *sv = s;
    if (reduce_threads >=  64) { add(sv, 0, 32); }
    if (reduce_threads >=  32) { add(sv, 0, 16); }
    if (reduce_threads >=  16) { add(sv, 0, 8); }
    if (reduce_threads >=  8)  { add(sv, 0, 4); }
    if (reduce_threads >=  4)  { add(sv, 0, 2); }
    if (reduce_threads >=  2)  { add(sv, 0, 1); }
  }
  
  // write result for this block to global mem 
  if (i == 0) {
    ReduceType tmp;
    copyfromshared(tmp, s, 0, reduce_threads);
    reduce[blockIdx.x] = tmp;
  }
}

/**
   Kernel launcher
*/
template <typename doubleN, typename ReduceType, typename ReduceSimpleType, typename FloatN, 
	  int M, int writeX, int writeY, int writeZ, 
	  typename InputX, typename InputY, typename InputZ, typename InputW, typename InputV,
	  typename Reducer, typename OutputX, typename OutputY, typename OutputZ>
doubleN reduceLaunch(InputX X, InputY Y, InputZ Z, InputW W, InputV V, Reducer r, 
		     OutputX XX, OutputY YY, OutputZ ZZ, int length) {
  // when there is only one warp per block, we need to allocate two warps 
  // worth of shared memory so that we don't index shared memory out of bounds
  size_t smemSize = (blasBlock.x <= 32) ? blasBlock.x * 2 * sizeof(ReduceType) : 
    blasBlock.x * sizeof(ReduceType);

  ReduceType *reduce = (ReduceType*)d_reduce;
  if (blasBlock.x == 32) {
    reduceKernel<32,ReduceType,ReduceSimpleType,FloatN,M,writeX,writeY,writeZ>
      <<< blasGrid, blasBlock, smemSize >>>(X, Y, Z, W, V, r, reduce, XX, YY, ZZ, length);
  } else if (blasBlock.x == 64) {
    reduceKernel<64,ReduceType,ReduceSimpleType,FloatN,M,writeX,writeY,writeZ>
      <<< blasGrid, blasBlock, smemSize >>>(X, Y, Z, W, V, r, reduce, XX, YY, ZZ, length);
  } else if (blasBlock.x == 128) {
    reduceKernel<128,ReduceType,ReduceSimpleType,FloatN,M,writeX,writeY,writeZ>
      <<< blasGrid, blasBlock, smemSize >>>(X, Y, Z, W, V, r, reduce, XX, YY, ZZ, length);
  } else if (blasBlock.x == 256) {
    reduceKernel<256,ReduceType,ReduceSimpleType,FloatN,M,writeX,writeY,writeZ>
      <<< blasGrid, blasBlock, smemSize >>>(X, Y, Z, W, V, r, reduce, XX, YY, ZZ, length);
  } else if (blasBlock.x == 512) {
    reduceKernel<512,ReduceType,ReduceSimpleType,FloatN,M,writeX,writeY,writeZ>
      <<< blasGrid, blasBlock, smemSize >>>(X, Y, Z, W, V, r, reduce, XX, YY, ZZ, length);
  } else if (blasBlock.x == 1024) {
    reduceKernel<1024,ReduceType,ReduceSimpleType,FloatN,M,writeX,writeY,writeZ>
      <<< blasGrid, blasBlock, smemSize >>>(X, Y, Z, W, V, r, reduce, XX, YY, ZZ, length);
  } else {
    errorQuda("Reduction not implemented for %d threads", blasBlock.x);
  }

  // copy result from device to host, and perform final reduction on CPU
  cudaMemcpy(h_reduce, d_reduce, blasGrid.x*sizeof(ReduceType), cudaMemcpyDeviceToHost);

  doubleN cpu_sum;
  zero(cpu_sum);
  for (unsigned int i = 0; i < blasGrid.x; i++) {
    cpu_sum += ((ReduceType*)h_reduce)[i];
  }

  const int Nreduce = sizeof(doubleN) / sizeof(double);
  reduceDoubleArray((double*)&cpu_sum, Nreduce);

  return cpu_sum;
}

/**
   Driver for generic reduction routine with two loads.

   @param ReduceType 
 */
template <typename doubleN, typename ReduceType, typename ReduceSimpleType,
	  template <typename ReduceType, typename Float, typename FloatN> class Reducer,
	  int writeX, int writeY, int writeZ>
doubleN reduceCuda(const int kernel, const double2 &a, const double2 &b, cudaColorSpinorField &x, 
		   cudaColorSpinorField &y, cudaColorSpinorField &z, cudaColorSpinorField &w,
		   cudaColorSpinorField &v) {
  if (x.SiteSubset() == QUDA_FULL_SITE_SUBSET) {
    doubleN even =
      reduceCuda<doubleN,ReduceType,ReduceSimpleType,Reducer,writeX,writeY,writeZ>
      (kernel, a, b, x.Even(), y.Even(), z.Even(), w.Even(), v.Even());
    doubleN odd = 
      reduceCuda<doubleN,ReduceType,ReduceSimpleType,Reducer,writeX,writeY,writeZ>
      (kernel, a, b, x.Odd(), y.Odd(), z.Odd(), w.Odd(), v.Odd());
    return even + odd;
  }

  setBlock(kernel, x.Length(), x.Precision());
  checkSpinor(x, y);
  checkSpinor(x, z);
  checkSpinor(x, w);
  checkSpinor(x, v);

  if (blasGrid.x > REDUCE_MAX_BLOCKS) {
    errorQuda("reduce_core: grid size %d must be smaller than %d", blasGrid.x, REDUCE_MAX_BLOCKS);
  }

  doubleN value;
  if (x.Precision() == QUDA_DOUBLE_PRECISION) {
    Spinor<double2,double2,double2,1> X(x);
    Spinor<double2,double2,double2,1> Y(y);
    Spinor<double2,double2,double2,1> Z(z);
    Spinor<double2,double2,double2,1> W(w);
    Spinor<double2,double2,double2,1> V(v);
    Reducer<ReduceType, double2, double2> r(a,b);
    value = reduceLaunch<doubleN,ReduceType,ReduceSimpleType,double2,1,writeX,writeY,writeZ>
      (X, Y, Z, W, V, r, X, Y, Z, x.Length()/2);
  } else if (x.Precision() == QUDA_SINGLE_PRECISION) {
    Spinor<float4,float4,float4,1> X(x);
    Spinor<float4,float4,float4,1> Y(y);
    Spinor<float4,float4,float4,1> Z(z);
    Spinor<float4,float4,float4,1> W(w);
    Spinor<float4,float4,float4,1> V(v);
    Reducer<ReduceType, float2, float4> r(make_float2(a.x, a.y), make_float2(b.x, b.y));
    value = reduceLaunch<doubleN,ReduceType,ReduceSimpleType,float4,1,writeX,writeY,writeZ>
      (X, Y, Z, W, V, r, X, Y, Z, x.Length()/4);
  } else {
    if (x.Nspin() == 4){ //wilson
      SpinorTexture<float4,float4,short4,6,0> xTex(x);
      SpinorTexture<float4,float4,short4,6,1> yTex(y);
      SpinorTexture<float4,float4,short4,6,2> zTex(z);
      SpinorTexture<float4,float4,short4,6,3> wTex(w);
      SpinorTexture<float4,float4,short4,6,4> vTex(v);
      Spinor<float4,float4,short4,6> xOut(x);
      Spinor<float4,float4,short4,6> yOut(y);
      Spinor<float4,float4,short4,6> zOut(z);
      Reducer<ReduceType, float2, float4> r(make_float2(a.x, a.y), make_float2(b.x, b.y));
      checkCudaError();
      value = reduceLaunch<doubleN,ReduceType,ReduceSimpleType,float4,6,writeX,writeY,writeZ>
	(xTex,yTex,zTex,wTex,vTex,r,xOut,yOut,zOut,y.Volume());
      checkCudaError();
    } else if (x.Nspin() == 1) {//staggered
      SpinorTexture<float2,float2,short2,3,0> xTex(x);
      SpinorTexture<float2,float2,short2,3,1> yTex(y);
      SpinorTexture<float2,float2,short2,3,2> zTex(z);
      SpinorTexture<float2,float2,short2,3,3> wTex(w);
      SpinorTexture<float2,float2,short2,3,4> vTex(v);
      Spinor<float2,float2,short2,3> xOut(x);
      Spinor<float2,float2,short2,3> yOut(y);
      Spinor<float2,float2,short2,3> zOut(z);
      Reducer<ReduceType, float2, float2> r(make_float2(a.x, a.y), make_float2(b.x, b.y));
      value = reduceLaunch<doubleN,ReduceType,ReduceSimpleType,float2,3,writeX,writeY,writeZ>
	(xTex,yTex,zTex,wTex,vTex,r,xOut,yOut,zOut,y.Volume());
    } else { errorQuda("ERROR: nSpin=%d is not supported\n", x.Nspin()); }
    quda::blas_bytes += Reducer<ReduceType,double2,double2>::streams()*x.Volume()*sizeof(float);
  }
  quda::blas_bytes += Reducer<ReduceType,double2,double2>::streams()*x.RealLength()*x.Precision();
  quda::blas_flops += Reducer<ReduceType,double2,double2>::flops()*x.RealLength();

  if (!blasTuning) checkCudaError();

  return value;
}

__device__ double norm2_(const double2 &a) { return a.x*a.x + a.y*a.y; }
__device__ float norm2_(const float2 &a) { return a.x*a.x + a.y*a.y; }
__device__ float norm2_(const float4 &a) { return a.x*a.x + a.y*a.y + a.z*a.z + a.w*a.w; }

template <typename ReduceType, typename Float2, typename FloatN>
struct Norm2 {
  Norm2(const Float2 &a, const Float2 &b) { ; }
  __device__ void operator()(ReduceType &sum, const FloatN &x, FloatN &y, const FloatN &z, const FloatN &w, const FloatN &v) { sum += norm2_(x); }
  static int streams() { return 1; } //! total number of input and output streams
  static int flops() { return 2; } //! flops per element
};

double normCuda(const cudaColorSpinorField &x) {
  const int kernel = 17;
  cudaColorSpinorField &y = (cudaColorSpinorField&)x; // FIXME
  return reduceCuda<double,QudaSumFloat,QudaSumFloat,Norm2,0,0,0>
    (kernel, make_double2(0.0, 0.0), make_double2(0.0, 0.0), y, y, y, y, y);
}

__device__ double dot_(const double &a, const double &b) { return a*b; }
__device__ float dot_(const float2 &a, const float2 &b) { return a.x*b.x + a.y*b.y; }
__device__ float dot_(const float4 &a, const float4 &b) { return a.x*b.x + a.y*b.y + a.z*b.z + a.w*b.w; }

template <typename ReduceType, typename Float2, typename FloatN>
struct Dot {
  Dot(const Float2 &a, const Float2 &b) { ; }
  __device__ void operator()(ReduceType &sum, const FloatN &x, FloatN &y, const FloatN &z, const FloatN &w, const FloatN &v) { sum += dot_(x); }
  static int streams() { return 2; } //! total number of input and output streams
  static int flops() { return 2; } //! flops per element
};

double reDotProductCuda(cudaColorSpinorField &x, cudaColorSpinorField &y) {
  const int kernel = 18;
  return reduceCuda<double,QudaSumFloat,QudaSumFloat,Norm2,0,0,0>
    (kernel, make_double2(0.0, 0.0), make_double2(0.0, 0.0), x, y, y, y, y);
}

template <typename ReduceType, typename Float2, typename FloatN>
struct axpyNorm2 {
  Float2 a;
  axpyNorm2(const Float2 &a, const Float2 &b) : a(a) { ; }
  __device__ void operator()(ReduceType &sum, const FloatN &x, FloatN &y, const FloatN &z, const FloatN &w, const FloatN &v) { 
    y += a.x*x; sum += norm2_(y); }
  static int streams() { return 3; } //! total number of input and output streams
  static int flops() { return 4; } //! flops per element
};

double axpyNormCuda(const double &a, cudaColorSpinorField &x, cudaColorSpinorField &y) {
  const int kernel = 19;
  return reduceCuda<double,QudaSumFloat,QudaSumFloat,axpyNorm2,0,1,0>
    (kernel, make_double2(a, 0.0), make_double2(0.0, 0.0), x, y, y, y, y);
}

/**
   First performs the operation y[i] = x[i] - y[i]
   Second returns the norm of y
*/
template <typename ReduceType, typename Float2, typename FloatN>
struct xmyNorm2 {
  xmyNorm2(const Float2 &a, const Float2 &b) { ; }
  __device__ void operator()(ReduceType &sum, const FloatN &x, FloatN &y, const FloatN &z, const FloatN &w, const FloatN &v) { 
    y = x - y; sum += norm2_(y); }
  static int streams() { return 3; } //! total number of input and output streams
  static int flops() { return 3; } //! flops per element
};

double xmyNormCuda(cudaColorSpinorField &x, cudaColorSpinorField &y) {
  const int kernel = 20;
  return reduceCuda<double,QudaSumFloat,QudaSumFloat,xmyNorm2,0,1,0>
    (kernel, make_double2(0.0, 0.0), make_double2(0.0, 0.0), x, y, y, y, y);
}

/**
  First performs the operation y[i] = a*x[i] + y[i]
  Second returns the norm of y
*/
template <typename ReduceType, typename Float2, typename FloatN>
struct caxpyNorm2 {
  Float2 a;
  caxpyNorm2(const Float2 &a, const Float2 &b) : a(a) { ; }
  __device__ void operator()(ReduceType &sum, const FloatN &x, FloatN &y, const FloatN &z, const FloatN &w, const FloatN &v) { 
    caxpy_(a, x, y); sum += norm2_(y); }
  static int streams() { return 3; } //! total number of input and output streams
  static int flops() { return 6; } //! flops per element
};

double caxpyNormCuda(const quda::Complex &a, cudaColorSpinorField &x, cudaColorSpinorField &y) {
  const int kernel = 21;
  return reduceCuda<double,QudaSumFloat,QudaSumFloat,caxpyNorm2,0,1,0>
    (kernel, make_double2(a.real(), a.imag()), make_double2(0.0, 0.0), x, y, x, x, x);
}

/**
   double caxpyXmayNormCuda(float a, float *x, float *y, n){}
   
   First performs the operation y[i] = a*x[i] + y[i]
   Second performs the operator x[i] -= a*z[i]
   Third returns the norm of x
*/
template <typename ReduceType, typename Float2, typename FloatN>
struct caxpyxmaznormx {
  Float2 a;
  caxpyxmaznormx(const Float2 &a, const Float2 &b) : a(a) { ; }
  __device__ void operator()(ReduceType &sum, FloatN &x, FloatN &y, const FloatN &z, const FloatN &w, const FloatN &v) { caxpy_(a, x, y); x-= a.x*z; sum += norm2_(x); }
  static int streams() { return 5; } //! total number of input and output streams
  static int flops() { return 10; } //! flops per element
};

double caxpyXmazNormXCuda(const quda::Complex &a, cudaColorSpinorField &x, 
			  cudaColorSpinorField &y, cudaColorSpinorField &z) {
  const int kernel = 22;
  return reduceCuda<double,QudaSumFloat,QudaSumFloat,caxpyxmaznormx,1,1,0>
    (kernel, make_double2(a.real(), a.imag()), make_double2(0.0, 0.0), x, y, z, x, x);
}

/**
   double cabxpyAxNormCuda(float a, complex b, float *x, float *y, n){}
   
   First performs the operation y[i] = a*x[i] + y[i]
   Second performs the operator x[i] -= a*z[i]
   Third returns the norm of x
*/
template <typename ReduceType, typename Float2, typename FloatN>
struct cabxpyaxnorm {
  Float2 a;
  Float2 b;
  cabxpyaxnorm(const Float2 &a, const Float2 &b) : a(a), b(b) { ; }
  __device__ void operator()(ReduceType &sum, FloatN &x, FloatN &y, const FloatN &z, const FloatN &w, const FloatN &v) { x *= a.x; caxpy_(b, x, y); sum += norm2_(y); }
  static int streams() { return 5; } //! total number of input and output streams
  static int flops() { return 10; } //! flops per element
};

double cabxpyAxNormCuda(const double &a, const quda::Complex &b, 
			cudaColorSpinorField &x, cudaColorSpinorField &y) {
  const int kernel = 23;
  return reduceCuda<double,QudaSumFloat,QudaSumFloat,cabxpyaxnorm,1,1,0>
    (kernel, make_double2(a, 0.0), make_double2(b.real(), b.imag()), x, y, x, x, x);
}

__device__ double2 cdot_(const double2 &a, const double2 &b) 
{ return make_double2(a.x*b.x + a.y*b.y, a.x*b.y - a.y*b.x); }
__device__ double2 cdot_(const float2 &a, const float2 &b) 
{ return make_double2(a.x*b.x + a.y*b.y, a.x*b.y - a.y*b.x); }
__device__ double2 cdot_(const float4 &a, const float4 &b) 
{ return make_double2(a.x*b.x + a.y*b.y + a.z*b.z + a.w*b.w, a.x*b.y - a.y*b.x + a.z*b.w - a.w*b.z); }

template <typename ReduceType, typename Float2, typename FloatN>
struct Cdot {
  Cdot(const Float2 &a, const Float2 &b) { ; }
  __device__ void operator()(ReduceType &sum, const FloatN &x, FloatN &y, const FloatN &z, const FloatN &w, const FloatN &v) { sum += cdot_(x,y); }
  static int streams() { return 2; } //! total number of input and output streams
  static int flops() { return 4; } //! flops per element
};

quda::Complex cDotProductCuda(cudaColorSpinorField &x, cudaColorSpinorField &y) {
  const int kernel = 24;
  double2 cdot = reduceCuda<double2,QudaSumFloat2,QudaSumFloat,Cdot,0,0,0>
    (kernel, make_double2(0.0, 0.0), make_double2(0.0, 0.0), x, y, y, y, y);
  return quda::Complex(cdot.x, cdot.y);
}

/**
   double2 xpaycDotzyCuda(float2 *x, float a, float2 *y, float2 *z, int n) {}
   
   First performs the operation y = x + a*y
   Second returns cdot product (z,y)
*/
template <typename ReduceType, typename Float2, typename FloatN>
struct xpaycdotzy {
  Float2 a;
  xpaycdotzy(const Float2 &a, const Float2 &b) : a(a) { ; }
  __device__ void operator()(ReduceType &sum, const FloatN &x, FloatN &y, const FloatN &z, const FloatN &w, const FloatN &v) { y = x + a.x*y; sum += cdot_(z,y); }
  static int streams() { return 4; } //! total number of input and output streams
  static int flops() { return 6; } //! flops per element
};

quda::Complex xpaycDotzyCuda(cudaColorSpinorField &x, const double &a, cudaColorSpinorField &y, cudaColorSpinorField &z) {
  const int kernel = 25;
  double2 cdot = reduceCuda<double2,QudaSumFloat2,QudaSumFloat,xpaycdotzy,0,1,0>
    (kernel, make_double2(a, 0.0), make_double2(0.0, 0.0), x, y, z, y, y);
  return quda::Complex(cdot.x, cdot.y);
}

/**
   double caxpyDotzyCuda(float a, float *x, float *y, float *z, n){}
   
   First performs the operation y[i] = a*x[i] + y[i]
   Second returns the dot product (z,y)
*/
template <typename ReduceType, typename Float2, typename FloatN>
struct caxpydotzy {
  Float2 a;
   caxpydotzy(const Float2 &a, const Float2 &b) : a(a) { ; }
  __device__ void operator()(ReduceType &sum, const FloatN &x, FloatN &y, const FloatN &z, const FloatN &w, const FloatN &v) { caxpy_(a, x, y); sum += cdot_(z,y); }
  static int streams() { return 4; } //! total number of input and output streams
  static int flops() { return 8; } //! flops per element
};

quda::Complex caxpyDotzyCuda(const quda::Complex &a, cudaColorSpinorField &x, cudaColorSpinorField &y,
		       cudaColorSpinorField &z) {
  const int kernel = 26;
  double2 cdot = reduceCuda<double2,QudaSumFloat2,QudaSumFloat,caxpydotzy,0,1,0>
    (kernel, make_double2(a.real(), a.imag()), make_double2(0.0, 0.0), x, y, z, y, y);
  return quda::Complex(cdot.x, cdot.y);
}

__device__ double3 cdotNormA_(const double2 &a, const double2 &b) 
{ return make_double3(a.x*b.x + a.y*b.y, a.x*b.y - a.y*b.x, a.x*a.x + a.y*a.y); }
__device__ double3 cdotNormA_(const float2 &a, const float2 &b) 
{ return make_double3(a.x*b.x + a.y*b.y, a.x*b.y - a.y*b.x, a.x*a.x + a.y*a.y); }
__device__ double3 cdotNormA_(const float4 &a, const float4 &b) 
{ return make_double3(a.x*b.x + a.y*b.y + a.z*b.z + a.w*b.w, 
		      a.x*b.y - a.y*b.x + a.z*b.w - a.w*b.z,
		      a.x*a.x + a.y*a.y + a.z*a.z + a.w*a.w); }

template <typename ReduceType, typename Float2, typename FloatN>
struct CdotNormA {
  CdotNormA(const Float2 &a, const Float2 &b) { ; }
  __device__ void operator()(ReduceType &sum, const FloatN &x, FloatN &y, const FloatN &z, const FloatN &w, const FloatN &v) { sum += cdotNormA_(x,y); }
  static int streams() { return 2; } //! total number of input and output streams
  static int flops() { return 6; } //! flops per element
};

double3 cDotProductNormACuda(cudaColorSpinorField &x, cudaColorSpinorField &y) {
  const int kernel = 27;
  return reduceCuda<double3,QudaSumFloat3,QudaSumFloat,CdotNormA,0,0,0>
    (kernel, make_double2(0.0, 0.0), make_double2(0.0, 0.0), x, y, y, y, y);
}

__device__ double3 cdotNormB_(const double2 &a, const double2 &b) 
{ return make_double3(a.x*b.x + a.y*b.y, a.x*b.y - a.y*b.x, b.x*b.x + b.y*b.y); }
__device__ double3 cdotNormB_(const float2 &a, const float2 &b) 
{ return make_double3(a.x*b.x + a.y*b.y, a.x*b.y - a.y*b.x, b.x*b.x + b.y*b.y); }
__device__ double3 cdotNormB_(const float4 &a, const float4 &b) 
{ return make_double3(a.x*b.x + a.y*b.y + a.z*b.z + a.w*b.w, a.x*b.y - a.y*b.x + a.z*b.w - a.w*b.z,
		      b.x*b.x + b.y*b.y + b.z*b.z + b.w*b.w); }

template <typename ReduceType, typename Float2, typename FloatN>
struct CdotNormB {
  CdotNormB(const Float2 &a, const Float2 &b) { ; }
  __device__ void operator()(ReduceType &sum, const FloatN &x, FloatN &y, const FloatN &z, const FloatN &w, const FloatN &v) { sum += cdotNormB_(x,y); }
  static int streams() { return 2; } //! total number of input and output streams
  static int flops() { return 6; } //! flops per element
};

double3 cDotProductNormBCuda(cudaColorSpinorField &x, cudaColorSpinorField &y) {
  const int kernel = 28;
  return reduceCuda<double3,QudaSumFloat3,QudaSumFloat,CdotNormB,0,0,0>
    (kernel, make_double2(0.0, 0.0), make_double2(0.0, 0.0), x, y, y, y, y);
}

/**
   This convoluted kernel does the following: 
   z += a*x + b*y, y -= b*w, norm = (y,y), dot = (u, y)
 */
template <typename ReduceType, typename Float2, typename FloatN>
struct caxpbypzYmbwcDotProductUYNormY {
  Float2 a;
  Float2 b;
  caxpbypzYmbwcDotProductUYNormY(const Float2 &a, const Float2 &b) : a(a), b(b) { ; }
  __device__ void operator()(ReduceType &sum, const FloatN &x, FloatN &y, FloatN &z, const FloatN &w, const FloatN &v) { caxpy_(a, x, z); caxpy_(b, y, z); caxpy_(-b, v, y); sum += cdotNormB_(v,y); }
  static int streams() { return 7; } //! total number of input and output streams
  static int flops() { return 18; } //! flops per element
};

double3 caxpbypzYmbwcDotProductUYNormYCuda(const quda::Complex &a, cudaColorSpinorField &x, 
					   const quda::Complex &b, cudaColorSpinorField &y,
					   cudaColorSpinorField &z, cudaColorSpinorField &w,
					   cudaColorSpinorField &u) {
  const int kernel = 29;
  return reduceCuda<double3,QudaSumFloat3,QudaSumFloat,caxpbypzYmbwcDotProductUYNormY,0,0,0>
    (kernel, make_double2(a.real(), a.imag()), make_double2(b.real(), b.imag()), x, y, z, w, u);
}
